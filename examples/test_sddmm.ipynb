{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spatha\n",
    "import spatha_sddmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from native_scripting import compile\n",
    "import functools\n",
    "import ctypes\n",
    "import time\n",
    "import math\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cache = functools.cache\n",
    "except AttributeError:\n",
    "    cache = functools.lru_cache(maxsize=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Weight candidate selection for removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def group_n_m2(dense_shape, dense_dtype, n, m, tileM):\n",
    "    nrows = dense_shape[0]\n",
    "    ncols = dense_shape[1]\n",
    "\n",
    "    m_fixed = 4\n",
    "    A_num_cols_sp_pad = round_up((round_up(ncols, m)/m)*n, 16)\n",
    "    indexes_cols = A_num_cols_sp_pad//n*m_fixed\n",
    "    assert dense_dtype in (torch.float32, torch.float64)\n",
    "    dtype = \"float\" if dense_dtype == torch.float32 else \"double\"\n",
    "    lib = compile(\n",
    "        f\"\"\"\n",
    "        #include <iostream>\n",
    "        #include <algorithm>\n",
    "        #include <utility>\n",
    "        #include <cstdlib>\n",
    "        #include <cstdio>\n",
    "        #include <cmath>\n",
    "        #include <functional>\n",
    "        #include <tuple>\n",
    "        #include <vector>\n",
    "        #include <numeric>\n",
    "        #include <chrono>\n",
    "\n",
    "        using namespace std;\n",
    "\n",
    "        int int_ceil(int x, int y){{\n",
    "            return (x - 1) / y + 1;\n",
    "        }}\n",
    "\n",
    "        extern \"C\" void func({dtype}* dense, {dtype}* sparse, int* masks, int *columns){{\n",
    "            int bm_m   = {nrows}/{tileM};\n",
    "            int mcol_k = {ncols}/{m};\n",
    "            int mcol_k_p = int_ceil({ncols},{m});\n",
    "            int m_fixed = 4;\n",
    "\n",
    "            std::vector<{dtype}> v(m_fixed, 0);\n",
    "            std::vector<int> vx(m_fixed, 0);\n",
    "            {dtype} max=0, total=0;\n",
    "\n",
    "            std::vector<size_t> indices(v.size());\n",
    "            std::iota(indices.begin(), indices.end(), 0);\n",
    "\n",
    "            for(int bm_i=0; bm_i<bm_m; bm_i++){{\n",
    "                int t_bm_i   = bm_i*{tileM}*{ncols};\n",
    "                for(int mcol_i=0; mcol_i<mcol_k; mcol_i++){{\n",
    "                    int t_mcol_i = mcol_i*{m};\n",
    "                    max = 0;\n",
    "\n",
    "                    std::vector<int> cols_max;\n",
    "                    cols_max.resize(m_fixed, 0);\n",
    "                    std::vector<int> masks_max;\n",
    "                    masks_max.resize({tileM}*{m}, 0);\n",
    "\n",
    "                    for(int col_i=0; col_i<{m}; col_i++){{\n",
    "                        vx[0]=col_i;\n",
    "                        for(int col_j=col_i+1; col_j<{m}; col_j++){{\n",
    "                            vx[1]=col_j;\n",
    "                            for(int col_k=col_j+1; col_k<{m}; col_k++){{\n",
    "                                vx[2]=col_k;\n",
    "                                for(int col_w=col_k+1; col_w<{m}; col_w++){{\n",
    "                                    vx[3]=col_w;\n",
    "                                    total=0;\n",
    "\n",
    "                                    std::vector<int> mask({tileM}*{m}, 0);\n",
    "\n",
    "                                    for(int row_i=0; row_i<{tileM}; row_i++){{\n",
    "                                        int t_row_i  = row_i*{ncols};\n",
    "                                        v[0]=dense[t_bm_i + t_row_i + t_mcol_i + col_i];\n",
    "                                        v[1]=dense[t_bm_i + t_row_i + t_mcol_i + col_j];\n",
    "                                        v[2]=dense[t_bm_i + t_row_i + t_mcol_i + col_k];\n",
    "                                        v[3]=dense[t_bm_i + t_row_i + t_mcol_i + col_w];\n",
    "\n",
    "                                        std::partial_sort(indices.begin(), indices.begin() + {n}, indices.end(), [&](size_t A, size_t B) {{\n",
    "                                                    return v[A] > v[B];}});\n",
    "\n",
    "                                        for(int id=0; id<{n}; id++){{\n",
    "                                            total += dense[t_bm_i + t_row_i + t_mcol_i + vx[indices[id]]];\n",
    "\n",
    "                                            mask[row_i*{m} + vx[indices[id]]] = 1;\n",
    "                                        }}\n",
    "                                    }}\n",
    "\n",
    "                                    if(total>max){{\n",
    "                                        max = total;\n",
    "                                        std::copy(mask.begin(), mask.end(), masks_max.begin());\n",
    "\n",
    "                                        std::sort(vx.begin(), vx.begin() + m_fixed);\n",
    "                                        std::copy(vx.begin(), vx.end(), cols_max.begin());\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }}\n",
    "                        }}\n",
    "                    }}\n",
    "\n",
    "                    for(int i=0; i<{tileM}; i++){{\n",
    "                        for(int j=0; j<{m}; j++){{\n",
    "                            int drop = masks_max[i*{m} + j];\n",
    "                            masks[t_bm_i  + i*{ncols}+ t_mcol_i + j]  = drop;\n",
    "                            sparse[t_bm_i + i*{ncols}+ t_mcol_i + j] *= drop;\n",
    "                        }}\n",
    "                    }}\n",
    "                    for(int i=0; i<m_fixed; i++){{\n",
    "                        columns[bm_i*{indexes_cols} + mcol_i*m_fixed + i] =\n",
    "                        cols_max[i];\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "\n",
    "            int remainder = {ncols}%{m};\n",
    "\n",
    "            if (remainder>0){{\n",
    "                int d_rows={tileM}, d_cols=remainder;\n",
    "\n",
    "                if(remainder<3){{\n",
    "                    for(int i=0; i<{nrows}; i++){{\n",
    "                        for(int j={ncols}-remainder; j<{ncols}; j++){{\n",
    "                            masks[i*{ncols}+j] = 1;\n",
    "                        }}\n",
    "                    }}\n",
    "                    for(int bm_i=0; bm_i<bm_m; bm_i++){{\n",
    "                        for(int j=0; j<m_fixed; j++){{\n",
    "                            columns[bm_i*{indexes_cols} + mcol_k*m_fixed + j] = j;\n",
    "                        }}\n",
    "                    }}\n",
    "                }} else if(remainder==3){{\n",
    "                    v[3] = -1;\n",
    "                    for(int bm_i=0; bm_i<bm_m; bm_i++){{\n",
    "                        int t_bm_i   = bm_i*{tileM}*{ncols};\n",
    "                        for(int mcol_i=mcol_k; mcol_i<mcol_k_p; mcol_i++){{\n",
    "                            max = 0;\n",
    "                            int t_mcol_i = mcol_i*{m};\n",
    "\n",
    "                            std::vector<int> cols_max(m_fixed, 0);\n",
    "                            std::vector<int> masks_max({tileM}*remainder, 0);\n",
    "\n",
    "                            for(int col_i=0; col_i<remainder; col_i++){{\n",
    "                                vx[0]=col_i;\n",
    "                                for(int col_j=col_i+1; col_j<remainder; col_j++){{\n",
    "                                    vx[1]=col_j;\n",
    "                                    for(int col_k=col_j+1; col_k<remainder; col_k++){{\n",
    "                                        vx[2]=col_k;\n",
    "                                        total=0;\n",
    "                                        std::vector<int> mask({tileM}*remainder, 0);\n",
    "\n",
    "                                        for(int row_i=0; row_i<{tileM}; row_i++){{\n",
    "                                            int t_row_i  = row_i*{ncols};\n",
    "                                            v[0]=dense[t_bm_i + t_row_i + t_mcol_i + col_i];\n",
    "                                            v[1]=dense[t_bm_i + t_row_i + t_mcol_i + col_j];\n",
    "                                            v[2]=dense[t_bm_i + t_row_i + t_mcol_i + col_k];\n",
    "\n",
    "                                            std::partial_sort(indices.begin(), indices.begin() + {n}, indices.end(), [&](size_t A, size_t B) {{\n",
    "                                                        return v[A] > v[B]; }});\n",
    "\n",
    "                                            for(int id=0; id<{n}; id++){{\n",
    "                                                total += dense[t_bm_i + t_row_i + t_mcol_i + vx[indices[id]]];\n",
    "\n",
    "                                                mask[row_i*remainder + vx[indices[id]]] = 1;\n",
    "                                            }}\n",
    "                                        }}\n",
    "\n",
    "                                        if(total>max){{\n",
    "                                            max = total;\n",
    "                                            std::copy(mask.begin(), mask.end(), masks_max.begin());\n",
    "\n",
    "                                            std::sort(vx.begin(), vx.begin() + remainder);//m_fixed\n",
    "                                            std::copy(vx.begin(), vx.end(), cols_max.begin());\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }}\n",
    "\n",
    "                            for(int i=0; i<{tileM}; i++){{\n",
    "                                for(int j=0; j<remainder; j++){{\n",
    "                                    int drop = masks_max[i*remainder + j];\n",
    "\n",
    "                                    masks[t_bm_i  + i*{ncols}+ t_mcol_i + j]  = drop;\n",
    "                                    sparse[t_bm_i + i*{ncols}+ t_mcol_i + j] *= drop;\n",
    "                                }}\n",
    "                            }}\n",
    "                            for(int i=0; i<remainder; i++){{\n",
    "                                columns[bm_i*{indexes_cols} + mcol_i*m_fixed + i] =\n",
    "                                cols_max[i];\n",
    "                            }}\n",
    "                        }}\n",
    "                    }}\n",
    "                }} else {{\n",
    "                    for(int bm_i=0; bm_i<bm_m; bm_i++){{\n",
    "                        int t_bm_i   = bm_i*{tileM}*{ncols};\n",
    "                        for(int mcol_i=mcol_k; mcol_i<mcol_k_p; mcol_i++){{\n",
    "                            max = 0;\n",
    "                            int t_mcol_i = mcol_i*{m};\n",
    "\n",
    "                            std::vector<int> cols_max(m_fixed, 0);\n",
    "                            std::vector<int> masks_max({tileM}*remainder, 0);\n",
    "\n",
    "                            for(int col_i=0; col_i<remainder; col_i++){{\n",
    "                                vx[0]=col_i;\n",
    "                                for(int col_j=col_i+1; col_j<remainder; col_j++){{\n",
    "                                    vx[1]=col_j;\n",
    "                                    for(int col_k=col_j+1; col_k<remainder; col_k++){{\n",
    "                                        vx[2]=col_k;\n",
    "                                        for(int col_w=col_k+1; col_w<remainder; col_w++){{\n",
    "                                            vx[3]=col_w;\n",
    "                                            total=0;\n",
    "                                            std::vector<int> mask({tileM}*remainder, 0);\n",
    "\n",
    "                                            for(int row_i=0; row_i<{tileM}; row_i++){{\n",
    "                                                int t_row_i  = row_i*{ncols};\n",
    "                                                v[0]=dense[t_bm_i + t_row_i + t_mcol_i + col_i];\n",
    "                                                v[1]=dense[t_bm_i + t_row_i + t_mcol_i + col_j];\n",
    "                                                v[2]=dense[t_bm_i + t_row_i + t_mcol_i + col_k];\n",
    "                                                v[3]=dense[t_bm_i + t_row_i + t_mcol_i + col_w];\n",
    "\n",
    "                                                std::partial_sort(indices.begin(), indices.begin() + {n}, indices.end(), [&](size_t A, size_t B) {{\n",
    "                                                            return v[A] > v[B]; }});\n",
    "\n",
    "                                                for(int id=0; id<{n}; id++){{\n",
    "                                                    total += dense[t_bm_i + t_row_i + t_mcol_i + vx[indices[id]]];\n",
    "\n",
    "                                                    mask[row_i*remainder + vx[indices[id]]] = 1;\n",
    "                                                }}\n",
    "                                            }}\n",
    "\n",
    "                                            if(total>max){{\n",
    "                                                max = total;\n",
    "                                                std::copy(mask.begin(), mask.end(), masks_max.begin());\n",
    "\n",
    "                                                std::sort(vx.begin(), vx.begin() + m_fixed);\n",
    "                                                std::copy(vx.begin(), vx.end(), cols_max.begin());\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }}\n",
    "\n",
    "                            for(int i=0; i<{tileM}; i++){{\n",
    "                                for(int j=0; j<remainder; j++){{\n",
    "                                    int drop = masks_max[i*remainder + j];\n",
    "\n",
    "                                    masks[t_bm_i  + i*{ncols}+ t_mcol_i + j]  = drop;\n",
    "                                    sparse[t_bm_i + i*{ncols}+ t_mcol_i + j] *= drop;\n",
    "                                }}\n",
    "                            }}\n",
    "                            for(int i=0; i<m_fixed; i++){{\n",
    "                                columns[bm_i*{indexes_cols} + mcol_i*m_fixed + i] =\n",
    "                                cols_max[i];\n",
    "                            }}\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\",\n",
    "    )\n",
    "    lib.func.argtypes = [\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "    ]\n",
    "    return lib.func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#VENOM to dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def to_dense(dense_shape, dense_dtype, n, m, tileM):\n",
    "    nrows = dense_shape[0]\n",
    "    ncols = dense_shape[1]\n",
    "\n",
    "    A_size = nrows*ncols\n",
    "    density = n/m\n",
    "\n",
    "    brow = 4 #this->brow = brow_;\n",
    "    mbrow = 32 #this->mbrow = mbrow_;\n",
    "\n",
    "    bm   = tileM\n",
    "    # !IMPORTANT! constants because of architecture constraints\n",
    "    m_fixed = 4\n",
    "    bits_elem_meta=2\n",
    "    mrow_m = 2\n",
    "    bits_elem_cols=8\n",
    "    brow_fixed = 16\n",
    "    nelems=32//bits_elem_meta #(sizeof(uint)*8)=32\n",
    "    nelems_col = nelems//mrow_m\n",
    "\n",
    "    A_num_cols_sp = (ncols/m)*n\n",
    "    A_num_cols_sp_pad_nm = (round_up(ncols, m)/m)*n\n",
    "    A_num_cols_sp_pad = round_up((round_up(ncols, m)/m)*n, 16)\n",
    "    A_nnz = nrows*A_num_cols_sp_pad\n",
    "\n",
    "    assert dense_dtype in (torch.float32, torch.float64)\n",
    "    dtype = \"float\" if dense_dtype == torch.float32 else \"double\"\n",
    "    lib = compile(\n",
    "        f\"\"\"\n",
    "        #include <iostream>\n",
    "        #include <algorithm>\n",
    "        #include <utility>\n",
    "        #include <cstdlib>\n",
    "        #include <cstdio>\n",
    "        #include <cmath>\n",
    "        #include <functional>\n",
    "        #include <tuple>\n",
    "        #include <vector>\n",
    "        #include <numeric>\n",
    "        #include <chrono>\n",
    "\n",
    "        using namespace std;\n",
    "\n",
    "\n",
    "        extern \"C\" void func3({dtype}* hA_dense, {dtype}* hA_values, int *hA_columns, int *hA_metadata){{\n",
    "            //this->hA_dense.resize(this->A_size, 0);\n",
    "\n",
    "            // general variables N:M format\n",
    "            int bm_m = {nrows}/{bm};\n",
    "            int mbrow_m = {bm}/{mbrow};\n",
    "            int mbrow_m2 = {mbrow}/{brow_fixed};\n",
    "            int brow_m = {brow_fixed}/{brow};\n",
    "            // metadata\n",
    "            int mcol_kk = {nelems}/{mrow_m}/{n};\n",
    "            int mcol_k = {A_num_cols_sp_pad}/{n}/mcol_kk;\n",
    "            // indices\n",
    "            int col_kk = mcol_kk;\n",
    "            int col_k = {A_num_cols_sp_pad}/{n}/col_kk;\n",
    "\n",
    "            uint indexes[{nelems}];\n",
    "            uint columns[col_kk*{m_fixed}];\n",
    "\n",
    "            for(int bm_i=0; bm_i<bm_m; bm_i++){{\n",
    "                for(int mbrow_i=0; mbrow_i<mbrow_m; mbrow_i++){{\n",
    "                    for(int mbrow_i2=0; mbrow_i2<mbrow_m2; mbrow_i2++){{\n",
    "                        for(int brow_i=0; brow_i<brow_m; brow_i++){{\n",
    "                            for(int mcol_i=0; mcol_i<mcol_k; mcol_i++){{\n",
    "                                //read columns indexes\n",
    "                                for(int col_i=0; col_i<col_kk; col_i++){{\n",
    "                                    for(int col_ii=0; col_ii<{m_fixed}; col_ii++){{\n",
    "                                        columns[col_i*{m_fixed} + col_ii] =\n",
    "                                        hA_columns[bm_i*col_k*col_kk*{m_fixed} + mcol_i*col_kk*{m_fixed} + col_i*{m_fixed} + col_ii];\n",
    "                                    }}\n",
    "                                }}\n",
    "                                // read metadata\n",
    "                                for(int mbrow_ii=0; mbrow_ii<({brow}/{mrow_m}); mbrow_ii++){{\n",
    "                                    for(int mbrow_iii=0; mbrow_iii<{mrow_m}; mbrow_iii++){{\n",
    "                                        for(int mcol_ii=0; mcol_ii<mcol_kk; mcol_ii++){{\n",
    "                                            for (int n_i=0; n_i<{n}; n_i++) {{\n",
    "                                                indexes[\n",
    "                                                    mbrow_iii*{n} +\n",
    "                                                    mcol_ii*{mrow_m}*{n} +\n",
    "                                                    n_i] =\n",
    "                                                (((hA_metadata[\n",
    "                                                    bm_i*mcol_k*{bm}/{mrow_m} +\n",
    "                                                    mbrow_i*mcol_k*{mbrow}/{mrow_m} +\n",
    "                                                    mbrow_i2*{brow_fixed}/{mrow_m} +\n",
    "                                                    brow_i*{brow}/{mrow_m}  +\n",
    "                                                    mcol_i*{mbrow}/{mrow_m} +\n",
    "                                                    mbrow_ii]) >> (mbrow_iii*({nelems}/{mrow_m})*{bits_elem_meta}+mcol_ii*{n}*{bits_elem_meta}+n_i*{bits_elem_meta})) & 0x3);\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "\n",
    "                                    for(int mcol_ii=0; mcol_ii<mcol_kk; mcol_ii++){{\n",
    "                                        for(int mbrow_iii=0; mbrow_iii<{mrow_m}; mbrow_iii++){{\n",
    "                                            for(int n_i=0; n_i<{n}; n_i++){{\n",
    "                                                unsigned int index = columns[mcol_ii*{m_fixed} + indexes[mcol_ii*{mrow_m}*{n}+mbrow_iii*{n}+n_i]];\n",
    "\n",
    "                                                if((mcol_i*{m}*mcol_kk + mcol_ii*{m} + index) < {ncols}){{\n",
    "                                                    hA_dense[\n",
    "                                                        bm_i*{bm}*{ncols} +\n",
    "                                                        mbrow_i*{mbrow}*{ncols} +\n",
    "                                                        mbrow_i2*{brow_fixed}*{ncols} +\n",
    "                                                        brow_i*{brow}*{ncols} +\n",
    "                                                        mcol_i*{m}*mcol_kk +\n",
    "                                                        mbrow_ii*{mrow_m}*{ncols} +\n",
    "                                                        mcol_ii*{m} +\n",
    "                                                        mbrow_iii*{ncols} +\n",
    "                                                        index] =\n",
    "                                                    hA_values[\n",
    "                                                        bm_i*{bm}*{A_num_cols_sp_pad} +\n",
    "                                                        mbrow_i*{mbrow}*{A_num_cols_sp_pad}+\n",
    "                                                        mbrow_i2*{brow_fixed}*{A_num_cols_sp_pad}+\n",
    "                                                        brow_i*{brow}*{nelems}/{mrow_m}+\n",
    "                                                        mcol_i*{brow_fixed}*{nelems}/{mrow_m} +\n",
    "                                                        mbrow_ii*{mrow_m}*{n} +\n",
    "                                                        mcol_ii*{n}*{brow} +\n",
    "                                                        mbrow_iii*{n} +\n",
    "                                                        n_i];\n",
    "                                                }}\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }}\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\",\n",
    "    )\n",
    "    lib.func3.argtypes = [\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "    ]\n",
    "    return lib.func3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Dense to VENOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def to_sparse_sr_nm(dense_shape, dense_dtype, n, m, tileM):\n",
    "    nrows = dense_shape[0]\n",
    "    ncols = dense_shape[1]\n",
    "\n",
    "    brow = 4 #this->brow = brow_;\n",
    "    mbrow = 32 #this->mbrow = mbrow_;\n",
    "\n",
    "    bm   = tileM\n",
    "    # !IMPORTANT! constants because of architecture constraints\n",
    "    m_fixed = 4\n",
    "    bits_elem_meta=2\n",
    "    mrow_m = 2\n",
    "    bits_elem_cols=8\n",
    "    brow_fixed = 16\n",
    "    nelems=32//bits_elem_meta #(sizeof(uint)*8)=32\n",
    "    nelems_col = nelems//mrow_m\n",
    "\n",
    "    A_num_cols_sp = (ncols//m)*n\n",
    "    A_num_cols_sp_pad_nm = (round_up(ncols, m)/m)*n\n",
    "    A_num_cols_sp_pad = round_up((round_up(ncols, m)/m)*n, 16)\n",
    "    A_nnz = nrows*A_num_cols_sp_pad\n",
    "\n",
    "    assert dense_dtype in (torch.float32, torch.float64)\n",
    "    dtype = \"float\" if dense_dtype == torch.float32 else \"double\"\n",
    "    lib = compile(\n",
    "        f\"\"\"\n",
    "        #include <iostream>\n",
    "        #include <algorithm>\n",
    "        #include <utility>\n",
    "        #include <cstdlib>\n",
    "        #include <cstdio>\n",
    "        #include <cmath>\n",
    "        #include <functional>\n",
    "        #include <tuple>\n",
    "        #include <vector>\n",
    "        #include <numeric>\n",
    "        #include <chrono>\n",
    "\n",
    "        using namespace std;\n",
    "\n",
    "\n",
    "        extern \"C\" void func2({dtype}* sparse, int* masks, {dtype}* hA_values, int *hA_columns, int *hA_metadata){{\n",
    "\n",
    "            int bm_m = {nrows}/{bm};\n",
    "            int mbrow_m = {bm}/{mbrow};\n",
    "            int mbrow_m2 = {mbrow}/{brow_fixed};\n",
    "            int brow_m = {brow_fixed}/{brow};\n",
    "            // metadata\n",
    "            int mcol_kk = {nelems}/{mrow_m}/{n};\n",
    "            int mcol_k = {A_num_cols_sp_pad}/{n}/mcol_kk;\n",
    "            // indices\n",
    "            int col_kk = mcol_kk;\n",
    "            int col_k = {A_num_cols_sp_pad}/{n}/col_kk;\n",
    "\n",
    "            {dtype} values[{nelems}];\n",
    "            uint indexes[{nelems}];\n",
    "            uint columns[col_kk*{m_fixed}];\n",
    "\n",
    "            int max_idx = 0;\n",
    "\n",
    "            for(int bm_i=0; bm_i<bm_m; bm_i++){{\n",
    "                for(int mbrow_i=0; mbrow_i<mbrow_m; mbrow_i++){{\n",
    "                    for(int mbrow_i2=0; mbrow_i2<mbrow_m2; mbrow_i2++){{\n",
    "                        for(int brow_i=0; brow_i<brow_m; brow_i++){{\n",
    "                            for(int mcol_i=0; mcol_i<mcol_k; mcol_i++){{\n",
    "                                for(int col_i=0; col_i<col_kk; col_i++){{\n",
    "                                    for(int col_ii=0; col_ii<{m_fixed}; col_ii++){{\n",
    "                                        columns[col_i*{m_fixed} + col_ii] =\n",
    "                                        hA_columns[bm_i*col_k*col_kk*{m_fixed} + mcol_i*col_kk*{m_fixed} + col_i*{m_fixed} + col_ii];\n",
    "                                    }}\n",
    "                                }}\n",
    "                                for(int mbrow_ii=0; mbrow_ii<({brow}/{mrow_m}); mbrow_ii++){{\n",
    "                                    for(int mcol_ii=0; mcol_ii<mcol_kk; mcol_ii++){{\n",
    "                                        for(int mbrow_iii=0; mbrow_iii<{mrow_m}; mbrow_iii++){{\n",
    "                                            int pos=0;\n",
    "                                            for(int n_i=0; n_i<{m_fixed}; n_i++){{\n",
    "                                                unsigned int index = columns[mcol_ii*{m_fixed} + n_i];\n",
    "\n",
    "                                                if((mcol_i*{m}*mcol_kk + mcol_ii*{m} + index) < {ncols}){{\n",
    "                                                    int nnz = masks[\n",
    "                                                            bm_i*{bm}*{ncols} +\n",
    "                                                            mbrow_i*{mbrow}*{ncols} +\n",
    "                                                            mbrow_i2*{brow_fixed}*{ncols} +\n",
    "                                                            brow_i*{brow}*{ncols} +\n",
    "                                                            mcol_i*{m}*mcol_kk +\n",
    "                                                            mbrow_ii*{mrow_m}*{ncols} +\n",
    "                                                            mcol_ii*{m} +\n",
    "                                                            mbrow_iii*{ncols} +\n",
    "                                                            index];\n",
    "\n",
    "                                                    if(nnz != 0){{\n",
    "                                                        indexes[\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            pos] = n_i;\n",
    "\n",
    "                                                        values[\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            pos] =\n",
    "                                                        sparse[\n",
    "                                                            bm_i*{bm}*{ncols} +\n",
    "                                                            mbrow_i*{mbrow}*{ncols} +\n",
    "                                                            mbrow_i2*{brow_fixed}*{ncols} +\n",
    "                                                            brow_i*{brow}*{ncols} +\n",
    "                                                            mcol_i*{m}*mcol_kk +\n",
    "                                                            mbrow_ii*{mrow_m}*{ncols} +\n",
    "                                                            mcol_ii*{m} +\n",
    "                                                            mbrow_iii*{ncols} +\n",
    "                                                            index];\n",
    "\n",
    "                                                        pos+=1;\n",
    "                                                    }}\n",
    "                                                }} else {{\n",
    "                                                    if(n_i<2){{\n",
    "                                                        indexes[\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            pos] = 0;\n",
    "\n",
    "                                                        values[\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            pos] = 0;\n",
    "\n",
    "                                                        pos+=1;\n",
    "                                                    }}\n",
    "                                                }}\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                    // write metadata\n",
    "                                    unsigned int meta=0;\n",
    "                                    for(int mbrow_iii=0; mbrow_iii<{mrow_m}; mbrow_iii++){{\n",
    "                                        for(int mcol_ii=0; mcol_ii<mcol_kk; mcol_ii++){{\n",
    "                                            for (int n_i=0; n_i<{n}; n_i++) {{\n",
    "\n",
    "                                                int idx = bm_i*{bm}*{A_num_cols_sp_pad} +\n",
    "                                                        mbrow_i*{mbrow}*{A_num_cols_sp_pad}+\n",
    "                                                        mbrow_i2*{brow_fixed}*{A_num_cols_sp_pad}+\n",
    "                                                        brow_i*{brow}*{nelems}/{mrow_m}+\n",
    "                                                        mcol_i*{brow_fixed}*{nelems}/{mrow_m} +\n",
    "                                                        mbrow_ii*{mrow_m}*{n} +\n",
    "                                                        mcol_ii*{n}*{brow} +\n",
    "                                                        mbrow_iii*{n} +\n",
    "                                                        n_i;\n",
    "\n",
    "                                                max_idx = (idx>max_idx)?(idx):(max_idx);\n",
    "\n",
    "                                                hA_values[\n",
    "                                                        idx] =\n",
    "                                                values[\n",
    "                                                    mcol_ii*{mrow_m}*{n} +\n",
    "                                                    mbrow_iii*{n} +\n",
    "                                                    n_i];\n",
    "\n",
    "                                                unsigned int tmp = indexes[\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            n_i];\n",
    "                                                meta |= (tmp << (mbrow_iii*({nelems}/{mrow_m})*{bits_elem_meta}+mcol_ii*{n}*{bits_elem_meta}+n_i*{bits_elem_meta}));\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                    hA_metadata[bm_i*mcol_k*{bm}/{mrow_m} +\n",
    "                                                mbrow_i*mcol_k*{mbrow}/{mrow_m} +\n",
    "                                                mbrow_i2*{brow_fixed}/{mrow_m} +\n",
    "                                                brow_i*{brow}/{mrow_m}  +\n",
    "                                                mcol_i*{mbrow}/{mrow_m} +\n",
    "                                                mbrow_ii] = meta;\n",
    "                                }}\n",
    "                            }}\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "            cout << \"max_idx: \" << max_idx << endl;\n",
    "        }}\n",
    "        \"\"\",\n",
    "    )\n",
    "    lib.func2.argtypes = [\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "    ]\n",
    "    return lib.func2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*************************************************/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(x,y):\n",
    "    return math.ceil(x/y)*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Input definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(64, 64, requires_grad=True)\n",
    "weights = torch.randn(64, 64, requires_grad=True)\n",
    "#b = torch.ones(6print_size, 64, requires_grad=True)\n",
    "mask = torch.ones(64, 64, requires_grad=True)\n",
    "grad_d = torch.randn(64, 64)\n",
    "# Known tensors to check operations\n",
    "reference_tensor = torch.ones(64, 64)\n",
    "#for index in range(0, 256*256):\n",
    "#    reference_tensor[index//256][index%256] = index\n",
    "\n",
    "#print(reference_tensor)\n",
    "result_forward_dense = torch.mm(torch.add(input, weights), mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#STen dense2sparse + sparse2dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseVNMTensor:\n",
    "    def __init__(self, v_, n_, m_,  dense_ = None, mask_ = None, columns_ = None, values_ = None, metadata_ = None):\n",
    "        self.v = v_\n",
    "        self.n = n_\n",
    "        self.m = m_\n",
    "        self.nnz = 0\n",
    "        if (dense_ is not None):\n",
    "            self.nrows, self.ncols = dense_.shape\n",
    "        elif (mask_ is not None):\n",
    "            self.nrows, self.ncols = mask_.shape\n",
    "        else:\n",
    "            self.nrows = None\n",
    "            self.ncols = None\n",
    "        #self.dense = dense_\n",
    "        self.mask = mask_\n",
    "        self.columns = columns_        \n",
    "        #self.data = None\n",
    "        if (values_ is None):\n",
    "            self.values = None\n",
    "        elif( values_.dtype == torch.float16):\n",
    "            self.values = values_.float()\n",
    "        else:\n",
    "            self.values = values_\n",
    "        #self.values = None\n",
    "        self.metadata = metadata_\n",
    "        if (dense_ is not None and mask_ is not None):\n",
    "            # Compress only if dense tensor was provided.\n",
    "            self.to_sparse_sr_nm(dense_, mask_) \n",
    "            # Otherwise already compressed data was provided.\n",
    "            \n",
    "        #self.dense = None\n",
    "        \n",
    "    \n",
    "    def to_sparse_sr_nm(self, dense_, mask_):\n",
    "        impl_builder = (\n",
    "            to_sparse_sr_nm\n",
    "            )\n",
    "        func = impl_builder(\n",
    "                dense_.shape,\n",
    "                dense_.dtype,\n",
    "                self.n,\n",
    "                self.m,\n",
    "                self.v\n",
    "            )\n",
    "        \n",
    "        self.nrows, self.ncols = dense_.shape\n",
    "        A_num_cols_sp_pad = round_up((round_up(self.ncols, self.m)/self.m)*self.n, 16)\n",
    "        self.nnz = self.nrows*A_num_cols_sp_pad\n",
    "        m_fixed = 4\n",
    "        mrow_m = 2\n",
    "        bits_elem_meta=2\n",
    "\n",
    "        nelems = 32//bits_elem_meta #32=(sizeof(uint)*8)\n",
    "        nelems_col = nelems//mrow_m\n",
    "\n",
    "        self.values = torch.zeros(self.nrows * A_num_cols_sp_pad, dtype=dense_.dtype)\n",
    "        self.metadata = torch.zeros(self.nrows//mrow_m * A_num_cols_sp_pad//nelems_col, dtype=torch.int32)\n",
    "        self.mask = mask_\n",
    "\n",
    "        func(dense_.data_ptr(), mask_.data_ptr(), self.values.data_ptr(), self.columns.data_ptr(), self.metadata.data_ptr())     \n",
    "\n",
    "    def to_dense(self):\n",
    "        impl_builder = (\n",
    "            to_dense\n",
    "            )\n",
    "        func = impl_builder(\n",
    "                (self.nrows, self.ncols),\n",
    "                torch.float32, #self.values.dtype,\n",
    "                self.n,\n",
    "                self.m,\n",
    "                self.v\n",
    "            )\n",
    "        # initialize with ones\n",
    "        #dense = torch.ones((self.nrows, self.ncols), dtype=torch.float32, device='cpu') #self.values.dtype\n",
    "        self.values=self.values.cpu()\n",
    "        # uncomment to keep initial values\n",
    "        #func(dense.data_ptr(), self.values.cpu().to(dtype=torch.float32).data_ptr(), self.columns.cpu().data_ptr(), self.metadata.cpu().data_ptr())\n",
    "        dense = torch.zeros((self.nrows, self.ncols), dtype=self.values.dtype, device=\"cpu\")\n",
    "        #print(\"Dense device\", dense.device, \"values device\", self.values.device, \"columns device\", self.columns.device, \"metadata device\", self.metadata.device)\n",
    "        #print(\"Dense shape\", dense.shape, \"values shape\", self.values.shape, \"columns shape\", self.columns.shape, \"metadata shape\", self.metadata.shape)\n",
    "        \"\"\" print(\"to_dense\", dense.dtype, self.values.dtype, self.columns.dtype, self.metadata.dtype)\n",
    "        print(\"to_dense\", dense.device, self.values.device, self.columns.device, self.metadata.device) \"\"\"\n",
    "        func(dense.data_ptr(), self.values.data_ptr(), self.columns.data_ptr(), self.metadata.data_ptr())\n",
    "\n",
    "        return dense.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#VENOM Sparsifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMVectorSparsifier:\n",
    "    def __init__(self, n, m, v):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.v = v\n",
    "\n",
    "    @staticmethod\n",
    "    def get_random_mask(tensor, m, v):\n",
    "        mask = torch.zeros(tensor.shape, dtype=tensor.dtype)\n",
    "        m_tmp = torch.cat( (torch.tensor([1,0,1,0]), torch.zeros(m-4)), 0 )\n",
    "        mask = mask.reshape(-1, v, m) + m_tmp\n",
    "        mask = mask.reshape(tensor.shape)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def __call__(self, tensor, grad_fmt=None):\n",
    "        # random pruning (cuSparseLt-like approach) -> mask, columns\n",
    "        nrows, ncols = tensor.shape\n",
    "        columns = torch.zeros(nrows//self.v, ncols//self.m*4, dtype=torch.int32)\n",
    "        columns = columns.reshape((-1,4)) + torch.tensor([0,1,2,3], dtype=torch.int32)\n",
    "        columns = columns.reshape((nrows//self.v, ncols//self.m*4))\n",
    "\n",
    "        mask = NMVectorSparsifier.get_random_mask(tensor, self.m, self.v)\n",
    "\n",
    "        sparse_mtx = sten.SparseTensorWrapper.wrapped_from_dense(\n",
    "            SparseVNMTensor(self.n, self.m, self.v, tensor, mask, columns, tensor.device),\n",
    "            tensor,\n",
    "            grad_fmt,\n",
    "        )\n",
    "\n",
    "        return sparse_mtx\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pruning function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nm_vector_mask_sparsify(tensor, v, n, m):\n",
    "    #print(\"nm_vector_mask_sparsify\", v, n, m)\n",
    "    \n",
    "\n",
    "    impl_builder = (\n",
    "                group_n_m2\n",
    "                )\n",
    "    nrows, ncols = tensor.shape\n",
    "    A_num_cols_sp_pad = round_up((round_up(ncols, m)/m)*n, 16)            \n",
    "    bm_m   = nrows//v\n",
    "    mcol_k_p = math.ceil(ncols/m)\n",
    "    m_fixed = 4\n",
    "    \n",
    "    # Structures represent sparse data\n",
    "    masks = torch.zeros(tensor.shape, dtype=torch.int32)    \n",
    "    columns = torch.zeros(nrows//v * A_num_cols_sp_pad//n*m_fixed,dtype=torch.int32)\n",
    "    \n",
    "    if len(tensor.shape) == 2:\n",
    "        tensor_temp = tensor.cpu().detach().abs()\n",
    "        sparse = tensor_temp.clone()        \n",
    "\n",
    "        func = impl_builder(\n",
    "                    tensor_temp.shape,\n",
    "                    tensor_temp.dtype,\n",
    "                    n,\n",
    "                    m,\n",
    "                    v\n",
    "                )\n",
    "        func(tensor_temp.data_ptr(), sparse.data_ptr(), masks.data_ptr(), columns.data_ptr())\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Only support layers of dimension 2 or 4\")\n",
    "\n",
    "    return masks, columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sten sparsifier definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sten.register_sparsifier_implementation(\n",
    "    sparsifier=NMVectorSparsifier, inp=torch.Tensor, out=SparseVNMTensor\n",
    ")\n",
    "def torch_tensor_to_srnm_random_fraction(sparsifier, tensor, grad_fmt=None):\n",
    "    #print(\"inside NMVectorSparsifier sparsifier\")\n",
    "    #print(tensor.dtype)\n",
    "    masks, columns = nm_vector_mask_sparsify(tensor, sparsifier.v, sparsifier.n, sparsifier.m)\n",
    "    return sten.SparseTensorWrapper.wrapped_from_dense(\n",
    "        SparseVNMTensor(sparsifier.v, sparsifier.n, sparsifier.m, dense_=tensor, mask_=masks, columns_=columns),\n",
    "        tensor,\n",
    "        grad_fmt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAutograd(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MMWithCustomGrad(torch.nn.Module):\n",
    "#    def __init__(self, original: torch.nn.Linear, v:int, n:int, m:int):\n",
    "#        super().__init__()\n",
    "#        #self.weights = torch.nn.Parameter(torch.rand(10))\n",
    "#        self.bias = original.bias\n",
    "#        self._v = v\n",
    "#        self._n = n\n",
    "#        self._m = m\n",
    "#        self.weigth = original.weight\n",
    "#        #self.bias = torch.zeros(original.bias.shape, dtype=original.bias.dtype, device=original.bias.device)\n",
    "#\n",
    "#        # Convert weights from original module to SrNM\n",
    "#        w = VenomSparsifier(n, m, v)(original.weight).wrapped_tensor\n",
    "#\n",
    "#        self.weight_values = torch.nn.Parameter(w.values)\n",
    "#        self.weight_columns = w.columns\n",
    "#        self.weight_metadata = w.metadata\n",
    "#\n",
    "#        self.nrows_sp = w.nrows\n",
    "#        self.ncols_sp = w.ncols\n",
    "#        self.nnz      = w.nnz\n",
    "#        \n",
    "#        \n",
    "#        self.fn = VenomSpmmGrad.apply\n",
    "#\n",
    "#    def forward(self, x):\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/*********************/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_test = False\n",
    "\n",
    "v=32; n=2; m=8\n",
    "BM=32\n",
    "BN=32\n",
    "BK=32\n",
    "WM=32\n",
    "WN=32\n",
    "WK=32\n",
    "MM=16\n",
    "MN=8\n",
    "MK=32\n",
    "NSTAGE=2\n",
    "\n",
    "\n",
    "if tiny_test:\n",
    "    tiny_test_A = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "    tiny_test_B = torch.tensor([[5., 6.], [7., 8.]], requires_grad=True)\n",
    "\n",
    "    # A con 0s, dispersa, pero en expresión densa (0s polo medio)\n",
    "    print(\"Tiny A: Shape:\", tiny_test_A.shape, \"\\n\", tiny_test_A, \"\\nGrad:\", tiny_test_A.grad)\n",
    "    print(\"Tiny B: Shape:\", tiny_test_B.shape, \"\\n\", tiny_test_B, \"\\nGrad:\", tiny_test_B.grad)\n",
    "    tiny_mm_output = torch.mm(tiny_test_A, tiny_test_B)\n",
    "    print(\"Tiny A · Tiny B: shape\", tiny_mm_output.shape, \"\\n\", tiny_mm_output)\n",
    "    tiny_mm_output.retain_grad()\n",
    "    tiny_loss = torch.sum(tiny_mm_output); \n",
    "    #print(\"Tiny loss:\", tiny_loss)\n",
    "    print(\"tiny_mm_output.grad:\", tiny_mm_output.grad)\n",
    "    tiny_loss.backward()\n",
    "    #tiny_mm_output.backward()\n",
    "    # Printear matrices A, B, e loss, a ver cal cambia\n",
    "    print(\"Tiny A: Shape:\", tiny_test_A.shape, \"\\n\", tiny_test_A, \"\\nGrad:\", tiny_test_A.grad)\n",
    "    print(\"Tiny B: Shape:\", tiny_test_B.shape, \"\\n\", tiny_test_B, \"\\nGrad:\", tiny_test_B.grad)\n",
    "    print(\"tiny_mm_output:\", tiny_mm_output)\n",
    "    print(\"tiny_mm_output.grad:\", tiny_mm_output.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_idx: 511\n",
      "max_idx: 511\n",
      "Custom forward completed. Output: tensor([[ 108160.,  108308.,  108456.,  ...,  112452.,  112600.,  112748.],\n",
      "        [ 259712.,  260116.,  260520.,  ...,  271428.,  271832.,  272236.],\n",
      "        [ 411264.,  411924.,  412584.,  ...,  430404.,  431064.,  431724.],\n",
      "        ...,\n",
      "        [4503168., 4510740., 4518312.,  ..., 4722756., 4730328., 4737900.],\n",
      "        [4654720., 4662548., 4670376.,  ..., 4881732., 4889560., 4897388.],\n",
      "        [4806272., 4814356., 4822440.,  ..., 5040708., 5048792., 5056876.]],\n",
      "       grad_fn=<SparseOperatorDispatcherBackward>)\n",
      "Backward on custom sten torch.mm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m small_sten_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(small_sten_mm_output); \n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m#loss.backwards() # Co backwards disperso\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[43msmall_sten_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Comparar resultados.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA operands to the torch.mm are equal?\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mallclose(masked_small_test_A, masked_small_sten_test_A))\n",
      "File \u001b[0;32m~/inno4scale_shared/anaconda3/envs/end2end/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/inno4scale_shared/anaconda3/envs/end2end/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/inno4scale_shared/anaconda3/envs/end2end/lib/python3.10/site-packages/torch/autograd/function.py:267\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/inno4scale_shared/anaconda3/envs/end2end/lib/python3.10/site-packages/sten/sten.py:1502\u001b[0m, in \u001b[0;36mSparseOperatorDispatcher.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1500\u001b[0m                 op_impl_bwd \u001b[38;5;241m=\u001b[39m create_failing_bwd_impl(e)\n\u001b[0;32m-> 1502\u001b[0m tmp_grad_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mop_impl_bwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mginp_sp1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m tmp_grad_inputs \u001b[38;5;241m=\u001b[39m canonicalize_tensor_tuple(tmp_grad_inputs)\n\u001b[1;32m   1504\u001b[0m check_formats(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctx\u001b[38;5;241m.\u001b[39mdisp_state\u001b[38;5;241m.\u001b[39morig_op\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (bwd)\u001b[39m\u001b[38;5;124m\"\u001b[39m, tmp_grad_inputs, ginp_fmt1)\n",
      "Cell \u001b[0;32mIn[37], line 62\u001b[0m, in \u001b[0;36mtorch_mm_bwd_impl\u001b[0;34m(ctx, grad_outputs, input_sparsifiers)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackward on custom sten torch.mm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m [grad_output] \u001b[38;5;241m=\u001b[39m grad_outputs\n\u001b[0;32m---> 62\u001b[0m A_operand, B_operand \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39msaved_tensors\n\u001b[1;32m     64\u001b[0m grad_A \u001b[38;5;241m=\u001b[39m grad_output \u001b[38;5;241m@\u001b[39m B_operand\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     65\u001b[0m grad_B \u001b[38;5;241m=\u001b[39m A_operand\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m grad_output\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "small_sten_test = True\n",
    "print_size = 16\n",
    "if small_sten_test:\n",
    "    #import numpy as np\n",
    "    # Definir matrices que funcionen con SrNMTensors. Mínimo de 32 por dimension\n",
    "    small_test_A = torch.arange(32 * 32).reshape(32, 32).float().requires_grad_()\n",
    "    A_masks, A_columns = nm_vector_mask_sparsify(small_test_A, v, n, m)\n",
    "    sparse_small_test_A = SparseVNMTensor(v, n, m, dense_=small_test_A, mask_=A_masks, columns_=A_columns)\n",
    "    masked_small_test_A = sparse_small_test_A.to_dense().float().requires_grad_()\n",
    "    #print(\"sparse_small_test_A: \", sparse_small_test_A)\n",
    "    small_test_B = torch.arange(32 * 32).reshape(32, 32).float().requires_grad_()\n",
    "    # Run normal torch.mm to get correct results.\n",
    "    #small_mm_output = torch.mm(small_test_A, small_test_B)\n",
    "    small_mm_output = torch.mm(masked_small_test_A, small_test_B)\n",
    "    #print(\"MM result: Shape:\", small_mm_output.shape, \"\\n\", small_mm_output[:print_size, :print_size])\n",
    "    # Run backwards, clear gradients just in case it has something stored\n",
    "    masked_small_test_A.grad = None\n",
    "    small_test_B.grad = None\n",
    "    small_loss = torch.sum(small_mm_output); \n",
    "    small_loss.backward()\n",
    "    #print(\"Small A: Shape:\", masked_small_test_A.shape, \"\\n\", masked_small_test_A[:print_size, :print_size], \"\\nGrad:\", None if masked_small_test_A.grad is None else masked_small_test_A.grad[:print_size, :print_size])\n",
    "    #print(\"Small B: Shape:\", small_test_B.shape, \"\\n\", small_test_B[:print_size, :print_size], \"\\nGrad:\", None if small_test_B.grad is None else small_test_B.grad[:print_size, :print_size])\n",
    "    \n",
    "    custom_mm = sten.sparsified_op(\n",
    "        orig_op=torch.mm,\n",
    "        out_fmt=(\n",
    "            (sten.KeepAll(), torch.Tensor,\n",
    "            sten.KeepAll(), torch.Tensor),\n",
    "        ),\n",
    "        grad_out_fmt=(\n",
    "            (sten.KeepAll(), torch.Tensor,\n",
    "            sten.KeepAll(), torch.Tensor),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    #Aplicar sten, definir forward/backwards que faga o que corresponda \n",
    "    @sten.register_fwd_op_impl(\n",
    "        operator=custom_mm,\n",
    "        inp=(torch.Tensor, torch.Tensor),\n",
    "        out=[(sten.KeepAll, torch.Tensor)],\n",
    "    )\n",
    "    def torch_mm_fwd_impl(ctx, inputs, output_sparsifiers):\n",
    "        A_operand_sparse, B_operand = inputs\n",
    "        print(\"Forward on custom sten torch.mm\")\n",
    "        ctx.save_for_backward(A_operand_sparse, B_operand)\n",
    "        #ctx.save_for_backward(input_matrix, weights, bias)\n",
    "        \n",
    "        return A_operand_sparse.to_dense() @ B_operand\n",
    "    \n",
    "    @sten.register_bwd_op_impl(\n",
    "        operator=custom_mm,\n",
    "        grad_out=[torch.Tensor],\n",
    "        grad_inp=(\n",
    "            (sten.KeepAll, torch.Tensor),\n",
    "            (sten.KeepAll, torch.Tensor),\n",
    "        ),\n",
    "        inp=(torch.Tensor, torch.Tensor ),\n",
    "    )\n",
    "    def torch_mm_bwd_impl(ctx, grad_outputs, input_sparsifiers):\n",
    "        print(\"Backward on custom sten torch.mm\")\n",
    "        [grad_output] = grad_outputs\n",
    "        A_operand, B_operand = ctx.saved_tensors\n",
    "        \n",
    "        grad_A = grad_output @ B_operand.T\n",
    "        grad_B = A_operand.T @ grad_output\n",
    "        \n",
    "        #print(\"grad_A: Shape:\", grad_A.shape, \"\\n\", grad_A[:print_size, :print_size])\n",
    "        #print(\"grad_B: Shape:\", grad_B.shape, \"\\n\", grad_B[:print_size, :print_size])\n",
    "        return grad_A, grad_B\n",
    "    # con algebra densa (operador @)\n",
    "    #....\n",
    "    #torch_mm_output = torch.mm(A, B)# Con operación forward disperso\n",
    "    small_sten_test_A = torch.arange(32 * 32).reshape(32, 32).float().requires_grad_()\n",
    "    sparse_sten_small_test_A = SparseVNMTensor(v, n, m, dense_=small_sten_test_A, mask_=A_masks, columns_=A_columns)\n",
    "    small_sten_test_B = torch.arange(32 * 32).reshape(32, 32).float().requires_grad_()\n",
    "    masked_small_sten_test_A = sparse_sten_small_test_A.to_dense().float().requires_grad_()\n",
    "    small_sten_mm_output = custom_mm(masked_small_sten_test_A, small_sten_test_B)\n",
    "    print(\"Custom forward completed.\\nOutput:\", small_sten_mm_output)\n",
    "    # Run backwards, clear gradients just in case it has something stored\n",
    "    masked_small_sten_test_A.grad = None\n",
    "    small_sten_test_B.grad = None \n",
    "    small_sten_loss = torch.sum(small_sten_mm_output); \n",
    "    #loss.backwards() # Co backwards disperso\n",
    "    small_sten_loss.backward()\n",
    "    # Comparar resultados.\n",
    "    print(\"\\nA operands to the torch.mm are equal?\", torch.allclose(masked_small_test_A, masked_small_sten_test_A))\n",
    "    #print(\"masked_small_test_A corner:\\n\", masked_small_test_A[:print_size, :print_size])\n",
    "    #print(\"masked_small_sten_test_A corner:\\n\", masked_small_sten_test_A[:print_size, :print_size])\n",
    "    \n",
    "    print(\"B operands to the torch.mm are equal?\",torch.allclose(small_test_B, small_sten_test_B) )\n",
    "    #print(\"small_test_B corner:\\n\", small_test_B[:print_size, :print_size])\n",
    "    #print(\"small_sten_test_B corner:\\n\", small_sten_test_B[:print_size, :print_size])\n",
    "    \n",
    "    print(\"Torch.mm results are equal?\", torch.allclose(small_mm_output, small_sten_mm_output))\n",
    "    #print(\"Result of unmodified torch.mm: Shape:\", small_mm_output.shape, \"\\n\", small_mm_output[:print_size, :print_size])\n",
    "    #print(\"Result of modified torch.mm: Shape:\", small_sten_mm_output.shape, \"\\n\", small_sten_mm_output[:print_size, :print_size])\n",
    "    \n",
    "    print(\"Gradients of the A operand are equal?\", torch.allclose(masked_small_test_A.grad, masked_small_sten_test_A.grad))\n",
    "    #print(\"\\nGradients of the A operand to the unmodified torch.mm (corner only):\\n\", None if masked_small_test_A.grad is None else masked_small_test_A.grad[:print_size, :print_size])\n",
    "    #print(\"\\nGradients of the A operand to the modified torch.mm  (corner only):\\n\", None if masked_small_sten_test_A.grad is None else masked_small_sten_test_A.grad[:print_size, :print_size])\n",
    "    \n",
    "    print(\"Gradients of the B operand are equal?\", torch.allclose(small_test_B.grad, small_sten_test_B.grad))\n",
    "    #print(\"\\nGradients of the B operand to the unmodified torch.mm (corner only):\\n\", None if small_test_B.grad is None else small_test_B.grad[:print_size, :print_size])\n",
    "    #print(\"\\nGradients of the B operand to the modified torch.mm (corner only):\\n\", None if small_sten_test_B.grad is None else small_sten_test_B.grad[:print_size, :print_size])\n",
    "    \n",
    "    \n",
    "    # Probar hasta que coincidan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sparse_add = sten.sparsified_op(\n",
    "    orig_op=torch.add,\n",
    "    out_fmt=(\n",
    "        (sten.KeepAll(), torch.Tensor,\n",
    "         NMVectorSparsifier(v,n,m), SparseVNMTensor),\n",
    "    ),\n",
    "    grad_out_fmt=(\n",
    "        (sten.KeepAll(), torch.Tensor,\n",
    "         NMVectorSparsifier(v,n,m), SparseVNMTensor),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_idx: 0\n"
     ]
    }
   ],
   "source": [
    "sparse_tensor = sparse_add(input, weights)\n",
    "#print(sparse_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity checking helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_VNM(v:int, n:int, m:int, m_fixed:int, tensor: torch.Tensor, verbose:bool=False):\n",
    "    shape = tensor.shape\n",
    "\n",
    "    blocks = (shape[0]/v)*(shape[1]/m)\n",
    "    ok_blocks = 0\n",
    "    invalid_blocks = 0\n",
    "\n",
    "    for row_block in range(0, shape[0], v):\n",
    "        for column_block in range(0, shape[1], m):\n",
    "            #print(f'Block {row_block}:{row_block+v}, {column_block}:{column_block+m}: ', end=\"\")\n",
    "            any_row_invalid = False\n",
    "            non_zero_columns = []\n",
    "            invalid_rows = 0\n",
    "            for row in range(row_block, row_block+v):\n",
    "                non_zero_position = 0\n",
    "                empty_positions = 0\n",
    "                for column in range(column_block, column_block+m):\n",
    "                    if row < shape[0] and column < shape[1]:\n",
    "                        if tensor[row][column] == 0:\n",
    "                            empty_positions+=1\n",
    "                        else:\n",
    "                            non_zero_position += 1\n",
    "                            if column not in non_zero_columns:\n",
    "                                non_zero_columns.append(column)\n",
    "                if non_zero_position > n:\n",
    "                    any_row_invalid = True\n",
    "                    invalid_rows += 1\n",
    "            if invalid_rows > 0:\n",
    "                print(f'\\t{invalid_rows} rows with  more than {n} non zero positions,')\n",
    "\n",
    "            #print(f'{len(non_zero_columns)} columns with some non zero element')\n",
    "            if len(non_zero_columns) > m_fixed:\n",
    "                any_row_invalid = True\n",
    "                print(f'\\tBlock has {len(non_zero_columns)} columns with some non zero element')\n",
    "                print(f'\\tError: more than the {m_fixed} columns with some non zero elements allowed.')\n",
    "\n",
    "            if any_row_invalid:\n",
    "                invalid_blocks+=1\n",
    "                print(f'\\tBlock {row_block}:{row_block+v}, {column_block}:{column_block+m} does not meet requirements')\n",
    "            else:\n",
    "                #print(f'\\tBlock {row_block}:{row_block+64}, {column_block}:{column_block+10} meets requirements')\n",
    "                ok_blocks+=1\n",
    "            if verbose: print(f'{ok_blocks+invalid_blocks}/{blocks} processed blocks. {ok_blocks} valid ones, {invalid_blocks} invalid. Processing block {row_block}:{row_block+v}, {column_block}:{column_block+m}                                             ', end=\"\\r\")\n",
    "    if not verbose: print(f'{ok_blocks+invalid_blocks}/{blocks} processed blocks. {ok_blocks} valid ones, {invalid_blocks} invalid.')\n",
    "    print(\"\")\n",
    "\n",
    "def check_sparsity(tensor: torch.Tensor):\n",
    "\n",
    "    zero_elements = 0\n",
    "    non_zero_elements = 0\n",
    "    total_elements = 0\n",
    "\n",
    "    for row in range(tensor.shape[0]):\n",
    "        for column in range(tensor.shape[1]):\n",
    "            total_elements+=1\n",
    "            if tensor[row][column] == 0:\n",
    "                zero_elements+=1\n",
    "            else:\n",
    "                non_zero_elements+=1\n",
    "\n",
    "    print(f'Tensor calculated sparsity: {zero_elements/total_elements}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_idx: 0\n"
     ]
    }
   ],
   "source": [
    "@sten.register_fwd_op_impl(\n",
    "    operator=torch.mm,\n",
    "    inp=(SparseVNMTensor, torch.Tensor),\n",
    "    out=[(sten.KeepAll, torch.Tensor)],\n",
    ")\n",
    "def sparse_torch_add_fwd_impl(ctx, inputs, output_sparsifiers):\n",
    "    weights, input_matrix = inputs\n",
    "    #ctx.save_for_backward(weights, input_matrix)\n",
    "\n",
    "    bias = torch.ones(weights.wrapped_tensor.nrows)*2\n",
    "    \n",
    "    ctx.save_for_backward(input_matrix, weights, bias)\n",
    "    #ctx.save_for_backward(input_matrix, \n",
    "    #                      weights.wrapped_tensor.values.to(dtype=torch.half).cuda(), \n",
    "    #                      weights.wrapped_tensor.columns.cuda(), \n",
    "    #                      weights.wrapped_tensor.metadata.cuda(), bias)\n",
    "\n",
    "    output = spatha.spmm_128x64x32_32x64x32_16x8x32_2(\n",
    "                          weights.wrapped_tensor.metadata.cuda(),                    # m-indices\n",
    "                          weights.wrapped_tensor.columns.cuda(),                     # col-loc\n",
    "                          weights.wrapped_tensor.values.to(dtype=torch.half).cuda(), # values\n",
    "                          input_matrix.to(dtype=torch.half).cuda(),                       # rhs_matrix\n",
    "                          bias.to(dtype=torch.half).cuda(),                         # bias\n",
    "                          weights.wrapped_tensor.nrows,                              # A_num_rows\n",
    "                          weights.wrapped_tensor.ncols,                              # A_num_cols\n",
    "                          input_matrix.shape[1],                                          # B_num_cols\n",
    "                          weights.wrapped_tensor.v,                              # vec_length\n",
    "                          weights.wrapped_tensor.n,                                  # n\n",
    "                          weights.wrapped_tensor.m,                                  # m\n",
    "                          weights.wrapped_tensor.nnz,                                # nnz\n",
    "                          0,                                                        # seed\n",
    "                          32,                                                       # mbrow\n",
    "                          4                                                         # brow\n",
    "                          )\n",
    "    return output\n",
    "\n",
    "torch_mm_output = torch.mm(sparse_add(input, weights), mask)\n",
    "# torch_mm_output = (A+B)@C\n",
    "# A+B sumado con sparse_add para que la salida de la suma (D) sea dispersa.\n",
    "# De esta forma, D@C tiene un operando disperso, que es lo que hace spmm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Compute masked results (dense computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = torch.from_numpy(sparse_tensor.wrapped_tensor.to_dense().cpu().to(dtype=torch.half).detach().numpy() @ mask.detach().numpy()).to(device=\"cuda:0\").to(dtype=torch.half)\n",
    "#print(dense)\n",
    "\n",
    "bias = torch.ones((dense.shape))*2\n",
    "dense+=bias.to(dtype=torch.half).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse.T\n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "dense\n",
      " tensor([[2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        ...,\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "        [2., 2., 2.,  ..., 2., 2., 2.]], device='cuda:0', dtype=torch.float16)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"sparse.T\\n\", torch_mm_output.T)\n",
    "print(\"dense\\n\", dense)\n",
    "\n",
    "print( torch.allclose(torch_mm_output.T,dense) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPMM and SDDMM kernels using sten\n",
    "\n",
    "Replicate small test that defines a custom mm operator using sten that works on sparse tensors, with forward and backwards pass.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_idx: 1023\n",
      "max_idx: 511\n",
      "Custom forward completed\n",
      "Backward on custom sten torch.mm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m small_sten_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(small_sten_mm_output); \n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#loss.backwards() # Co backwards disperso\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[43msmall_sten_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Comparar resultados.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA operands to the torch.mm are equal?\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mallclose(masked_kernel_test_A, masked_small_sten_test_A))\n",
      "File \u001b[0;32m~/inno4scale_shared/anaconda3/envs/end2end/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/inno4scale_shared/anaconda3/envs/end2end/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/inno4scale_shared/anaconda3/envs/end2end/lib/python3.10/site-packages/torch/autograd/function.py:267\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/inno4scale_shared/anaconda3/envs/end2end/lib/python3.10/site-packages/sten/sten.py:1502\u001b[0m, in \u001b[0;36mSparseOperatorDispatcher.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1500\u001b[0m                 op_impl_bwd \u001b[38;5;241m=\u001b[39m create_failing_bwd_impl(e)\n\u001b[0;32m-> 1502\u001b[0m tmp_grad_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mop_impl_bwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mginp_sp1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m tmp_grad_inputs \u001b[38;5;241m=\u001b[39m canonicalize_tensor_tuple(tmp_grad_inputs)\n\u001b[1;32m   1504\u001b[0m check_formats(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctx\u001b[38;5;241m.\u001b[39mdisp_state\u001b[38;5;241m.\u001b[39morig_op\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (bwd)\u001b[39m\u001b[38;5;124m\"\u001b[39m, tmp_grad_inputs, ginp_fmt1)\n",
      "Cell \u001b[0;32mIn[28], line 93\u001b[0m, in \u001b[0;36mtorch_mm_bwd_impl\u001b[0;34m(ctx, grad_outputs, input_sparsifiers)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackward on custom sten torch.mm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m [grad_output] \u001b[38;5;241m=\u001b[39m grad_outputs\n\u001b[0;32m---> 93\u001b[0m A_operand, B_operand \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39msaved_tensors\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Compute gradients for A operand, as grad_output @ B_operand.T. Use SPMM\u001b[39;00m\n\u001b[1;32m     96\u001b[0m grad_A \u001b[38;5;241m=\u001b[39m grad_output \u001b[38;5;241m@\u001b[39m B_operand\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "kernels_test = True\n",
    "print_size = 16\n",
    "\n",
    "if kernels_test:\n",
    "    A_operand_shape = [64, 64]\n",
    "    B_operand_shape = [64, 64]\n",
    "    \n",
    "    # Define custom operator that works on sparse tensors with sten\n",
    "    # Definir matrices que funcionen con SrNMTensors. Mínimo de 32 por dimension\n",
    "    kernel_test_A = torch.arange(A_operand_shape[0] * A_operand_shape[1]).reshape(A_operand_shape[0], A_operand_shape[1]).float().requires_grad_()\n",
    "    # Sparsify to SparseVNMTensor\n",
    "    A_masks, A_columns = nm_vector_mask_sparsify(kernel_test_A, v, n, m)\n",
    "    sparse_kernel_test_A = SparseVNMTensor(v, n, m, dense_=kernel_test_A, mask_=A_masks, columns_=A_columns)\n",
    "    #Redensify with 0s to compute reference results with standard torch.mm\n",
    "    masked_kernel_test_A = sparse_kernel_test_A.to_dense().float().requires_grad_() \n",
    "    #print(\"sparse_small_test_A: \", sparse_small_test_A)\n",
    "    kernel_test_B = torch.arange(B_operand_shape[0] * B_operand_shape[1]).reshape(B_operand_shape[0], B_operand_shape[1]).float().requires_grad_()\n",
    "    # Run normal torch.mm to get correct results.\n",
    "    kernel_mm_output = torch.mm(masked_kernel_test_A, kernel_test_B)\n",
    "    #print(\"MM result: Shape:\", kernel_mm_output.shape, \"\\n\", kernel_mm_output[:print_size, :print_size])\n",
    "    # Run backwards, clear gradients just in case it has something stored\n",
    "    masked_kernel_test_A.grad = None\n",
    "    kernel_test_B.grad = None\n",
    "    kernel_loss = torch.sum(kernel_mm_output); \n",
    "    kernel_loss.backward()\n",
    "    #print(\"Small A: Shape:\", masked_kernel_test_A.shape, \"\\n\", masked_kernel_test_A[:print_size, :print_size], \"\\nGrad:\", None if masked_kernel_test_A.grad is None else masked_kernel_test_A.grad[:print_size, :print_size])\n",
    "    #print(\"Small B: Shape:\", kernel_test_B.shape, \"\\n\", kernel_test_B[:print_size, :print_size], \"\\nGrad:\", None if kernel_test_B.grad is None else kernel_test_B.grad[:print_size, :print_size])\n",
    "    \n",
    "    custom_sparse_mm = sten.sparsified_op(\n",
    "        orig_op=torch.mm,\n",
    "        out_fmt=(\n",
    "            (sten.KeepAll(), torch.Tensor,\n",
    "            #NMVectorSparsifier(v,n,m), SparseVNMTensor),\n",
    "            sten.KeepAll(), torch.Tensor),\n",
    "        ),\n",
    "        grad_out_fmt=(\n",
    "            (sten.KeepAll(), torch.Tensor,\n",
    "            #NMVectorSparsifier(v,n,m), SparseVNMTensor),\n",
    "            sten.KeepAll(), torch.Tensor),\n",
    "        ),\n",
    "    )\n",
    "    # NMVectorSparsifier(v,n,m), SparseVNMTensor, para out_fmt\n",
    "    # SparseVNMTensor, torch.Tensor Para register_fwd_op_impl\n",
    "    #Aplicar sten, definir forward/backwards que faga o que corresponda \n",
    "    @sten.register_fwd_op_impl(\n",
    "        operator=torch.mm,\n",
    "        inp=(SparseVNMTensor, torch.Tensor),\n",
    "        out=[(sten.KeepAll, torch.Tensor)],\n",
    "    )\n",
    "    def torch_mm_fwd_impl(ctx, inputs, output_sparsifiers):\n",
    "        A_operand_sparse, B_operand = inputs\n",
    "        print(\"Forward on custom sparse torch.mm with sten\")\n",
    "        ctx.save_for_backward(A_operand_sparse, B_operand)\n",
    "        \n",
    "        forward_bias = torch.ones(A_operand_sparse.wrapped_tensor.nrows)*2\n",
    "        \n",
    "        # Use SPMM. SPMM(B, A) performs A@B with B being in VENOM's sparse format. Or is it SPMM(A, B) -> A@B? Which is compressed, A or B.\n",
    "        # Assume SPMM(A, B) -> A@B with A being sparse.\n",
    "        # May cause problems for the layer if the weights are the sparse ones, since the operation should be input @ weights, but weights is the sparse one.\n",
    "        output = spatha.spmm_128x64x32_32x64x32_16x8x32_2(\n",
    "                        A_operand_sparse.wrapped_tensor.metadata.cuda(),                    # A m-indices\n",
    "                        A_operand_sparse.wrapped_tensor.columns.cuda(),                     # A col-loc\n",
    "                        A_operand_sparse.wrapped_tensor.values.to(dtype=torch.half).cuda(), # A values\n",
    "                        B_operand.to(dtype=torch.half).cuda(),                              # B, rhs_matrix\n",
    "                        forward_bias.to(dtype=torch.half).cuda(),                           # bias\n",
    "                        A_operand_sparse.wrapped_tensor.nrows,                              # A_num_rows\n",
    "                        A_operand_sparse.wrapped_tensor.ncols,                              # A_num_cols\n",
    "                        B_operand.shape[1],                                                 # B_num_cols\n",
    "                        A_operand_sparse.wrapped_tensor.v,                                  # vec_length, V\n",
    "                        A_operand_sparse.wrapped_tensor.n,                                  # n\n",
    "                        A_operand_sparse.wrapped_tensor.m,                                  # m\n",
    "                        A_operand_sparse.wrapped_tensor.nnz,                                # nnz\n",
    "                        0,                                                                  # seed\n",
    "                        32,                                                                 # mbrow\n",
    "                        4                                                                   # brow\n",
    "                        )\n",
    "        print(\"SPMM output:\", output)\n",
    "        return output\n",
    "        #return A_operand_sparse.to_dense() @ B_operand\n",
    "    \n",
    "    @sten.register_bwd_op_impl(\n",
    "        operator=torch.mm,\n",
    "        grad_out=[torch.Tensor],\n",
    "        grad_inp=(\n",
    "            (sten.KeepAll, torch.Tensor),\n",
    "            (sten.KeepAll, torch.Tensor),\n",
    "        ),\n",
    "        inp=(torch.Tensor, torch.Tensor ),\n",
    "    )\n",
    "    def torch_mm_bwd_impl(ctx, grad_outputs, input_sparsifiers):\n",
    "        print(\"Backward on custom sten torch.mm\")\n",
    "        [grad_output] = grad_outputs\n",
    "        A_operand, B_operand = ctx.saved_tensors\n",
    "        \n",
    "        # Compute gradients for A operand, as grad_output @ B_operand.T. Use SPMM\n",
    "        grad_A = grad_output @ B_operand.T\n",
    "        \n",
    "        # Compute gradients for B operand, as A_operand.T @ grad_output, with the same sparsity as B_operand.\n",
    "        grad_B = A_operand.T @ grad_output\n",
    "        \n",
    "        return grad_A, grad_B\n",
    "    # con algebra densa (operador @)\n",
    "    #....\n",
    "    #torch_mm_output = torch.mm(A, B)# Con operación forward disperso\n",
    "    small_sten_test_A = torch.arange(32 * 32).reshape(32, 32).float().requires_grad_()\n",
    "    sparse_sten_small_test_A = SparseVNMTensor(v, n, m, dense_=small_sten_test_A, mask_=A_masks, columns_=A_columns)\n",
    "    small_sten_test_B = torch.arange(32 * 32).reshape(32, 32).float().requires_grad_()\n",
    "    masked_small_sten_test_A = sparse_sten_small_test_A.to_dense().float().requires_grad_()\n",
    "    small_sten_mm_output = custom_sparse_mm(masked_small_sten_test_A, small_sten_test_B)\n",
    "    print(\"Custom forward completed\")\n",
    "    # Run backwards, clear gradients just in case it has something stored\n",
    "    masked_small_sten_test_A.grad = None\n",
    "    small_sten_test_B.grad = None \n",
    "    small_sten_loss = torch.sum(small_sten_mm_output); \n",
    "    #loss.backwards() # Co backwards disperso\n",
    "    small_sten_loss.backward()\n",
    "    # Comparar resultados.\n",
    "    print(\"\\nA operands to the torch.mm are equal?\", torch.allclose(masked_kernel_test_A, masked_small_sten_test_A))\n",
    "    #print(\"masked_kernel_test_A corner:\\n\", masked_kernel_test_A[:print_size, :print_size])\n",
    "    #print(\"masked_small_sten_test_A corner:\\n\", masked_small_sten_test_A[:print_size, :print_size])\n",
    "    \n",
    "    print(\"B operands to the torch.mm are equal?\",torch.allclose(kernel_test_B, small_sten_test_B) )\n",
    "    #print(\"kernel_test_B corner:\\n\", kernel_test_B[:print_size, :print_size])\n",
    "    #print(\"small_sten_test_B corner:\\n\", small_sten_test_B[:print_size, :print_size])\n",
    "    \n",
    "    print(\"Torch.mm results are equal?\", torch.allclose(small_mm_output, small_sten_mm_output))\n",
    "    #print(\"Result of unmodified torch.mm: Shape:\", small_mm_output.shape, \"\\n\", small_mm_output[:print_size, :print_size])\n",
    "    #print(\"Result of modified torch.mm: Shape:\", small_sten_mm_output.shape, \"\\n\", small_sten_mm_output[:print_size, :print_size])\n",
    "    \n",
    "    print(\"Gradients of the A operand are equal?\", torch.allclose(masked_kernel_test_A.grad, masked_small_sten_test_A.grad))\n",
    "    #print(\"\\nGradients of the A operand to the unmodified torch.mm (corner only):\\n\", None if masked_kernel_test_A.grad is None else masked_kernel_test_A.grad[:print_size, :print_size])\n",
    "    #print(\"\\nGradients of the A operand to the modified torch.mm  (corner only):\\n\", None if masked_small_sten_test_A.grad is None else masked_small_sten_test_A.grad[:print_size, :print_size])\n",
    "    \n",
    "    print(\"Gradients of the B operand are equal?\", torch.allclose(kernel_test_B.grad, small_sten_test_B.grad))\n",
    "    #print(\"\\nGradients of the B operand to the unmodified torch.mm (corner only):\\n\", None if kernel_test_B.grad is None else kernel_test_B.grad[:print_size, :print_size])\n",
    "    #print(\"\\nGradients of the B operand to the modified torch.mm (corner only):\\n\", None if small_sten_test_B.grad is None else small_sten_test_B.grad[:print_size, :print_size])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Prepare inputs for all tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectangular matrices\n",
    "#A_matrix_shape = [256, 128]\n",
    "#B_matrix_shape = [128, 256]\n",
    "\n",
    "# Square matrices\n",
    "A_matrix_shape = [64, 64]\n",
    "B_matrix_shape = [64, 64]\n",
    "\n",
    "torch.set_printoptions(linewidth=1000, threshold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_idx: 0\n",
      "A_matrix_new shape: torch.Size([64, 64]) B_matrix_new shape: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#A_matrix_new = torch.randn(A_matrix_shape[0], A_matrix_shape[1], requires_grad=True)\n",
    "A_matrix_new = torch.ones(A_matrix_shape[0], A_matrix_shape[1], requires_grad=True)\n",
    "B_matrix_new = torch.randn(B_matrix_shape[0], B_matrix_shape[1], requires_grad=True)\n",
    "\n",
    "add_matrix = torch.randn(A_matrix_new.shape[0], A_matrix_new.shape[1], requires_grad=True)\n",
    "sparse_tensor_new = sparse_add(A_matrix_new, add_matrix)\n",
    "\n",
    "print(\"A_matrix_new shape:\", A_matrix_new.shape, \"B_matrix_new shape:\", B_matrix_new.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded_matrix: shape: torch.Size([64, 64]) \n",
      " tensor([[ 0.,  2.,  1.,  0.,  1.,  4., -1., -4., -2., -3.,  4.,  2., -2., -3., -2.,  3.,  4.,  3.,  0., -4., -1.,  3.,  1., -4., -1.,  0., -3.,  3., -4.,  0.,  2.,  0.,  4., -2., -4., -2.,  4., -3., -2., -4., -4.,  4.,  2., -2.,  3., -4.,  4.,  3.,  3., -1.,  1.,  4., -4., -4.,  2., -1.,  0.,  4.,  4.,  1., -3.,  1.,  3., -4.],\n",
      "        [ 1.,  1., -1.,  1.,  2., -1., -1.,  3., -3.,  3.,  3.,  4.,  3.,  0.,  2., -1.,  4., -1., -3.,  4., -3.,  1., -4.,  1., -2.,  4.,  4., -1.,  1.,  3.,  0., -3., -3.,  1.,  0.,  4.,  4.,  3.,  0., -4.,  2., -2.,  3., -2.,  3.,  0.,  0.,  0.,  1., -1., -3.,  2.,  4., -3., -3., -2., -4., -4., -1., -1.,  1.,  3., -1.,  3.],\n",
      "        [-3.,  3.,  0.,  4.,  0.,  4.,  2.,  4.,  4.,  0.,  4.,  0., -4., -1.,  2., -1.,  0.,  3.,  4., -2.,  2.,  3., -2.,  1.,  1.,  1.,  4., -2., -2., -2.,  3., -1.,  4., -2., -2., -1.,  4., -4., -2.,  1.,  0., -3., -3., -2., -1.,  3.,  0.,  1.,  0., -1.,  3., -3.,  4., -4.,  0.,  0.,  2.,  0.,  0.,  2.,  0., -2., -4., -1.],\n",
      "        [-2., -1.,  0.,  4., -3.,  2.,  1.,  1.,  2.,  2.,  2.,  3., -2., -3., -1.,  2., -2., -4.,  2., -3.,  3., -3.,  1.,  0., -1., -3.,  4.,  2., -3., -4., -4.,  0., -3.,  1., -1., -2., -4.,  2.,  1.,  0., -1., -2., -2.,  0., -3.,  0., -3., -1.,  0.,  1., -2., -2.,  1.,  4.,  1.,  2.,  3.,  0., -1.,  2., -2.,  0.,  4., -1.],\n",
      "        [ 3., -3.,  1.,  3.,  3., -2., -3., -4., -2., -1.,  0.,  0.,  1., -1.,  3.,  3., -4., -3., -3., -1.,  3.,  0., -4., -1.,  2., -2., -4., -4.,  0.,  3., -1., -2.,  4.,  3.,  3.,  2., -4.,  2.,  2., -2., -3., -3.,  2.,  0.,  1., -1., -1., -1., -2., -2.,  0., -4.,  2., -2., -3., -3.,  0.,  4., -3.,  4.,  2., -1.,  4.,  1.],\n",
      "        [-3.,  1., -1., -3., -2.,  3., -3., -3., -4.,  2.,  2., -1., -4.,  3.,  0., -2., -4.,  4., -4.,  2., -3.,  4.,  3.,  0.,  3.,  3., -3.,  1., -3.,  4., -3., -2., -2., -2., -3., -2., -3., -2.,  0.,  4.,  2.,  4., -2.,  2.,  0.,  2.,  2.,  0., -1.,  0.,  4.,  0., -1.,  0.,  2., -2., -2.,  4.,  1., -3.,  3.,  2., -1., -4.],\n",
      "        [-4., -2., -2.,  4.,  1.,  0.,  3., -4., -3., -4.,  3.,  1.,  0., -2., -4.,  3.,  1.,  2., -4.,  4., -2.,  3.,  4.,  0.,  0.,  0.,  2., -2., -4.,  3., -4.,  3., -3., -2.,  2.,  0.,  1.,  0.,  0.,  0., -2., -4., -3.,  2., -1.,  4., -2.,  2.,  2., -2.,  1.,  4.,  3., -1., -3., -2.,  1.,  3., -2.,  1.,  0., -2., -1.,  1.],\n",
      "        [-1., -4.,  3.,  2., -2., -4., -3.,  0., -3., -2., -3., -2., -4., -3.,  4.,  2., -2., -2., -1., -4., -1.,  0., -3., -4., -3., -3., -1.,  1.,  0.,  3.,  4.,  1.,  1.,  2., -2.,  2.,  2., -3., -3.,  1.,  0., -3.,  3., -2., -2.,  0.,  4.,  0.,  1., -2., -1.,  4.,  2., -2.,  4.,  3., -1., -1., -3.,  3.,  4., -4., -3., -2.],\n",
      "        [ 0., -3.,  4., -4., -1.,  4.,  1.,  1., -4., -2.,  3., -4.,  0.,  2.,  1., -4.,  2.,  2., -4., -3.,  4.,  2., -4., -2.,  3.,  4.,  4.,  2., -4., -4.,  3.,  0., -4.,  2.,  0., -1., -1.,  4.,  4., -1.,  4.,  2.,  0., -1., -3.,  3.,  0.,  3., -2., -2.,  4., -3.,  4.,  2., -3.,  3.,  2., -4., -2.,  0.,  4., -4.,  4.,  2.],\n",
      "        [ 1., -2., -4.,  4.,  4.,  2., -3.,  3., -1., -1.,  4., -2.,  4., -1., -4., -3.,  1.,  2., -2., -1., -2., -3.,  4.,  4., -4., -3., -1.,  2., -4., -4., -1.,  1., -4., -1., -2.,  2., -4., -3.,  0., -3.,  0.,  0., -1., -1.,  1., -3., -2., -4.,  1., -2., -3.,  3.,  0., -4.,  0.,  0., -3.,  1., -3., -3.,  2.,  0.,  0.,  0.],\n",
      "        [ 1.,  1., -2., -1.,  2.,  0.,  0., -4.,  2.,  1., -3., -1.,  2.,  0., -3., -2.,  0., -2.,  4.,  4., -2., -1., -3., -3., -4., -4., -3.,  0., -2.,  1.,  2.,  3.,  4.,  4., -3.,  0., -2., -1., -2.,  4.,  4., -1., -4., -1.,  1., -3.,  2., -3., -3., -1.,  3., -1.,  2., -4., -1.,  0.,  3.,  0.,  2., -3.,  3.,  0.,  2.,  3.],\n",
      "        [-3.,  2., -2., -1., -4.,  0., -3.,  3.,  1., -3., -3., -2., -3.,  1., -3., -4.,  4.,  2., -2.,  0.,  2.,  1.,  4., -1., -4.,  2.,  0.,  3.,  4.,  4.,  1., -4.,  1.,  1., -2.,  0.,  3., -1., -2.,  0., -2., -3.,  0., -3.,  3., -1., -2.,  0., -4.,  0.,  4.,  1., -4.,  4.,  4., -4., -1., -3.,  1., -2., -4., -2., -4.,  2.],\n",
      "        [ 1., -2.,  4.,  0., -1.,  4.,  2.,  2., -4., -3.,  1.,  1., -2.,  3., -3., -2., -4., -4.,  3., -4.,  2.,  0.,  3.,  3.,  1.,  0.,  3.,  2.,  0.,  4., -3.,  3., -3.,  3., -2.,  1.,  2.,  2., -4.,  0.,  1.,  1., -3.,  3.,  0., -4.,  4., -2.,  3.,  2., -1., -1., -4.,  4., -3.,  1., -3., -4., -4., -1.,  4.,  4., -2.,  3.],\n",
      "        [ 0., -2., -3., -3., -4., -3.,  0., -1.,  1.,  1., -2.,  3., -1., -3., -4., -3.,  1., -1., -2.,  1., -3.,  0., -4., -4., -2.,  3.,  0.,  4.,  1.,  0.,  2., -4.,  2.,  2., -2.,  0.,  3.,  0.,  4., -1., -4.,  4., -3., -3., -4., -2., -4., -1., -1., -1.,  3., -2.,  3.,  3., -1.,  3., -1.,  1.,  2.,  4., -4., -3., -4.,  0.],\n",
      "        [ 3., -4.,  3.,  0.,  0.,  0.,  1., -2., -1.,  1., -3., -3.,  3., -3., -2.,  4.,  0., -4., -4., -4.,  1., -1.,  3., -4.,  4., -2.,  4.,  2.,  0.,  2., -3., -4.,  0.,  4.,  0.,  2., -3., -3.,  2.,  0.,  0.,  3., -1., -4., -4.,  2.,  4.,  0.,  0.,  2.,  1., -3., -4., -3., -3.,  2.,  0.,  3., -3.,  2., -1., -1.,  2.,  3.],\n",
      "        [-4., -4.,  1., -2.,  4., -4.,  0., -1.,  4.,  2., -1.,  4., -3., -4., -3.,  1.,  3.,  2.,  2.,  1.,  2.,  1., -3., -3.,  0., -2.,  3.,  3., -1., -2., -1., -2., -4.,  2., -2.,  4.,  3.,  0., -2.,  2., -3., -1., -1., -4.,  0.,  0.,  2., -4., -2., -3.,  1.,  4.,  3.,  0.,  3., -4.,  3., -1.,  3.,  4., -1.,  4., -3.,  0.],\n",
      "        [ 2., -1., -1., -2.,  4.,  0.,  4.,  3.,  1., -2.,  4., -4.,  2., -1.,  3.,  2.,  0., -2., -1., -2.,  2.,  4., -4., -2., -2.,  3., -2.,  0.,  1., -1.,  4., -4.,  1., -2., -2., -2.,  0.,  4., -4., -3., -3.,  4.,  4.,  1., -4.,  3., -3.,  1., -4., -2.,  1.,  2., -4.,  2., -4., -4., -2., -4.,  0.,  3., -1., -1.,  3.,  2.],\n",
      "        [ 0.,  3.,  4.,  4.,  2., -4., -4.,  1.,  2.,  3., -2.,  3.,  1., -3., -3.,  1., -3.,  2., -4., -3., -1.,  3., -2.,  0.,  3.,  2., -4., -4., -3.,  4.,  2., -1.,  2.,  2., -2., -2.,  0., -3.,  3.,  4.,  4.,  3.,  0.,  0.,  4.,  1.,  3., -3., -3.,  1., -4., -2.,  0., -2.,  2., -2.,  3.,  3., -4.,  4.,  0.,  3., -2.,  4.],\n",
      "        [-2., -2., -3.,  0., -1., -4., -3., -4.,  3.,  2., -2.,  1., -4., -4.,  0., -3.,  0.,  0.,  0.,  4.,  3., -3.,  4., -1.,  2., -4., -4., -3.,  1., -4., -3.,  1., -2., -4.,  3.,  0., -4.,  4.,  0.,  2.,  1.,  1., -2.,  2., -1.,  2.,  1.,  3., -3., -4.,  2.,  2., -4.,  2., -4.,  2.,  0., -4.,  2.,  3., -4.,  1., -1., -3.],\n",
      "        [ 1.,  4.,  1.,  2.,  3.,  3., -1., -2., -1.,  1.,  2.,  3., -4., -2.,  1., -3., -1., -3.,  2., -3.,  1.,  2.,  3., -4.,  1., -2.,  3., -1.,  3.,  4.,  0.,  0.,  3., -4.,  4., -1.,  2., -4.,  1., -4.,  1., -2.,  1., -1., -1., -2.,  0.,  2., -1.,  4.,  1.,  4.,  0., -3.,  2., -4.,  0., -2., -1., -2.,  4.,  4.,  0.,  0.],\n",
      "        [ 2., -1.,  3., -1., -1.,  0., -2.,  2.,  0.,  3., -4.,  3., -4., -1., -2., -3., -2.,  3.,  3.,  0.,  3., -2.,  0., -2., -2.,  2.,  0., -3., -1.,  3.,  1., -4., -3., -2., -2.,  1.,  0.,  0., -2.,  4., -4., -3.,  1.,  4.,  0.,  1., -4.,  0., -2.,  2., -4., -4.,  4., -2., -2., -4.,  4.,  0., -3., -4., -2.,  1.,  4.,  0.],\n",
      "        [ 3., -3.,  3., -2., -1.,  3.,  4., -1.,  2., -2., -2.,  4.,  4., -3.,  0., -3.,  3., -2., -3.,  0., -2., -3., -2., -4.,  1.,  0., -4.,  2.,  3.,  4.,  4., -1.,  3.,  2.,  1.,  4., -2., -2., -1., -4.,  1., -1.,  4.,  0.,  0., -3., -1., -4., -3.,  0., -1.,  0.,  1.,  1.,  0.,  4.,  3.,  0., -1.,  1., -2., -2.,  4.,  3.],\n",
      "        [ 2., -2.,  3.,  3.,  1., -3.,  3.,  4.,  0.,  0., -3.,  3.,  2.,  0.,  1.,  3.,  4.,  4., -2., -2., -1.,  0.,  4., -3.,  3., -1.,  1., -4., -1., -2.,  3.,  4.,  1.,  1.,  2.,  4.,  0.,  0.,  1.,  3.,  4.,  2., -1., -1., -3.,  4., -2., -4.,  2., -2., -4., -4.,  3.,  2.,  4., -1.,  3.,  0., -1., -2.,  3.,  4., -3., -3.],\n",
      "        [-1.,  3., -4.,  1., -4.,  1., -1.,  4., -2.,  0., -3., -3.,  0., -1., -3.,  4., -1.,  4.,  4.,  4.,  1.,  1., -2.,  0., -3., -1.,  0.,  2., -3., -1.,  3.,  0., -3.,  1., -4., -3.,  4., -2.,  3., -3.,  2.,  4., -2.,  4., -4., -3.,  3., -2.,  3.,  0.,  4., -3., -3., -2., -1., -4., -1.,  3.,  2.,  0., -3., -2.,  4., -4.],\n",
      "        [ 3.,  3.,  4.,  2.,  3.,  1.,  1., -2., -2.,  1., -2., -3.,  0.,  3., -1., -4., -3., -2., -3., -4., -2.,  1.,  3.,  2., -3.,  0., -3., -3.,  0.,  4., -3., -2.,  2.,  4.,  2.,  0.,  0., -4.,  0.,  2.,  1.,  2.,  1.,  3., -1.,  4.,  3., -2., -4.,  2., -4., -4., -2.,  3.,  2., -2., -2.,  2., -1.,  2.,  1., -2.,  2., -4.],\n",
      "        [-3., -3., -2., -1., -3.,  2.,  4.,  0., -2.,  0., -2.,  1.,  0.,  3.,  1., -2.,  0.,  1., -2.,  1., -1.,  3.,  3., -1.,  0., -3.,  3., -4., -1., -2., -4., -1., -1., -2.,  2.,  0.,  4.,  1.,  2.,  4., -3.,  2., -2., -1.,  0.,  3.,  1.,  2., -3.,  1., -2.,  0., -1.,  3.,  1.,  3.,  4., -1.,  1., -3.,  1., -1.,  0.,  2.],\n",
      "        [-1.,  4.,  4., -1.,  1.,  1., -4.,  0., -4., -1.,  3.,  0., -3., -3., -4., -1.,  2., -4.,  1.,  4.,  4., -2.,  2.,  3., -1., -3.,  2., -4.,  0., -3.,  0.,  4., -3.,  0., -4.,  0.,  3., -4.,  4.,  4., -1.,  0., -3., -1.,  1., -3.,  2., -4., -2., -4.,  4., -3., -2.,  0.,  2.,  2.,  1.,  0.,  0., -4., -1.,  4.,  2., -2.],\n",
      "        [-3.,  2.,  0., -4.,  1., -1.,  4.,  4.,  3., -4., -2., -3., -2.,  2., -2., -2.,  3., -3., -3.,  3.,  1.,  3.,  0.,  4., -4.,  2.,  3.,  0.,  0.,  0.,  0.,  1., -4., -4.,  1.,  1., -3.,  0.,  0.,  3., -1.,  1.,  4., -1., -4., -3.,  1.,  3., -3.,  2.,  0.,  0., -2.,  2.,  0., -1.,  0., -2.,  1.,  4.,  0.,  3., -2.,  0.],\n",
      "        [ 3.,  2., -4.,  3.,  4., -1.,  1., -2.,  2.,  0.,  1.,  2.,  0.,  4., -1.,  1.,  1.,  3.,  3.,  2.,  0., -2.,  3.,  2., -2., -3., -1.,  1.,  4.,  2., -4.,  1., -1.,  4., -1., -4., -4.,  4., -1.,  0., -1.,  4.,  4.,  1.,  2., -2., -3., -4.,  3., -4.,  2., -3., -4.,  0.,  3., -3.,  1., -4.,  2., -1.,  2.,  0.,  4.,  3.],\n",
      "        [-3., -4.,  3.,  4.,  4.,  4., -1., -4.,  2., -3.,  2., -1., -3.,  1., -1., -4., -1.,  4.,  4., -2., -1.,  2., -1.,  3.,  2.,  3.,  4., -3., -4.,  3.,  2., -4.,  1., -1.,  2.,  0., -4.,  4., -2.,  2., -4.,  2.,  3., -3., -1., -2.,  4.,  0., -3.,  3.,  2., -2., -1.,  3.,  3.,  3.,  1.,  2.,  3.,  0., -2.,  0., -2.,  3.],\n",
      "        [ 1.,  4., -4.,  0.,  3., -1., -3.,  3., -4., -4., -4., -3., -4.,  2.,  2.,  4.,  1., -1.,  4.,  4., -2.,  3.,  2.,  1., -2.,  0.,  3.,  1.,  3., -4., -3., -3., -4., -2.,  1.,  1., -1.,  1.,  0., -3., -1., -2., -1., -1.,  4.,  3., -4., -2., -3.,  4., -3., -3.,  0.,  4.,  1.,  3., -3., -1., -1.,  4., -2., -2.,  4., -4.],\n",
      "        [ 0.,  0.,  1.,  3., -4.,  3., -4., -1., -4., -3.,  0.,  3.,  4.,  0., -4.,  3., -2., -2., -4.,  2.,  4., -1.,  0.,  3.,  0.,  1.,  1.,  2.,  4.,  0.,  2., -3.,  2., -3.,  3.,  3.,  4.,  1.,  4.,  3.,  2., -2.,  1., -1.,  2.,  1., -3.,  2.,  1.,  4.,  0., -2., -2.,  2.,  4.,  3., -3.,  0., -2., -4.,  2.,  3.,  4., -1.],\n",
      "        [ 4.,  2.,  4.,  3., -2.,  2.,  1.,  2.,  4.,  4., -4., -1., -1., -4., -3.,  4.,  4.,  1., -4., -4., -4.,  4.,  1., -3., -3.,  1.,  4.,  3., -1.,  4., -3., -2., -1.,  4.,  4.,  2.,  1., -2., -3.,  0., -4., -4.,  3., -1., -4.,  4., -4.,  2., -2.,  3.,  0., -1.,  2., -4., -2.,  3.,  0., -2., -1.,  3., -3.,  1.,  4.,  0.],\n",
      "        [ 0.,  1.,  4., -4.,  1., -3., -2.,  1., -3., -4.,  4.,  4., -4.,  3.,  1., -4.,  1.,  4., -1., -3.,  2.,  2.,  2., -3.,  2., -3., -4.,  3.,  0.,  2., -4.,  2., -2.,  3.,  0.,  3.,  2.,  2.,  0.,  1.,  2., -3.,  0.,  0.,  4.,  3.,  1.,  1.,  2.,  2.,  2., -2., -3., -1., -1.,  3., -2., -1.,  1.,  2., -4.,  0., -1., -4.],\n",
      "        [-4.,  3.,  4.,  2.,  0., -3., -2., -4., -2.,  0.,  0., -4., -4.,  3.,  1.,  1.,  1., -4.,  3.,  2., -3., -3.,  1.,  0., -1.,  4., -3., -3., -3., -1., -2., -3., -3.,  4.,  3.,  0., -4.,  3.,  0., -2., -4.,  2., -2.,  4.,  1.,  1.,  0., -3., -1., -4.,  1.,  1.,  4., -3.,  3., -2.,  4.,  2., -2., -4., -4.,  0., -2., -4.],\n",
      "        [-3.,  3., -2., -3.,  0.,  2., -3., -2., -3., -2., -3.,  2.,  3., -1.,  1., -4., -3., -4., -1., -3., -3., -3., -3.,  3.,  1., -1.,  4.,  0.,  1.,  4., -2.,  2.,  0.,  0.,  1.,  4.,  4.,  3., -4., -4., -4.,  4.,  1.,  1., -1.,  4., -1., -2.,  4.,  2., -1.,  4.,  1.,  1.,  2., -4.,  2., -1.,  0., -1., -2.,  2.,  3.,  3.],\n",
      "        [-3.,  0.,  0., -4., -4.,  0.,  3., -4.,  0., -1., -1.,  1., -3.,  3.,  3., -4., -2., -2.,  2.,  2.,  1.,  0.,  2., -2.,  1., -3., -1.,  4.,  1., -2.,  2.,  2.,  0., -3.,  0.,  0.,  2., -2., -2.,  4.,  0., -1.,  0.,  1.,  4., -2., -1., -2., -2.,  4.,  2.,  3., -3., -1.,  4.,  2.,  0., -2.,  1.,  3., -2., -4.,  0.,  2.],\n",
      "        [-2.,  2.,  4.,  2.,  2., -3., -1., -3., -1.,  3.,  0., -2.,  3.,  2., -2., -3.,  1.,  4.,  2.,  0., -4.,  1., -3., -2.,  2.,  3., -4.,  4.,  3., -2.,  0.,  3.,  4., -1., -2.,  2., -1.,  1.,  1.,  0., -3.,  4.,  2., -4., -1.,  3.,  4.,  2.,  0.,  1., -3.,  1., -4., -2.,  3.,  2.,  3.,  4., -1.,  0., -3.,  3., -4.,  4.],\n",
      "        [-4., -1.,  1., -3.,  2.,  4.,  1.,  4.,  3., -2.,  4.,  4.,  3.,  3.,  1., -1., -3.,  2.,  2.,  4.,  3.,  0., -1.,  1., -3.,  3.,  3., -1., -1.,  4., -4., -3., -4., -1., -2.,  2., -2.,  3.,  1.,  4.,  4.,  0.,  3.,  2., -4.,  0.,  3., -4., -3.,  0.,  4.,  4., -4., -3., -1.,  4.,  4., -3., -4., -4.,  3., -4., -2.,  3.],\n",
      "        [-1., -2.,  1.,  2., -3., -3., -1., -4.,  0., -3.,  0., -2.,  1., -1., -4.,  1.,  3., -4., -2.,  1., -3.,  1.,  1.,  3.,  1.,  1.,  1., -1.,  2.,  1., -2., -4.,  4.,  1.,  0.,  3.,  2.,  3.,  3.,  4., -4., -2., -4., -1., -1., -4.,  2., -2.,  3., -4.,  1.,  4., -1., -3.,  0.,  2.,  1.,  4.,  4., -2.,  0., -3., -4., -3.],\n",
      "        [ 0., -1.,  4.,  4., -3.,  2.,  4.,  4.,  3.,  2., -3., -3.,  2.,  3., -3.,  1.,  1.,  3., -2.,  3.,  2.,  1., -2., -2.,  0., -3., -2.,  2., -4., -3.,  4.,  0.,  0.,  3.,  0.,  1., -1., -3., -1.,  4.,  3.,  0.,  3.,  0., -4., -4.,  3.,  0.,  1., -3., -2., -2.,  0.,  0., -1.,  4.,  0.,  1., -1.,  0.,  2., -2., -4., -3.],\n",
      "        [ 4.,  0.,  1., -2.,  1.,  4.,  4., -3., -3.,  2.,  1., -3.,  0., -2.,  1., -4., -3.,  1., -3.,  1.,  4.,  0., -2., -1.,  3.,  1.,  1.,  1.,  4.,  1.,  0.,  1.,  3.,  3.,  1., -3.,  0.,  0., -2.,  1., -3.,  2.,  1.,  2.,  4.,  4.,  0., -4., -1.,  1.,  1., -2., -4.,  3., -1.,  2., -3.,  4., -2.,  3., -1.,  0., -1., -3.],\n",
      "        [-3.,  4., -1.,  1., -3., -1., -4., -3., -4., -1.,  1.,  4., -4., -4.,  2., -1.,  0., -2., -1., -2.,  3.,  2.,  4.,  3.,  0.,  4.,  1.,  3.,  0., -4.,  2.,  1.,  2.,  3.,  4.,  2., -2.,  4.,  3., -4., -4., -1.,  3., -3., -3.,  0.,  0.,  1.,  1.,  2.,  4., -3., -3.,  3.,  4.,  1.,  3.,  1., -3., -4., -1.,  4., -1.,  3.],\n",
      "        [ 2., -4.,  0.,  4.,  3., -4., -4.,  3., -3.,  1.,  4., -1., -4., -2.,  2.,  1.,  4., -1.,  1.,  3., -2.,  0.,  0.,  3.,  3., -1.,  1., -3., -2.,  2.,  3., -4.,  2., -2.,  2.,  0., -3.,  0., -1., -2., -4., -4., -1.,  4., -2., -3., -2.,  4.,  0.,  3.,  1.,  0., -1.,  3., -4.,  4., -3.,  1.,  4.,  0., -2.,  2., -2.,  3.],\n",
      "        [ 2.,  2., -2.,  3., -2., -1., -3., -4.,  0.,  0.,  4.,  0., -1., -2., -2.,  4., -4.,  3., -3., -3.,  1., -2.,  4.,  2.,  3.,  3.,  4.,  4.,  0., -3.,  2., -3.,  2.,  2.,  3.,  4., -3.,  2.,  4.,  1.,  4.,  2., -4., -1.,  4., -2., -4.,  2.,  3., -3.,  2., -1., -3.,  1.,  4.,  3., -3.,  3.,  2.,  1.,  3., -3.,  0.,  0.],\n",
      "        [ 3., -4., -3.,  4.,  2.,  3., -2.,  2., -2., -1., -4., -3., -1.,  3.,  2., -4.,  2., -3., -1.,  4.,  0., -4.,  2.,  1.,  2., -3.,  4.,  0., -2., -1.,  2.,  3.,  0.,  1.,  3.,  4., -3., -4.,  1.,  0., -3., -1., -1., -1., -3., -4., -1.,  2.,  4.,  0.,  1., -1.,  1., -2.,  2., -2., -1.,  1.,  0., -1., -4.,  4., -3., -2.],\n",
      "        [-2.,  2., -3., -1.,  1.,  1.,  3.,  0.,  2., -2.,  3.,  2., -4.,  4., -1., -4., -2.,  4., -3.,  3.,  4.,  4.,  3., -4., -2., -4., -1., -2.,  2., -2., -2.,  4., -4., -2., -2.,  1.,  1.,  4.,  3., -2., -3.,  0.,  4.,  4., -1., -4.,  2.,  1.,  2.,  4., -3.,  1.,  1.,  2.,  1.,  3.,  2.,  2.,  4., -1., -4.,  4., -4.,  3.],\n",
      "        [ 4., -1., -3.,  0., -2.,  4.,  1., -3., -1., -2., -4.,  1., -1.,  3.,  4., -4.,  0.,  4.,  2.,  3.,  1., -4., -1., -3.,  1., -2., -2.,  1., -4., -1., -3.,  4.,  0., -2.,  0.,  0.,  4., -4.,  1., -1., -2., -1.,  2., -1.,  4.,  1.,  0., -3., -2.,  4.,  2.,  4.,  2., -3.,  3., -2., -3., -4.,  3., -3., -1.,  4., -3.,  1.],\n",
      "        [ 4., -1., -4.,  4., -3., -1., -4., -3.,  1.,  2.,  1.,  0., -4.,  3.,  0., -1.,  2., -3., -4., -1., -4.,  3.,  0., -2.,  3., -2., -3., -4., -4., -4.,  1.,  2., -1.,  2.,  1., -2., -4., -1.,  0.,  1.,  3.,  3.,  4.,  4., -1., -3., -4.,  3., -4., -4., -3., -3.,  3., -1., -3., -1.,  2., -2., -1.,  0., -1., -4.,  4.,  0.],\n",
      "        [ 2., -1.,  3.,  0.,  2., -4.,  4.,  1.,  1.,  1., -2.,  4.,  0., -2.,  2.,  0., -2.,  1., -1.,  3., -4.,  0., -3.,  0.,  3.,  1.,  2., -3., -1.,  1.,  1.,  3.,  4., -3., -2.,  0., -3.,  4., -4.,  0., -2., -2., -1.,  2., -2., -4., -2.,  0.,  2.,  1., -4.,  0., -3., -3.,  4.,  2.,  2.,  1.,  1., -4., -3.,  4.,  1.,  4.],\n",
      "        [-3.,  4., -1., -4.,  1., -1.,  1.,  4., -1.,  4.,  1.,  1.,  3.,  1.,  3., -2., -2.,  3.,  2., -3.,  2.,  1.,  1., -2.,  4., -3., -4.,  3., -4.,  2.,  2.,  4.,  1., -4., -4.,  4., -3., -1.,  3., -2., -2.,  0.,  1.,  3.,  3., -1., -4., -4.,  4.,  2.,  4.,  2., -4.,  0.,  4.,  4.,  1.,  2.,  3.,  0., -1.,  0., -1.,  2.],\n",
      "        [-1., -1.,  2., -2.,  2.,  0.,  1.,  3.,  2., -3.,  1.,  1., -1.,  0., -1., -2., -3., -3.,  2., -4.,  1., -1.,  4., -4., -3.,  0.,  0.,  0.,  3.,  3., -2., -3., -4.,  4., -1.,  0., -3.,  2., -4.,  2.,  4.,  0., -4., -2.,  4., -1., -2.,  3.,  0.,  3.,  3.,  4., -3.,  1.,  4., -4., -4., -3.,  1.,  3.,  2.,  1.,  2.,  2.],\n",
      "        [-2.,  4., -4., -1.,  1., -4.,  3., -2., -2.,  3., -2., -3., -2.,  1., -4.,  0., -3.,  1., -1., -2., -3., -4., -3., -4., -3.,  0.,  1.,  4., -4., -3., -1., -2., -4., -1., -1., -1., -2., -4.,  2.,  0.,  3.,  4.,  2., -4., -2.,  0., -1., -1., -4.,  2.,  0., -4.,  1.,  1., -4.,  2., -4.,  1.,  1.,  3.,  2., -4.,  3.,  0.],\n",
      "        [-3.,  4.,  4., -1.,  4., -1.,  2.,  3., -4., -1.,  1., -3.,  1.,  4.,  0., -1.,  0.,  2., -1., -4., -2., -1.,  0., -4.,  3.,  4.,  3., -2.,  2.,  1.,  3.,  3.,  1.,  0., -4.,  0.,  1.,  2., -2.,  1.,  3.,  2.,  2., -1.,  1., -4.,  0.,  3.,  2.,  4.,  1.,  3., -4., -3.,  3.,  1., -4.,  2.,  4.,  2., -4.,  0., -2., -1.],\n",
      "        [ 4., -4.,  4., -2.,  2.,  4.,  3., -2.,  1., -1., -1., -4., -1.,  4.,  1.,  3.,  3., -2.,  1.,  1., -3., -2., -2., -3.,  2., -3.,  1.,  2., -1.,  1., -3., -2.,  2.,  3., -2., -3.,  0., -4.,  0., -3., -3.,  1.,  4., -2.,  0.,  0.,  4., -4.,  0.,  0.,  2.,  1.,  0.,  2.,  0., -3.,  3.,  3.,  2., -3.,  0.,  1., -3.,  4.],\n",
      "        [-1., -1.,  3.,  3., -3., -4.,  2., -3.,  2., -1., -1.,  4.,  4., -4., -4., -1.,  1.,  0.,  3.,  3., -3., -2., -4.,  2., -3.,  0.,  1., -1., -4.,  0., -4., -3.,  1.,  3.,  2.,  0.,  4.,  0.,  1.,  1.,  1.,  3., -2.,  0.,  3., -2.,  2., -3.,  0.,  0., -4., -1.,  0.,  3., -4.,  1., -4., -1.,  2., -4.,  3.,  2.,  4., -1.],\n",
      "        [ 1.,  1.,  4., -2.,  3.,  0.,  1.,  0., -4.,  3.,  2.,  4.,  4., -3.,  3., -1.,  1.,  3.,  1.,  4., -1.,  1., -2., -1.,  3., -4., -3.,  1.,  0., -4., -4.,  3.,  0.,  2., -4., -4.,  4., -1.,  0., -4., -2., -4.,  2., -3., -3.,  0., -2.,  1., -3.,  3.,  0., -2., -2.,  0.,  2., -4.,  0.,  3.,  1., -4.,  2., -1.,  1., -3.],\n",
      "        [ 3.,  2.,  4.,  3., -4., -2.,  1., -4., -2., -2., -3., -3.,  0., -2.,  0., -1., -4.,  2.,  2., -2., -2., -3., -4.,  2.,  2.,  2.,  0., -1.,  3., -3., -2.,  2.,  1., -2., -2., -1.,  0.,  3.,  0.,  0.,  3., -1.,  1., -4., -1., -3.,  0.,  0.,  3.,  4.,  0., -4., -4.,  1.,  1.,  0., -4.,  3.,  2.,  1.,  2.,  4., -4., -2.],\n",
      "        [ 4., -2.,  0., -3., -4.,  4., -1.,  3., -4.,  3.,  2.,  0.,  2.,  4.,  2., -2.,  3.,  4., -1.,  1., -2.,  2., -4., -4., -2.,  2.,  2., -4.,  0.,  2., -2., -3., -4.,  0., -3.,  3., -3.,  0.,  0., -2., -4., -3.,  0.,  2., -4., -3., -4.,  3.,  3., -3., -3., -3.,  3.,  4., -3., -4.,  2.,  3., -4.,  4., -1., -3.,  3., -3.],\n",
      "        [-1.,  4.,  4.,  1., -2., -1.,  1., -2., -2., -4., -4., -2.,  4.,  3.,  3.,  2.,  4.,  4.,  3.,  2.,  2.,  3.,  0., -3., -1., -1., -4.,  2., -2.,  1.,  3.,  1.,  1.,  1.,  4.,  1.,  2.,  0.,  1., -4.,  0., -1., -2., -3., -3.,  4.,  2., -4.,  1.,  0.,  0., -2., -4.,  3., -1.,  0., -3., -3., -3., -3.,  3.,  3.,  0., -3.],\n",
      "        [-1.,  0.,  2., -4.,  4., -2., -4., -2.,  0., -3., -1.,  1., -4., -4.,  2., -1., -1., -3.,  2., -1.,  4., -4.,  1.,  3., -4.,  3.,  2.,  3., -1., -2.,  4.,  2.,  0., -1.,  0., -1., -1., -1., -1.,  3., -2.,  1.,  0., -4., -1.,  4., -1.,  0., -4., -4.,  4.,  3.,  4.,  0., -1.,  4., -4., -3.,  0., -1., -3., -3.,  3.,  1.],\n",
      "        [ 0., -3.,  2.,  3.,  0.,  4.,  0.,  2.,  0.,  2.,  0.,  3.,  1.,  3., -1., -1.,  2., -4.,  4., -1., -2., -1., -4., -3., -2.,  0., -2., -3.,  1., -3.,  0.,  3., -2.,  4., -1.,  0.,  3.,  3.,  4., -2.,  0., -1.,  4.,  3.,  4., -4., -2.,  1.,  3., -3.,  4.,  3., -2.,  2.,  4.,  0., -3., -2.,  1.,  0., -3.,  3., -2., -3.],\n",
      "        [ 3.,  0.,  1.,  1., -2.,  0.,  2.,  1.,  1., -1., -1.,  0., -1., -1.,  4.,  4.,  1.,  1.,  2.,  3., -1.,  2., -3., -2.,  2.,  0.,  1.,  1., -1.,  1.,  2., -3., -4., -4.,  2., -3., -2., -1.,  2.,  3.,  2.,  3., -4.,  3., -2.,  4.,  2.,  3.,  1., -2., -1.,  2.,  4.,  0.,  4., -1., -4.,  0.,  4., -3., -3., -1., -2., -3.],\n",
      "        [-1.,  4., -4.,  1., -3.,  2., -1.,  1., -1., -3., -2.,  1., -3.,  2., -3.,  2.,  4., -2., -1.,  1.,  3., -4.,  4.,  1.,  1.,  1.,  2.,  0.,  4.,  2., -1., -4., -1.,  0.,  1.,  0.,  4.,  2.,  4., -2.,  4., -3.,  3.,  3.,  1.,  2.,  0.,  1., -4.,  1., -3.,  1., -1.,  4.,  4.,  2.,  0., -1., -3., -2., -4., -1., -4., -3.]], dtype=torch.float64)\n",
      "Operands shapes: A: torch.Size([64, 64]) B: torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Load known B matrix from file\n",
    "import numpy as np\n",
    "file_matrix = []\n",
    "file = \"Reference_b_matrix_64x64.txt\" if B_matrix_shape[0]==64 else \"Reference_b_matrix_32x64.txt\"\n",
    "with open(file, \"r\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        row = line.strip().replace('[', '').replace(']', '').split()\n",
    "        file_matrix.append([float(num) for num in row])\n",
    "file_matrix = np.array(file_matrix)\n",
    "A_matrix_new = torch.from_numpy(file_matrix)# Load into A for B·A suspicion\n",
    "B_matrix_new = torch.ones(A_matrix_shape[0], A_matrix_shape[1], requires_grad=True)\n",
    "\n",
    "print(\"Loaded_matrix: shape:\", A_matrix_new.shape, \"\\n\", A_matrix_new)\n",
    "#B_matrix_new = B_matrix_new.T\n",
    "#print(\"B_matrix_new: shape:\", B_matrix_new.shape, \"\\n\", B_matrix_new)\n",
    "# GEMM in C version seems to be doing AxB.T\n",
    "\n",
    "print(\"Operands shapes: A:\", A_matrix_new.shape, \"B:\", B_matrix_new.shape)\n",
    "\n",
    "# Create sparse vector to get a mask of the same size as the resulting product\n",
    "placeholder_output_tensor = torch.randn(A_matrix_new.shape[1], B_matrix_new.shape[0])\n",
    "placeholder_tensor_mask, placeholder_tensor_columns = nm_vector_mask_sparsify(placeholder_output_tensor, v, n, m)\n",
    "#sparse_tensor_new = SparseVNMTensor(v, n, m, dense_=placeholder_output_tensor, \n",
    "#                                    mask_=placeholder_tensor_mask, columns_=placeholder_tensor_columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight = sparse_add(a, b)\n",
    "#input =  torch.randn(256, 256, requires_grad=True)\n",
    "#grad_output = grad_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define sparse backwards operation. Input is two tensors and the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"operator=torch.mm,\n",
    "    operator=torch.mm,\n",
    "    grad_out=[torch.Tensor],\n",
    "    grad_inp=(\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "    ),\n",
    "    inp=(SparseVNMTensor, torch.Tensor ),\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "operator=torch.nn.functional.linear,\n",
    "    grad_out=None,\n",
    "    grad_inp=None,\n",
    "    inp=[torch.Tensor, SparseVNMTensor]\n",
    "    \"\"\"\n",
    "@sten.register_bwd_op_impl(\n",
    "    operator=torch.mm,\n",
    "    grad_out=[torch.Tensor],\n",
    "    grad_inp=(\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "        (sten.KeepAll, torch.Tensor),\n",
    "    ),\n",
    "    inp=(SparseVNMTensor, torch.Tensor ),\n",
    ")\n",
    "def torch_mm_bwd_impl(ctx, grad_outputs, input_sparsifiers):\n",
    "    \n",
    "    input, weights, bias = ctx.saved_tensors\n",
    "    print(f\"weights type: {type(weights)} \")\n",
    "    \n",
    "    [grad_output] = grad_outputs\n",
    "    \n",
    "    #   grad_input = grad_output @ weigths.T -> spmm(grad_output, weigths.T)\n",
    "    # Since the sparse matrix is the weights one, and needs to be transposed, \n",
    "    # this operations is changed to the following\n",
    "    #    grad_input = (weigths @ grad_output.T).T -> spmm(weigths * grad_output.T).T\n",
    "    # Now the weights can be given directly, in the compressed sparse format.\n",
    "    transposed_grad_output = grad_output.T\n",
    "    grad_input = spatha.spmm_128x64x32_32x64x32_16x8x32_2(\n",
    "                          weights.wrapped_tensor.metadata.cuda(),  # metadata\n",
    "                          weights.wrapped_tensor.columns.cuda(),   # indices\n",
    "                          weights.wrapped_tensor.values.to(dtype=torch.half).cuda(),    # values\n",
    "                          transposed_grad_output.to(dtype=torch.half).cuda(),           # rhs_matrix\n",
    "                          bias.to(dtype=torch.half).cuda(),             # bias\n",
    "                          weights.wrapped_tensor.nrows,         # A_num_rows\n",
    "                          weights.wrapped_tensor.ncols,         # A_num_cols\n",
    "                          transposed_grad_output.shape[1],          # B_num_cols\n",
    "                          weights.wrapped_tensor.v,                # V\n",
    "                          weights.wrapped_tensor.n,                # N\n",
    "                          weights.wrapped_tensor.m,                # M\n",
    "                          weights.wrapped_tensor.nnz,              # nnz\n",
    "                          0,                # seed\n",
    "                          32,               # mbrow\n",
    "                          4                 # brow\n",
    "                          ).T\n",
    "    \n",
    "    # grad_weights = torch.from_numpy(input.T @ grad_outputs)\n",
    "    # grad_input2 = torch.from_numpy(input1.wrapped_tensor.data.transpose() @ grad_output)\n",
    "    \n",
    "    #transposed_flattened_input = torch.flatten(input, start_dim=0, end_dim=-2).T\n",
    "    transposed_flattened_input = torch.flatten(input, start_dim=0, end_dim=-2).T\n",
    "    flattened_grad_output = torch.flatten(grad_output, start_dim=0, end_dim=-2)\n",
    "    #flattened_grad_output = flattened_grad_output.T # Transpose required due to internal working of sddmm kernel.\n",
    "    \n",
    "    # External inputs for testing\n",
    "    transposed_flattened_input = A_matrix_new\n",
    "    flattened_grad_output = B_matrix_new\n",
    "    \n",
    "    #print(\"A_operand: Shape: \", transposed_flattened_input.shape, \"\\n\", transposed_flattened_input)\n",
    "    #print(\"B_operand: Shape: \", flattened_grad_output.shape, \"\\n\", flattened_grad_output)\n",
    "    #print(\"B_operand transposed back: Shape: \", flattened_grad_output.T.shape, \"\\n\", flattened_grad_output.T)\n",
    "    \n",
    "    compressed_grad_weights = spatha_sddmm.sddmm(\n",
    "                          transposed_flattened_input.to(dtype=torch.half).cuda(),   # lhs operand\n",
    "                          flattened_grad_output.to(dtype=torch.half).cuda(),   # rhs operand\n",
    "                          weights.wrapped_tensor.metadata.cuda(),    # metada for output sparsity distribution\n",
    "                          #sparse_tensor.wrapped_tensor.values.to(dtype=torch.half).cuda(),    # Values for output sparsity distribution\n",
    "                          weights.wrapped_tensor.columns.cuda(),           # indices for output sparsity distribution\n",
    "                          weights.wrapped_tensor.nrows, # Since LHS is transposed, LHS.shape[0] is LHS.shape[1]\n",
    "                          weights.wrapped_tensor.ncols,         \n",
    "                          flattened_grad_output.shape[1],         \n",
    "#                          sddmm_C_matrix.wrapped_tensor.v,          \n",
    "                          weights.wrapped_tensor.n,                # N\n",
    "                          weights.wrapped_tensor.m,                # M\n",
    "                          weights.wrapped_tensor.nnz,              # nnz\n",
    "                          0,                # seed\n",
    "                          32,               # mbrow\n",
    "                          4                 # brow\n",
    "                          )\n",
    "    # Create a SparseVNMTensor from weights SparseVNMTensor data to densify sddmm output.\n",
    "    grad_weights = SparseVNMTensor(weights.wrapped_tensor.v, weights.wrapped_tensor.n, weights.wrapped_tensor.m, \n",
    "                              mask_=weights.wrapped_tensor.mask, columns_=weights.wrapped_tensor.columns, \n",
    "                              values_=compressed_grad_weights, metadata_=weights.wrapped_tensor.metadata)\n",
    "    \n",
    "    #print(f\"compressed_grad_weights type: {type(compressed_grad_weights)} and dtype: {compressed_grad_weights.dtype}\")\n",
    "    #print(f\"grad_weights SparseVNMTensor type: {type(grad_weights)} and data: {grad_weights}\")\n",
    "    densified_grad_weights = grad_weights.to_dense()\n",
    "    #dense_grad_weights = input.T.to(dtype=torch.half).cuda() @ grad_output.to(dtype=torch.half).cuda()\n",
    "    #print(\"densified_grad_weights: Shape: \", densified_grad_weights.shape, \"\\n\", densified_grad_weights)\n",
    "    \n",
    "    # Compute dense version and check they are the same\n",
    "    dense_grad_weights = torch.from_numpy(transposed_flattened_input.detach().numpy() @ flattened_grad_output.detach().numpy()\n",
    "                                      ).to(device=\"cuda:0\").to(dtype=torch.half)\n",
    "    #print(\"dense_grad_weights: transposed_flattened_input @ B_matrixflattened_grad_output_new: shape:\", \n",
    "    #      dense_grad_weights.shape,\"\\n\", dense_grad_weights)\n",
    "    masked_dense_grad_weights = torch.multiply(dense_grad_weights, weights.wrapped_tensor.mask.to(device=\"cuda:0\"))\n",
    "    #print(\"masked_dense: shape:\", masked_dense_grad_weights.shape, \"\\n\", masked_dense_grad_weights)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print( torch.allclose(densified_grad_weights.cpu(), masked_dense_grad_weights.cpu()) )\n",
    "    #return grad_input, grad_weights \n",
    "    #return grad_input.float().cpu(), dense_grad_weights.cpu()\n",
    "    \n",
    "    return grad_input.float().cpu(), densified_grad_weights.cpu()\n",
    "    #densified_grad_weights.float().cpu()\n",
    "    #return torch.zeros(256, 256), torch.zeros(256, 256)\n",
    "    #return sten.SparseTensorWrapper.wrapped_from_dense(\n",
    "    #    SparseVNMTensor(sparsifier.v, sparsifier.n, sparsifier.m, dense_=tensor, mask_=masks, columns_=columns),\n",
    "    #    tensor,\n",
    "    #    grad_fmt,\n",
    "    #)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute dense version for grad_input: operation grad_outputs @ weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#[grad_output] = grad_d\n",
    "# Operation should be grad_d @ weights.T, but in order to match C benchmark results, second operand has to be transposed.\n",
    "dense_grad_input = torch.from_numpy(grad_d.detach().numpy() @ weights.detach().numpy()\n",
    "                                    ).to(device=\"cuda:0\").to(dtype=torch.half)\n",
    "\n",
    "#print(dense_grad_input)\n",
    "\n",
    "#bias = torch.ones((dense.shape))*2\n",
    "#dense+=bias.to(dtype=torch.half).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute sparse version of the operation input.T @ grad_output as sddmm(input.T,  grad_output, sparse) through backward execution of the operation.\n",
    "\n",
    "Uses sddmm.\n",
    "\n",
    "This is done by calling the backwards method that computes two outputs, grad_input and grad_weights. Only grad_weigths uses sddmm operand, but the full backwards operation is needed to use the sparse versions through sten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)\n",
      "weights type: <class 'sten.sten.SparseTensorWrapper'> \n",
      "True\n",
      "max_idx: 0\n",
      "Sparse after backwards: tensor(0., device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#torch_mm_output.retain_grad()\n",
    "loss = torch.sum(torch_mm_output); \n",
    "print(\"weights:\", loss)\n",
    "#print(\"loss:\", loss)\n",
    "#print(\"torch_mm_output grad before backwards:\", torch_mm_output.grad)\n",
    "loss.backward()\n",
    "#torch_mm_output.backward(grad_d.to(device=\"cuda:0\").to(dtype=torch.half))\n",
    "print(\"Sparse after backwards:\", loss)\n",
    "#print(\"sparse grad after backwards:\", sparse.grad.shape, \"\\n\", sparse.grad)\n",
    "#print(\"sparse_grad_input:\", sparse_grad_input, \"sparse_grad_weights:\", sparse_grad_weights)\n",
    "#print(\"sparse_grad_input dtype:\", sparse_grad_input.dtype, \"sparse_grad_weights dtype:\", sparse_grad_weights.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolated kernel verification.\n",
    "Use dense computation and direct call to sddmm kernel to test correct execution\n",
    "\n",
    "Set matrix sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute dense version for grad_weights: input.T @ grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_matrix_new: shape: torch.Size([64, 64]) \n",
      " tensor([[ 0.,  2.,  1.,  0.,  1.,  4., -1., -4., -2., -3.,  4.,  2., -2., -3., -2.,  3.,  4.,  3.,  0., -4., -1.,  3.,  1., -4., -1.,  0., -3.,  3., -4.,  0.,  2.,  0.,  4., -2., -4., -2.,  4., -3., -2., -4., -4.,  4.,  2., -2.,  3., -4.,  4.,  3.,  3., -1.,  1.,  4., -4., -4.,  2., -1.,  0.,  4.,  4.,  1., -3.,  1.,  3., -4.],\n",
      "        [ 1.,  1., -1.,  1.,  2., -1., -1.,  3., -3.,  3.,  3.,  4.,  3.,  0.,  2., -1.,  4., -1., -3.,  4., -3.,  1., -4.,  1., -2.,  4.,  4., -1.,  1.,  3.,  0., -3., -3.,  1.,  0.,  4.,  4.,  3.,  0., -4.,  2., -2.,  3., -2.,  3.,  0.,  0.,  0.,  1., -1., -3.,  2.,  4., -3., -3., -2., -4., -4., -1., -1.,  1.,  3., -1.,  3.],\n",
      "        [-3.,  3.,  0.,  4.,  0.,  4.,  2.,  4.,  4.,  0.,  4.,  0., -4., -1.,  2., -1.,  0.,  3.,  4., -2.,  2.,  3., -2.,  1.,  1.,  1.,  4., -2., -2., -2.,  3., -1.,  4., -2., -2., -1.,  4., -4., -2.,  1.,  0., -3., -3., -2., -1.,  3.,  0.,  1.,  0., -1.,  3., -3.,  4., -4.,  0.,  0.,  2.,  0.,  0.,  2.,  0., -2., -4., -1.],\n",
      "        [-2., -1.,  0.,  4., -3.,  2.,  1.,  1.,  2.,  2.,  2.,  3., -2., -3., -1.,  2., -2., -4.,  2., -3.,  3., -3.,  1.,  0., -1., -3.,  4.,  2., -3., -4., -4.,  0., -3.,  1., -1., -2., -4.,  2.,  1.,  0., -1., -2., -2.,  0., -3.,  0., -3., -1.,  0.,  1., -2., -2.,  1.,  4.,  1.,  2.,  3.,  0., -1.,  2., -2.,  0.,  4., -1.],\n",
      "        [ 3., -3.,  1.,  3.,  3., -2., -3., -4., -2., -1.,  0.,  0.,  1., -1.,  3.,  3., -4., -3., -3., -1.,  3.,  0., -4., -1.,  2., -2., -4., -4.,  0.,  3., -1., -2.,  4.,  3.,  3.,  2., -4.,  2.,  2., -2., -3., -3.,  2.,  0.,  1., -1., -1., -1., -2., -2.,  0., -4.,  2., -2., -3., -3.,  0.,  4., -3.,  4.,  2., -1.,  4.,  1.],\n",
      "        [-3.,  1., -1., -3., -2.,  3., -3., -3., -4.,  2.,  2., -1., -4.,  3.,  0., -2., -4.,  4., -4.,  2., -3.,  4.,  3.,  0.,  3.,  3., -3.,  1., -3.,  4., -3., -2., -2., -2., -3., -2., -3., -2.,  0.,  4.,  2.,  4., -2.,  2.,  0.,  2.,  2.,  0., -1.,  0.,  4.,  0., -1.,  0.,  2., -2., -2.,  4.,  1., -3.,  3.,  2., -1., -4.],\n",
      "        [-4., -2., -2.,  4.,  1.,  0.,  3., -4., -3., -4.,  3.,  1.,  0., -2., -4.,  3.,  1.,  2., -4.,  4., -2.,  3.,  4.,  0.,  0.,  0.,  2., -2., -4.,  3., -4.,  3., -3., -2.,  2.,  0.,  1.,  0.,  0.,  0., -2., -4., -3.,  2., -1.,  4., -2.,  2.,  2., -2.,  1.,  4.,  3., -1., -3., -2.,  1.,  3., -2.,  1.,  0., -2., -1.,  1.],\n",
      "        [-1., -4.,  3.,  2., -2., -4., -3.,  0., -3., -2., -3., -2., -4., -3.,  4.,  2., -2., -2., -1., -4., -1.,  0., -3., -4., -3., -3., -1.,  1.,  0.,  3.,  4.,  1.,  1.,  2., -2.,  2.,  2., -3., -3.,  1.,  0., -3.,  3., -2., -2.,  0.,  4.,  0.,  1., -2., -1.,  4.,  2., -2.,  4.,  3., -1., -1., -3.,  3.,  4., -4., -3., -2.],\n",
      "        [ 0., -3.,  4., -4., -1.,  4.,  1.,  1., -4., -2.,  3., -4.,  0.,  2.,  1., -4.,  2.,  2., -4., -3.,  4.,  2., -4., -2.,  3.,  4.,  4.,  2., -4., -4.,  3.,  0., -4.,  2.,  0., -1., -1.,  4.,  4., -1.,  4.,  2.,  0., -1., -3.,  3.,  0.,  3., -2., -2.,  4., -3.,  4.,  2., -3.,  3.,  2., -4., -2.,  0.,  4., -4.,  4.,  2.],\n",
      "        [ 1., -2., -4.,  4.,  4.,  2., -3.,  3., -1., -1.,  4., -2.,  4., -1., -4., -3.,  1.,  2., -2., -1., -2., -3.,  4.,  4., -4., -3., -1.,  2., -4., -4., -1.,  1., -4., -1., -2.,  2., -4., -3.,  0., -3.,  0.,  0., -1., -1.,  1., -3., -2., -4.,  1., -2., -3.,  3.,  0., -4.,  0.,  0., -3.,  1., -3., -3.,  2.,  0.,  0.,  0.],\n",
      "        [ 1.,  1., -2., -1.,  2.,  0.,  0., -4.,  2.,  1., -3., -1.,  2.,  0., -3., -2.,  0., -2.,  4.,  4., -2., -1., -3., -3., -4., -4., -3.,  0., -2.,  1.,  2.,  3.,  4.,  4., -3.,  0., -2., -1., -2.,  4.,  4., -1., -4., -1.,  1., -3.,  2., -3., -3., -1.,  3., -1.,  2., -4., -1.,  0.,  3.,  0.,  2., -3.,  3.,  0.,  2.,  3.],\n",
      "        [-3.,  2., -2., -1., -4.,  0., -3.,  3.,  1., -3., -3., -2., -3.,  1., -3., -4.,  4.,  2., -2.,  0.,  2.,  1.,  4., -1., -4.,  2.,  0.,  3.,  4.,  4.,  1., -4.,  1.,  1., -2.,  0.,  3., -1., -2.,  0., -2., -3.,  0., -3.,  3., -1., -2.,  0., -4.,  0.,  4.,  1., -4.,  4.,  4., -4., -1., -3.,  1., -2., -4., -2., -4.,  2.],\n",
      "        [ 1., -2.,  4.,  0., -1.,  4.,  2.,  2., -4., -3.,  1.,  1., -2.,  3., -3., -2., -4., -4.,  3., -4.,  2.,  0.,  3.,  3.,  1.,  0.,  3.,  2.,  0.,  4., -3.,  3., -3.,  3., -2.,  1.,  2.,  2., -4.,  0.,  1.,  1., -3.,  3.,  0., -4.,  4., -2.,  3.,  2., -1., -1., -4.,  4., -3.,  1., -3., -4., -4., -1.,  4.,  4., -2.,  3.],\n",
      "        [ 0., -2., -3., -3., -4., -3.,  0., -1.,  1.,  1., -2.,  3., -1., -3., -4., -3.,  1., -1., -2.,  1., -3.,  0., -4., -4., -2.,  3.,  0.,  4.,  1.,  0.,  2., -4.,  2.,  2., -2.,  0.,  3.,  0.,  4., -1., -4.,  4., -3., -3., -4., -2., -4., -1., -1., -1.,  3., -2.,  3.,  3., -1.,  3., -1.,  1.,  2.,  4., -4., -3., -4.,  0.],\n",
      "        [ 3., -4.,  3.,  0.,  0.,  0.,  1., -2., -1.,  1., -3., -3.,  3., -3., -2.,  4.,  0., -4., -4., -4.,  1., -1.,  3., -4.,  4., -2.,  4.,  2.,  0.,  2., -3., -4.,  0.,  4.,  0.,  2., -3., -3.,  2.,  0.,  0.,  3., -1., -4., -4.,  2.,  4.,  0.,  0.,  2.,  1., -3., -4., -3., -3.,  2.,  0.,  3., -3.,  2., -1., -1.,  2.,  3.],\n",
      "        [-4., -4.,  1., -2.,  4., -4.,  0., -1.,  4.,  2., -1.,  4., -3., -4., -3.,  1.,  3.,  2.,  2.,  1.,  2.,  1., -3., -3.,  0., -2.,  3.,  3., -1., -2., -1., -2., -4.,  2., -2.,  4.,  3.,  0., -2.,  2., -3., -1., -1., -4.,  0.,  0.,  2., -4., -2., -3.,  1.,  4.,  3.,  0.,  3., -4.,  3., -1.,  3.,  4., -1.,  4., -3.,  0.],\n",
      "        [ 2., -1., -1., -2.,  4.,  0.,  4.,  3.,  1., -2.,  4., -4.,  2., -1.,  3.,  2.,  0., -2., -1., -2.,  2.,  4., -4., -2., -2.,  3., -2.,  0.,  1., -1.,  4., -4.,  1., -2., -2., -2.,  0.,  4., -4., -3., -3.,  4.,  4.,  1., -4.,  3., -3.,  1., -4., -2.,  1.,  2., -4.,  2., -4., -4., -2., -4.,  0.,  3., -1., -1.,  3.,  2.],\n",
      "        [ 0.,  3.,  4.,  4.,  2., -4., -4.,  1.,  2.,  3., -2.,  3.,  1., -3., -3.,  1., -3.,  2., -4., -3., -1.,  3., -2.,  0.,  3.,  2., -4., -4., -3.,  4.,  2., -1.,  2.,  2., -2., -2.,  0., -3.,  3.,  4.,  4.,  3.,  0.,  0.,  4.,  1.,  3., -3., -3.,  1., -4., -2.,  0., -2.,  2., -2.,  3.,  3., -4.,  4.,  0.,  3., -2.,  4.],\n",
      "        [-2., -2., -3.,  0., -1., -4., -3., -4.,  3.,  2., -2.,  1., -4., -4.,  0., -3.,  0.,  0.,  0.,  4.,  3., -3.,  4., -1.,  2., -4., -4., -3.,  1., -4., -3.,  1., -2., -4.,  3.,  0., -4.,  4.,  0.,  2.,  1.,  1., -2.,  2., -1.,  2.,  1.,  3., -3., -4.,  2.,  2., -4.,  2., -4.,  2.,  0., -4.,  2.,  3., -4.,  1., -1., -3.],\n",
      "        [ 1.,  4.,  1.,  2.,  3.,  3., -1., -2., -1.,  1.,  2.,  3., -4., -2.,  1., -3., -1., -3.,  2., -3.,  1.,  2.,  3., -4.,  1., -2.,  3., -1.,  3.,  4.,  0.,  0.,  3., -4.,  4., -1.,  2., -4.,  1., -4.,  1., -2.,  1., -1., -1., -2.,  0.,  2., -1.,  4.,  1.,  4.,  0., -3.,  2., -4.,  0., -2., -1., -2.,  4.,  4.,  0.,  0.],\n",
      "        [ 2., -1.,  3., -1., -1.,  0., -2.,  2.,  0.,  3., -4.,  3., -4., -1., -2., -3., -2.,  3.,  3.,  0.,  3., -2.,  0., -2., -2.,  2.,  0., -3., -1.,  3.,  1., -4., -3., -2., -2.,  1.,  0.,  0., -2.,  4., -4., -3.,  1.,  4.,  0.,  1., -4.,  0., -2.,  2., -4., -4.,  4., -2., -2., -4.,  4.,  0., -3., -4., -2.,  1.,  4.,  0.],\n",
      "        [ 3., -3.,  3., -2., -1.,  3.,  4., -1.,  2., -2., -2.,  4.,  4., -3.,  0., -3.,  3., -2., -3.,  0., -2., -3., -2., -4.,  1.,  0., -4.,  2.,  3.,  4.,  4., -1.,  3.,  2.,  1.,  4., -2., -2., -1., -4.,  1., -1.,  4.,  0.,  0., -3., -1., -4., -3.,  0., -1.,  0.,  1.,  1.,  0.,  4.,  3.,  0., -1.,  1., -2., -2.,  4.,  3.],\n",
      "        [ 2., -2.,  3.,  3.,  1., -3.,  3.,  4.,  0.,  0., -3.,  3.,  2.,  0.,  1.,  3.,  4.,  4., -2., -2., -1.,  0.,  4., -3.,  3., -1.,  1., -4., -1., -2.,  3.,  4.,  1.,  1.,  2.,  4.,  0.,  0.,  1.,  3.,  4.,  2., -1., -1., -3.,  4., -2., -4.,  2., -2., -4., -4.,  3.,  2.,  4., -1.,  3.,  0., -1., -2.,  3.,  4., -3., -3.],\n",
      "        [-1.,  3., -4.,  1., -4.,  1., -1.,  4., -2.,  0., -3., -3.,  0., -1., -3.,  4., -1.,  4.,  4.,  4.,  1.,  1., -2.,  0., -3., -1.,  0.,  2., -3., -1.,  3.,  0., -3.,  1., -4., -3.,  4., -2.,  3., -3.,  2.,  4., -2.,  4., -4., -3.,  3., -2.,  3.,  0.,  4., -3., -3., -2., -1., -4., -1.,  3.,  2.,  0., -3., -2.,  4., -4.],\n",
      "        [ 3.,  3.,  4.,  2.,  3.,  1.,  1., -2., -2.,  1., -2., -3.,  0.,  3., -1., -4., -3., -2., -3., -4., -2.,  1.,  3.,  2., -3.,  0., -3., -3.,  0.,  4., -3., -2.,  2.,  4.,  2.,  0.,  0., -4.,  0.,  2.,  1.,  2.,  1.,  3., -1.,  4.,  3., -2., -4.,  2., -4., -4., -2.,  3.,  2., -2., -2.,  2., -1.,  2.,  1., -2.,  2., -4.],\n",
      "        [-3., -3., -2., -1., -3.,  2.,  4.,  0., -2.,  0., -2.,  1.,  0.,  3.,  1., -2.,  0.,  1., -2.,  1., -1.,  3.,  3., -1.,  0., -3.,  3., -4., -1., -2., -4., -1., -1., -2.,  2.,  0.,  4.,  1.,  2.,  4., -3.,  2., -2., -1.,  0.,  3.,  1.,  2., -3.,  1., -2.,  0., -1.,  3.,  1.,  3.,  4., -1.,  1., -3.,  1., -1.,  0.,  2.],\n",
      "        [-1.,  4.,  4., -1.,  1.,  1., -4.,  0., -4., -1.,  3.,  0., -3., -3., -4., -1.,  2., -4.,  1.,  4.,  4., -2.,  2.,  3., -1., -3.,  2., -4.,  0., -3.,  0.,  4., -3.,  0., -4.,  0.,  3., -4.,  4.,  4., -1.,  0., -3., -1.,  1., -3.,  2., -4., -2., -4.,  4., -3., -2.,  0.,  2.,  2.,  1.,  0.,  0., -4., -1.,  4.,  2., -2.],\n",
      "        [-3.,  2.,  0., -4.,  1., -1.,  4.,  4.,  3., -4., -2., -3., -2.,  2., -2., -2.,  3., -3., -3.,  3.,  1.,  3.,  0.,  4., -4.,  2.,  3.,  0.,  0.,  0.,  0.,  1., -4., -4.,  1.,  1., -3.,  0.,  0.,  3., -1.,  1.,  4., -1., -4., -3.,  1.,  3., -3.,  2.,  0.,  0., -2.,  2.,  0., -1.,  0., -2.,  1.,  4.,  0.,  3., -2.,  0.],\n",
      "        [ 3.,  2., -4.,  3.,  4., -1.,  1., -2.,  2.,  0.,  1.,  2.,  0.,  4., -1.,  1.,  1.,  3.,  3.,  2.,  0., -2.,  3.,  2., -2., -3., -1.,  1.,  4.,  2., -4.,  1., -1.,  4., -1., -4., -4.,  4., -1.,  0., -1.,  4.,  4.,  1.,  2., -2., -3., -4.,  3., -4.,  2., -3., -4.,  0.,  3., -3.,  1., -4.,  2., -1.,  2.,  0.,  4.,  3.],\n",
      "        [-3., -4.,  3.,  4.,  4.,  4., -1., -4.,  2., -3.,  2., -1., -3.,  1., -1., -4., -1.,  4.,  4., -2., -1.,  2., -1.,  3.,  2.,  3.,  4., -3., -4.,  3.,  2., -4.,  1., -1.,  2.,  0., -4.,  4., -2.,  2., -4.,  2.,  3., -3., -1., -2.,  4.,  0., -3.,  3.,  2., -2., -1.,  3.,  3.,  3.,  1.,  2.,  3.,  0., -2.,  0., -2.,  3.],\n",
      "        [ 1.,  4., -4.,  0.,  3., -1., -3.,  3., -4., -4., -4., -3., -4.,  2.,  2.,  4.,  1., -1.,  4.,  4., -2.,  3.,  2.,  1., -2.,  0.,  3.,  1.,  3., -4., -3., -3., -4., -2.,  1.,  1., -1.,  1.,  0., -3., -1., -2., -1., -1.,  4.,  3., -4., -2., -3.,  4., -3., -3.,  0.,  4.,  1.,  3., -3., -1., -1.,  4., -2., -2.,  4., -4.],\n",
      "        [ 0.,  0.,  1.,  3., -4.,  3., -4., -1., -4., -3.,  0.,  3.,  4.,  0., -4.,  3., -2., -2., -4.,  2.,  4., -1.,  0.,  3.,  0.,  1.,  1.,  2.,  4.,  0.,  2., -3.,  2., -3.,  3.,  3.,  4.,  1.,  4.,  3.,  2., -2.,  1., -1.,  2.,  1., -3.,  2.,  1.,  4.,  0., -2., -2.,  2.,  4.,  3., -3.,  0., -2., -4.,  2.,  3.,  4., -1.],\n",
      "        [ 4.,  2.,  4.,  3., -2.,  2.,  1.,  2.,  4.,  4., -4., -1., -1., -4., -3.,  4.,  4.,  1., -4., -4., -4.,  4.,  1., -3., -3.,  1.,  4.,  3., -1.,  4., -3., -2., -1.,  4.,  4.,  2.,  1., -2., -3.,  0., -4., -4.,  3., -1., -4.,  4., -4.,  2., -2.,  3.,  0., -1.,  2., -4., -2.,  3.,  0., -2., -1.,  3., -3.,  1.,  4.,  0.],\n",
      "        [ 0.,  1.,  4., -4.,  1., -3., -2.,  1., -3., -4.,  4.,  4., -4.,  3.,  1., -4.,  1.,  4., -1., -3.,  2.,  2.,  2., -3.,  2., -3., -4.,  3.,  0.,  2., -4.,  2., -2.,  3.,  0.,  3.,  2.,  2.,  0.,  1.,  2., -3.,  0.,  0.,  4.,  3.,  1.,  1.,  2.,  2.,  2., -2., -3., -1., -1.,  3., -2., -1.,  1.,  2., -4.,  0., -1., -4.],\n",
      "        [-4.,  3.,  4.,  2.,  0., -3., -2., -4., -2.,  0.,  0., -4., -4.,  3.,  1.,  1.,  1., -4.,  3.,  2., -3., -3.,  1.,  0., -1.,  4., -3., -3., -3., -1., -2., -3., -3.,  4.,  3.,  0., -4.,  3.,  0., -2., -4.,  2., -2.,  4.,  1.,  1.,  0., -3., -1., -4.,  1.,  1.,  4., -3.,  3., -2.,  4.,  2., -2., -4., -4.,  0., -2., -4.],\n",
      "        [-3.,  3., -2., -3.,  0.,  2., -3., -2., -3., -2., -3.,  2.,  3., -1.,  1., -4., -3., -4., -1., -3., -3., -3., -3.,  3.,  1., -1.,  4.,  0.,  1.,  4., -2.,  2.,  0.,  0.,  1.,  4.,  4.,  3., -4., -4., -4.,  4.,  1.,  1., -1.,  4., -1., -2.,  4.,  2., -1.,  4.,  1.,  1.,  2., -4.,  2., -1.,  0., -1., -2.,  2.,  3.,  3.],\n",
      "        [-3.,  0.,  0., -4., -4.,  0.,  3., -4.,  0., -1., -1.,  1., -3.,  3.,  3., -4., -2., -2.,  2.,  2.,  1.,  0.,  2., -2.,  1., -3., -1.,  4.,  1., -2.,  2.,  2.,  0., -3.,  0.,  0.,  2., -2., -2.,  4.,  0., -1.,  0.,  1.,  4., -2., -1., -2., -2.,  4.,  2.,  3., -3., -1.,  4.,  2.,  0., -2.,  1.,  3., -2., -4.,  0.,  2.],\n",
      "        [-2.,  2.,  4.,  2.,  2., -3., -1., -3., -1.,  3.,  0., -2.,  3.,  2., -2., -3.,  1.,  4.,  2.,  0., -4.,  1., -3., -2.,  2.,  3., -4.,  4.,  3., -2.,  0.,  3.,  4., -1., -2.,  2., -1.,  1.,  1.,  0., -3.,  4.,  2., -4., -1.,  3.,  4.,  2.,  0.,  1., -3.,  1., -4., -2.,  3.,  2.,  3.,  4., -1.,  0., -3.,  3., -4.,  4.],\n",
      "        [-4., -1.,  1., -3.,  2.,  4.,  1.,  4.,  3., -2.,  4.,  4.,  3.,  3.,  1., -1., -3.,  2.,  2.,  4.,  3.,  0., -1.,  1., -3.,  3.,  3., -1., -1.,  4., -4., -3., -4., -1., -2.,  2., -2.,  3.,  1.,  4.,  4.,  0.,  3.,  2., -4.,  0.,  3., -4., -3.,  0.,  4.,  4., -4., -3., -1.,  4.,  4., -3., -4., -4.,  3., -4., -2.,  3.],\n",
      "        [-1., -2.,  1.,  2., -3., -3., -1., -4.,  0., -3.,  0., -2.,  1., -1., -4.,  1.,  3., -4., -2.,  1., -3.,  1.,  1.,  3.,  1.,  1.,  1., -1.,  2.,  1., -2., -4.,  4.,  1.,  0.,  3.,  2.,  3.,  3.,  4., -4., -2., -4., -1., -1., -4.,  2., -2.,  3., -4.,  1.,  4., -1., -3.,  0.,  2.,  1.,  4.,  4., -2.,  0., -3., -4., -3.],\n",
      "        [ 0., -1.,  4.,  4., -3.,  2.,  4.,  4.,  3.,  2., -3., -3.,  2.,  3., -3.,  1.,  1.,  3., -2.,  3.,  2.,  1., -2., -2.,  0., -3., -2.,  2., -4., -3.,  4.,  0.,  0.,  3.,  0.,  1., -1., -3., -1.,  4.,  3.,  0.,  3.,  0., -4., -4.,  3.,  0.,  1., -3., -2., -2.,  0.,  0., -1.,  4.,  0.,  1., -1.,  0.,  2., -2., -4., -3.],\n",
      "        [ 4.,  0.,  1., -2.,  1.,  4.,  4., -3., -3.,  2.,  1., -3.,  0., -2.,  1., -4., -3.,  1., -3.,  1.,  4.,  0., -2., -1.,  3.,  1.,  1.,  1.,  4.,  1.,  0.,  1.,  3.,  3.,  1., -3.,  0.,  0., -2.,  1., -3.,  2.,  1.,  2.,  4.,  4.,  0., -4., -1.,  1.,  1., -2., -4.,  3., -1.,  2., -3.,  4., -2.,  3., -1.,  0., -1., -3.],\n",
      "        [-3.,  4., -1.,  1., -3., -1., -4., -3., -4., -1.,  1.,  4., -4., -4.,  2., -1.,  0., -2., -1., -2.,  3.,  2.,  4.,  3.,  0.,  4.,  1.,  3.,  0., -4.,  2.,  1.,  2.,  3.,  4.,  2., -2.,  4.,  3., -4., -4., -1.,  3., -3., -3.,  0.,  0.,  1.,  1.,  2.,  4., -3., -3.,  3.,  4.,  1.,  3.,  1., -3., -4., -1.,  4., -1.,  3.],\n",
      "        [ 2., -4.,  0.,  4.,  3., -4., -4.,  3., -3.,  1.,  4., -1., -4., -2.,  2.,  1.,  4., -1.,  1.,  3., -2.,  0.,  0.,  3.,  3., -1.,  1., -3., -2.,  2.,  3., -4.,  2., -2.,  2.,  0., -3.,  0., -1., -2., -4., -4., -1.,  4., -2., -3., -2.,  4.,  0.,  3.,  1.,  0., -1.,  3., -4.,  4., -3.,  1.,  4.,  0., -2.,  2., -2.,  3.],\n",
      "        [ 2.,  2., -2.,  3., -2., -1., -3., -4.,  0.,  0.,  4.,  0., -1., -2., -2.,  4., -4.,  3., -3., -3.,  1., -2.,  4.,  2.,  3.,  3.,  4.,  4.,  0., -3.,  2., -3.,  2.,  2.,  3.,  4., -3.,  2.,  4.,  1.,  4.,  2., -4., -1.,  4., -2., -4.,  2.,  3., -3.,  2., -1., -3.,  1.,  4.,  3., -3.,  3.,  2.,  1.,  3., -3.,  0.,  0.],\n",
      "        [ 3., -4., -3.,  4.,  2.,  3., -2.,  2., -2., -1., -4., -3., -1.,  3.,  2., -4.,  2., -3., -1.,  4.,  0., -4.,  2.,  1.,  2., -3.,  4.,  0., -2., -1.,  2.,  3.,  0.,  1.,  3.,  4., -3., -4.,  1.,  0., -3., -1., -1., -1., -3., -4., -1.,  2.,  4.,  0.,  1., -1.,  1., -2.,  2., -2., -1.,  1.,  0., -1., -4.,  4., -3., -2.],\n",
      "        [-2.,  2., -3., -1.,  1.,  1.,  3.,  0.,  2., -2.,  3.,  2., -4.,  4., -1., -4., -2.,  4., -3.,  3.,  4.,  4.,  3., -4., -2., -4., -1., -2.,  2., -2., -2.,  4., -4., -2., -2.,  1.,  1.,  4.,  3., -2., -3.,  0.,  4.,  4., -1., -4.,  2.,  1.,  2.,  4., -3.,  1.,  1.,  2.,  1.,  3.,  2.,  2.,  4., -1., -4.,  4., -4.,  3.],\n",
      "        [ 4., -1., -3.,  0., -2.,  4.,  1., -3., -1., -2., -4.,  1., -1.,  3.,  4., -4.,  0.,  4.,  2.,  3.,  1., -4., -1., -3.,  1., -2., -2.,  1., -4., -1., -3.,  4.,  0., -2.,  0.,  0.,  4., -4.,  1., -1., -2., -1.,  2., -1.,  4.,  1.,  0., -3., -2.,  4.,  2.,  4.,  2., -3.,  3., -2., -3., -4.,  3., -3., -1.,  4., -3.,  1.],\n",
      "        [ 4., -1., -4.,  4., -3., -1., -4., -3.,  1.,  2.,  1.,  0., -4.,  3.,  0., -1.,  2., -3., -4., -1., -4.,  3.,  0., -2.,  3., -2., -3., -4., -4., -4.,  1.,  2., -1.,  2.,  1., -2., -4., -1.,  0.,  1.,  3.,  3.,  4.,  4., -1., -3., -4.,  3., -4., -4., -3., -3.,  3., -1., -3., -1.,  2., -2., -1.,  0., -1., -4.,  4.,  0.],\n",
      "        [ 2., -1.,  3.,  0.,  2., -4.,  4.,  1.,  1.,  1., -2.,  4.,  0., -2.,  2.,  0., -2.,  1., -1.,  3., -4.,  0., -3.,  0.,  3.,  1.,  2., -3., -1.,  1.,  1.,  3.,  4., -3., -2.,  0., -3.,  4., -4.,  0., -2., -2., -1.,  2., -2., -4., -2.,  0.,  2.,  1., -4.,  0., -3., -3.,  4.,  2.,  2.,  1.,  1., -4., -3.,  4.,  1.,  4.],\n",
      "        [-3.,  4., -1., -4.,  1., -1.,  1.,  4., -1.,  4.,  1.,  1.,  3.,  1.,  3., -2., -2.,  3.,  2., -3.,  2.,  1.,  1., -2.,  4., -3., -4.,  3., -4.,  2.,  2.,  4.,  1., -4., -4.,  4., -3., -1.,  3., -2., -2.,  0.,  1.,  3.,  3., -1., -4., -4.,  4.,  2.,  4.,  2., -4.,  0.,  4.,  4.,  1.,  2.,  3.,  0., -1.,  0., -1.,  2.],\n",
      "        [-1., -1.,  2., -2.,  2.,  0.,  1.,  3.,  2., -3.,  1.,  1., -1.,  0., -1., -2., -3., -3.,  2., -4.,  1., -1.,  4., -4., -3.,  0.,  0.,  0.,  3.,  3., -2., -3., -4.,  4., -1.,  0., -3.,  2., -4.,  2.,  4.,  0., -4., -2.,  4., -1., -2.,  3.,  0.,  3.,  3.,  4., -3.,  1.,  4., -4., -4., -3.,  1.,  3.,  2.,  1.,  2.,  2.],\n",
      "        [-2.,  4., -4., -1.,  1., -4.,  3., -2., -2.,  3., -2., -3., -2.,  1., -4.,  0., -3.,  1., -1., -2., -3., -4., -3., -4., -3.,  0.,  1.,  4., -4., -3., -1., -2., -4., -1., -1., -1., -2., -4.,  2.,  0.,  3.,  4.,  2., -4., -2.,  0., -1., -1., -4.,  2.,  0., -4.,  1.,  1., -4.,  2., -4.,  1.,  1.,  3.,  2., -4.,  3.,  0.],\n",
      "        [-3.,  4.,  4., -1.,  4., -1.,  2.,  3., -4., -1.,  1., -3.,  1.,  4.,  0., -1.,  0.,  2., -1., -4., -2., -1.,  0., -4.,  3.,  4.,  3., -2.,  2.,  1.,  3.,  3.,  1.,  0., -4.,  0.,  1.,  2., -2.,  1.,  3.,  2.,  2., -1.,  1., -4.,  0.,  3.,  2.,  4.,  1.,  3., -4., -3.,  3.,  1., -4.,  2.,  4.,  2., -4.,  0., -2., -1.],\n",
      "        [ 4., -4.,  4., -2.,  2.,  4.,  3., -2.,  1., -1., -1., -4., -1.,  4.,  1.,  3.,  3., -2.,  1.,  1., -3., -2., -2., -3.,  2., -3.,  1.,  2., -1.,  1., -3., -2.,  2.,  3., -2., -3.,  0., -4.,  0., -3., -3.,  1.,  4., -2.,  0.,  0.,  4., -4.,  0.,  0.,  2.,  1.,  0.,  2.,  0., -3.,  3.,  3.,  2., -3.,  0.,  1., -3.,  4.],\n",
      "        [-1., -1.,  3.,  3., -3., -4.,  2., -3.,  2., -1., -1.,  4.,  4., -4., -4., -1.,  1.,  0.,  3.,  3., -3., -2., -4.,  2., -3.,  0.,  1., -1., -4.,  0., -4., -3.,  1.,  3.,  2.,  0.,  4.,  0.,  1.,  1.,  1.,  3., -2.,  0.,  3., -2.,  2., -3.,  0.,  0., -4., -1.,  0.,  3., -4.,  1., -4., -1.,  2., -4.,  3.,  2.,  4., -1.],\n",
      "        [ 1.,  1.,  4., -2.,  3.,  0.,  1.,  0., -4.,  3.,  2.,  4.,  4., -3.,  3., -1.,  1.,  3.,  1.,  4., -1.,  1., -2., -1.,  3., -4., -3.,  1.,  0., -4., -4.,  3.,  0.,  2., -4., -4.,  4., -1.,  0., -4., -2., -4.,  2., -3., -3.,  0., -2.,  1., -3.,  3.,  0., -2., -2.,  0.,  2., -4.,  0.,  3.,  1., -4.,  2., -1.,  1., -3.],\n",
      "        [ 3.,  2.,  4.,  3., -4., -2.,  1., -4., -2., -2., -3., -3.,  0., -2.,  0., -1., -4.,  2.,  2., -2., -2., -3., -4.,  2.,  2.,  2.,  0., -1.,  3., -3., -2.,  2.,  1., -2., -2., -1.,  0.,  3.,  0.,  0.,  3., -1.,  1., -4., -1., -3.,  0.,  0.,  3.,  4.,  0., -4., -4.,  1.,  1.,  0., -4.,  3.,  2.,  1.,  2.,  4., -4., -2.],\n",
      "        [ 4., -2.,  0., -3., -4.,  4., -1.,  3., -4.,  3.,  2.,  0.,  2.,  4.,  2., -2.,  3.,  4., -1.,  1., -2.,  2., -4., -4., -2.,  2.,  2., -4.,  0.,  2., -2., -3., -4.,  0., -3.,  3., -3.,  0.,  0., -2., -4., -3.,  0.,  2., -4., -3., -4.,  3.,  3., -3., -3., -3.,  3.,  4., -3., -4.,  2.,  3., -4.,  4., -1., -3.,  3., -3.],\n",
      "        [-1.,  4.,  4.,  1., -2., -1.,  1., -2., -2., -4., -4., -2.,  4.,  3.,  3.,  2.,  4.,  4.,  3.,  2.,  2.,  3.,  0., -3., -1., -1., -4.,  2., -2.,  1.,  3.,  1.,  1.,  1.,  4.,  1.,  2.,  0.,  1., -4.,  0., -1., -2., -3., -3.,  4.,  2., -4.,  1.,  0.,  0., -2., -4.,  3., -1.,  0., -3., -3., -3., -3.,  3.,  3.,  0., -3.],\n",
      "        [-1.,  0.,  2., -4.,  4., -2., -4., -2.,  0., -3., -1.,  1., -4., -4.,  2., -1., -1., -3.,  2., -1.,  4., -4.,  1.,  3., -4.,  3.,  2.,  3., -1., -2.,  4.,  2.,  0., -1.,  0., -1., -1., -1., -1.,  3., -2.,  1.,  0., -4., -1.,  4., -1.,  0., -4., -4.,  4.,  3.,  4.,  0., -1.,  4., -4., -3.,  0., -1., -3., -3.,  3.,  1.],\n",
      "        [ 0., -3.,  2.,  3.,  0.,  4.,  0.,  2.,  0.,  2.,  0.,  3.,  1.,  3., -1., -1.,  2., -4.,  4., -1., -2., -1., -4., -3., -2.,  0., -2., -3.,  1., -3.,  0.,  3., -2.,  4., -1.,  0.,  3.,  3.,  4., -2.,  0., -1.,  4.,  3.,  4., -4., -2.,  1.,  3., -3.,  4.,  3., -2.,  2.,  4.,  0., -3., -2.,  1.,  0., -3.,  3., -2., -3.],\n",
      "        [ 3.,  0.,  1.,  1., -2.,  0.,  2.,  1.,  1., -1., -1.,  0., -1., -1.,  4.,  4.,  1.,  1.,  2.,  3., -1.,  2., -3., -2.,  2.,  0.,  1.,  1., -1.,  1.,  2., -3., -4., -4.,  2., -3., -2., -1.,  2.,  3.,  2.,  3., -4.,  3., -2.,  4.,  2.,  3.,  1., -2., -1.,  2.,  4.,  0.,  4., -1., -4.,  0.,  4., -3., -3., -1., -2., -3.],\n",
      "        [-1.,  4., -4.,  1., -3.,  2., -1.,  1., -1., -3., -2.,  1., -3.,  2., -3.,  2.,  4., -2., -1.,  1.,  3., -4.,  4.,  1.,  1.,  1.,  2.,  0.,  4.,  2., -1., -4., -1.,  0.,  1.,  0.,  4.,  2.,  4., -2.,  4., -3.,  3.,  3.,  1.,  2.,  0.,  1., -4.,  1., -3.,  1., -1.,  4.,  4.,  2.,  0., -1., -3., -2., -4., -1., -4., -3.]], dtype=torch.float64)\n",
      "B_matrix_new: shape: torch.Size([64, 64]) \n",
      " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], requires_grad=True)\n",
      "dense_grad_weights: A_matrix_new @ B_matrix_new: shape: torch.Size([64, 64]) \n",
      " tensor([[  2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.],\n",
      "        [ 21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.],\n",
      "        [ 18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.,  18.],\n",
      "        [-16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16.],\n",
      "        [-19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19.],\n",
      "        [-11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11.],\n",
      "        [ -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.,  -7.],\n",
      "        [-33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33., -33.],\n",
      "        [ 15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.],\n",
      "        [-46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46., -46.],\n",
      "        [-13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13.],\n",
      "        [-28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28.],\n",
      "        [  7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.],\n",
      "        [-39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39.],\n",
      "        [-14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14.],\n",
      "        [ -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.],\n",
      "        [-10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10., -10.],\n",
      "        [ 16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.],\n",
      "        [-40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40., -40.],\n",
      "        [ 14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.,  14.],\n",
      "        [-28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28., -28.],\n",
      "        [  7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.],\n",
      "        [ 36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.,  36.],\n",
      "        [-13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13., -13.],\n",
      "        [ -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.,  -5.],\n",
      "        [  2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.],\n",
      "        [-16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16., -16.],\n",
      "        [ -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.],\n",
      "        [ 24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.],\n",
      "        [ 21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.,  21.],\n",
      "        [-14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14., -14.],\n",
      "        [ 32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.,  32.],\n",
      "        [ 11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.,  11.],\n",
      "        [  7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.,   7.],\n",
      "        [-35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35., -35.],\n",
      "        [ -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.,  -2.],\n",
      "        [ -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.,  -4.],\n",
      "        [ 24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.],\n",
      "        [ 24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.,  24.],\n",
      "        [-17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17., -17.],\n",
      "        [  8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.],\n",
      "        [ 15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.,  15.],\n",
      "        [ 13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.,  13.],\n",
      "        [  2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.],\n",
      "        [ 31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.,  31.],\n",
      "        [-12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12., -12.],\n",
      "        [ 22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.,  22.],\n",
      "        [ -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.,  -8.],\n",
      "        [-39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39., -39.],\n",
      "        [  2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.,   2.],\n",
      "        [ 29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.,  29.],\n",
      "        [  1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.],\n",
      "        [-55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55., -55.],\n",
      "        [ 25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.,  25.],\n",
      "        [  3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.,   3.],\n",
      "        [ -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.,  -9.],\n",
      "        [-11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11., -11.],\n",
      "        [-19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19., -19.],\n",
      "        [-25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25., -25.],\n",
      "        [  5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.],\n",
      "        [-18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18., -18.],\n",
      "        [ 16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.],\n",
      "        [ 16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.],\n",
      "        [  8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.,   8.]], device='cuda:0', dtype=torch.float16)\n",
      "sparse_tensor.wrapped_tensor.mask:  torch.Size([64, 64])\n",
      "Checking mask tensor for correct sparsity.\n",
      "16/16.0 processed blocks. 16 valid ones, 0 invalid.\n",
      "\n",
      "Checking masked dense result tensor for correct sparsity.\n",
      "16/16.0 processed blocks. 16 valid ones, 0 invalid.\n",
      "\n",
      "masked_dense: shape: torch.Size([64, 64]) \n",
      " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#dense_grad_weights = torch.from_numpy(input.T.detach().numpy() @ grad_d.detach().numpy()\n",
    "#                                      ).to(device=\"cuda:0\").to(dtype=torch.half)\n",
    "\n",
    "print(\"A_matrix_new: shape:\", A_matrix_new.shape,\"\\n\", A_matrix_new)\n",
    "print(\"B_matrix_new: shape:\", B_matrix_new.shape,\"\\n\", B_matrix_new)\n",
    "\n",
    "dense_grad_weights = torch.from_numpy(A_matrix_new.detach().numpy() @ B_matrix_new.detach().numpy()\n",
    "                                      ).to(device=\"cuda:0\").to(dtype=torch.half)\n",
    "#positions_dense_grad_weights =  torch.from_numpy(reference_tensor.T.detach().numpy() @ grad_d.detach().numpy()\n",
    "#                                      ).to(device=\"cuda:0\").to(dtype=torch.half)\n",
    "print(\"dense_grad_weights: A_matrix_new @ B_matrix_new: shape:\", dense_grad_weights.shape,\"\\n\", dense_grad_weights)\n",
    "\n",
    "\n",
    "# Apply mask\n",
    "print(\"sparse_tensor.wrapped_tensor.mask: \", sparse_tensor_new.wrapped_tensor.mask.shape)\n",
    "#print(\"dense_grad_weights: \", dense_grad_weights.shape)\n",
    "#padded_mask =  torch.nn.functional.pad(sparse_tensor_new.wrapped_tensor.mask, (0, 128), \"constant\", 1)\n",
    "#masked_dense_grad_weights = torch.multiply(dense_grad_weights, padded_mask.to(device=\"cuda:0\"))\n",
    "print(\"Checking mask tensor for correct sparsity.\")\n",
    "check_VNM(v, n, m, n+2, sparse_tensor_new.wrapped_tensor.mask)\n",
    "masked_dense_grad_weights = torch.multiply(dense_grad_weights, sparse_tensor_new.wrapped_tensor.mask.to(device=\"cuda:0\"))\n",
    "#positions_masked_dense_grad_weights = torch.multiply(positions_dense_grad_weights, sparse_tensor.wrapped_tensor.mask.to(device=\"cuda:0\"))\n",
    "print(\"Checking masked dense result tensor for correct sparsity.\")\n",
    "check_VNM(v, n, m, n+2, masked_dense_grad_weights)\n",
    "\n",
    "print(\"masked_dense: shape:\", masked_dense_grad_weights.shape, \"\\n\", masked_dense_grad_weights)\n",
    "\n",
    "#print(\"dense_grad_weights:\\n\", positions_dense_grad_weights)\n",
    "#print(\"masked_dense:\\n\", positions_masked_dense_grad_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute sparse version of the operation input.T @ grad_output as sddmm(input.T,  grad_output, sparse). \n",
    "sddmm(left_operand, right_operand, sparsity_guide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_matrix shape: torch.Size([64, 64])\n",
      "B_matrix shape: torch.Size([64, 64])\n",
      "A_matrix: shape: torch.Size([64, 64]) \n",
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  2.,  1.,  0.,  1.,  4., -1., -4., -2., -3.,  4.,  2., -2., -3., -2.,  3.,  4.,  3.,  0., -4., -1.,  3.,  1., -4., -1.,  0., -3.,  3., -4.,  0.,  2.,  0.,  4., -2., -4., -2.,  4., -3., -2., -4., -4.,  4.,  2., -2.,  3., -4.,  4.,  3.,  3., -1.,  1.,  4., -4., -4.,  2., -1.,  0.,  4.,  4.,  1., -3.,  1.,  3., -4.],\n",
      "        [ 1.,  1., -1.,  1.,  2., -1., -1.,  3., -3.,  3.,  3.,  4.,  3.,  0.,  2., -1.,  4., -1., -3.,  4., -3.,  1., -4.,  1., -2.,  4.,  4., -1.,  1.,  3.,  0., -3., -3.,  1.,  0.,  4.,  4.,  3.,  0., -4.,  2., -2.,  3., -2.,  3.,  0.,  0.,  0.,  1., -1., -3.,  2.,  4., -3., -3., -2., -4., -4., -1., -1.,  1.,  3., -1.,  3.],\n",
      "        [-3.,  3.,  0.,  4.,  0.,  4.,  2.,  4.,  4.,  0.,  4.,  0., -4., -1.,  2., -1.,  0.,  3.,  4., -2.,  2.,  3., -2.,  1.,  1.,  1.,  4., -2., -2., -2.,  3., -1.,  4., -2., -2., -1.,  4., -4., -2.,  1.,  0., -3., -3., -2., -1.,  3.,  0.,  1.,  0., -1.,  3., -3.,  4., -4.,  0.,  0.,  2.,  0.,  0.,  2.,  0., -2., -4., -1.],\n",
      "        [-2., -1.,  0.,  4., -3.,  2.,  1.,  1.,  2.,  2.,  2.,  3., -2., -3., -1.,  2., -2., -4.,  2., -3.,  3., -3.,  1.,  0., -1., -3.,  4.,  2., -3., -4., -4.,  0., -3.,  1., -1., -2., -4.,  2.,  1.,  0., -1., -2., -2.,  0., -3.,  0., -3., -1.,  0.,  1., -2., -2.,  1.,  4.,  1.,  2.,  3.,  0., -1.,  2., -2.,  0.,  4., -1.],\n",
      "        [ 3., -3.,  1.,  3.,  3., -2., -3., -4., -2., -1.,  0.,  0.,  1., -1.,  3.,  3., -4., -3., -3., -1.,  3.,  0., -4., -1.,  2., -2., -4., -4.,  0.,  3., -1., -2.,  4.,  3.,  3.,  2., -4.,  2.,  2., -2., -3., -3.,  2.,  0.,  1., -1., -1., -1., -2., -2.,  0., -4.,  2., -2., -3., -3.,  0.,  4., -3.,  4.,  2., -1.,  4.,  1.],\n",
      "        [-3.,  1., -1., -3., -2.,  3., -3., -3., -4.,  2.,  2., -1., -4.,  3.,  0., -2., -4.,  4., -4.,  2., -3.,  4.,  3.,  0.,  3.,  3., -3.,  1., -3.,  4., -3., -2., -2., -2., -3., -2., -3., -2.,  0.,  4.,  2.,  4., -2.,  2.,  0.,  2.,  2.,  0., -1.,  0.,  4.,  0., -1.,  0.,  2., -2., -2.,  4.,  1., -3.,  3.,  2., -1., -4.],\n",
      "        [-4., -2., -2.,  4.,  1.,  0.,  3., -4., -3., -4.,  3.,  1.,  0., -2., -4.,  3.,  1.,  2., -4.,  4., -2.,  3.,  4.,  0.,  0.,  0.,  2., -2., -4.,  3., -4.,  3., -3., -2.,  2.,  0.,  1.,  0.,  0.,  0., -2., -4., -3.,  2., -1.,  4., -2.,  2.,  2., -2.,  1.,  4.,  3., -1., -3., -2.,  1.,  3., -2.,  1.,  0., -2., -1.,  1.],\n",
      "        [-1., -4.,  3.,  2., -2., -4., -3.,  0., -3., -2., -3., -2., -4., -3.,  4.,  2., -2., -2., -1., -4., -1.,  0., -3., -4., -3., -3., -1.,  1.,  0.,  3.,  4.,  1.,  1.,  2., -2.,  2.,  2., -3., -3.,  1.,  0., -3.,  3., -2., -2.,  0.,  4.,  0.,  1., -2., -1.,  4.,  2., -2.,  4.,  3., -1., -1., -3.,  3.,  4., -4., -3., -2.],\n",
      "        [ 0., -3.,  4., -4., -1.,  4.,  1.,  1., -4., -2.,  3., -4.,  0.,  2.,  1., -4.,  2.,  2., -4., -3.,  4.,  2., -4., -2.,  3.,  4.,  4.,  2., -4., -4.,  3.,  0., -4.,  2.,  0., -1., -1.,  4.,  4., -1.,  4.,  2.,  0., -1., -3.,  3.,  0.,  3., -2., -2.,  4., -3.,  4.,  2., -3.,  3.,  2., -4., -2.,  0.,  4., -4.,  4.,  2.],\n",
      "        [ 1., -2., -4.,  4.,  4.,  2., -3.,  3., -1., -1.,  4., -2.,  4., -1., -4., -3.,  1.,  2., -2., -1., -2., -3.,  4.,  4., -4., -3., -1.,  2., -4., -4., -1.,  1., -4., -1., -2.,  2., -4., -3.,  0., -3.,  0.,  0., -1., -1.,  1., -3., -2., -4.,  1., -2., -3.,  3.,  0., -4.,  0.,  0., -3.,  1., -3., -3.,  2.,  0.,  0.,  0.],\n",
      "        [ 1.,  1., -2., -1.,  2.,  0.,  0., -4.,  2.,  1., -3., -1.,  2.,  0., -3., -2.,  0., -2.,  4.,  4., -2., -1., -3., -3., -4., -4., -3.,  0., -2.,  1.,  2.,  3.,  4.,  4., -3.,  0., -2., -1., -2.,  4.,  4., -1., -4., -1.,  1., -3.,  2., -3., -3., -1.,  3., -1.,  2., -4., -1.,  0.,  3.,  0.,  2., -3.,  3.,  0.,  2.,  3.],\n",
      "        [-3.,  2., -2., -1., -4.,  0., -3.,  3.,  1., -3., -3., -2., -3.,  1., -3., -4.,  4.,  2., -2.,  0.,  2.,  1.,  4., -1., -4.,  2.,  0.,  3.,  4.,  4.,  1., -4.,  1.,  1., -2.,  0.,  3., -1., -2.,  0., -2., -3.,  0., -3.,  3., -1., -2.,  0., -4.,  0.,  4.,  1., -4.,  4.,  4., -4., -1., -3.,  1., -2., -4., -2., -4.,  2.],\n",
      "        [ 1., -2.,  4.,  0., -1.,  4.,  2.,  2., -4., -3.,  1.,  1., -2.,  3., -3., -2., -4., -4.,  3., -4.,  2.,  0.,  3.,  3.,  1.,  0.,  3.,  2.,  0.,  4., -3.,  3., -3.,  3., -2.,  1.,  2.,  2., -4.,  0.,  1.,  1., -3.,  3.,  0., -4.,  4., -2.,  3.,  2., -1., -1., -4.,  4., -3.,  1., -3., -4., -4., -1.,  4.,  4., -2.,  3.],\n",
      "        [ 0., -2., -3., -3., -4., -3.,  0., -1.,  1.,  1., -2.,  3., -1., -3., -4., -3.,  1., -1., -2.,  1., -3.,  0., -4., -4., -2.,  3.,  0.,  4.,  1.,  0.,  2., -4.,  2.,  2., -2.,  0.,  3.,  0.,  4., -1., -4.,  4., -3., -3., -4., -2., -4., -1., -1., -1.,  3., -2.,  3.,  3., -1.,  3., -1.,  1.,  2.,  4., -4., -3., -4.,  0.],\n",
      "        [ 3., -4.,  3.,  0.,  0.,  0.,  1., -2., -1.,  1., -3., -3.,  3., -3., -2.,  4.,  0., -4., -4., -4.,  1., -1.,  3., -4.,  4., -2.,  4.,  2.,  0.,  2., -3., -4.,  0.,  4.,  0.,  2., -3., -3.,  2.,  0.,  0.,  3., -1., -4., -4.,  2.,  4.,  0.,  0.,  2.,  1., -3., -4., -3., -3.,  2.,  0.,  3., -3.,  2., -1., -1.,  2.,  3.],\n",
      "        [-4., -4.,  1., -2.,  4., -4.,  0., -1.,  4.,  2., -1.,  4., -3., -4., -3.,  1.,  3.,  2.,  2.,  1.,  2.,  1., -3., -3.,  0., -2.,  3.,  3., -1., -2., -1., -2., -4.,  2., -2.,  4.,  3.,  0., -2.,  2., -3., -1., -1., -4.,  0.,  0.,  2., -4., -2., -3.,  1.,  4.,  3.,  0.,  3., -4.,  3., -1.,  3.,  4., -1.,  4., -3.,  0.],\n",
      "        [ 2., -1., -1., -2.,  4.,  0.,  4.,  3.,  1., -2.,  4., -4.,  2., -1.,  3.,  2.,  0., -2., -1., -2.,  2.,  4., -4., -2., -2.,  3., -2.,  0.,  1., -1.,  4., -4.,  1., -2., -2., -2.,  0.,  4., -4., -3., -3.,  4.,  4.,  1., -4.,  3., -3.,  1., -4., -2.,  1.,  2., -4.,  2., -4., -4., -2., -4.,  0.,  3., -1., -1.,  3.,  2.],\n",
      "        [ 0.,  3.,  4.,  4.,  2., -4., -4.,  1.,  2.,  3., -2.,  3.,  1., -3., -3.,  1., -3.,  2., -4., -3., -1.,  3., -2.,  0.,  3.,  2., -4., -4., -3.,  4.,  2., -1.,  2.,  2., -2., -2.,  0., -3.,  3.,  4.,  4.,  3.,  0.,  0.,  4.,  1.,  3., -3., -3.,  1., -4., -2.,  0., -2.,  2., -2.,  3.,  3., -4.,  4.,  0.,  3., -2.,  4.],\n",
      "        [-2., -2., -3.,  0., -1., -4., -3., -4.,  3.,  2., -2.,  1., -4., -4.,  0., -3.,  0.,  0.,  0.,  4.,  3., -3.,  4., -1.,  2., -4., -4., -3.,  1., -4., -3.,  1., -2., -4.,  3.,  0., -4.,  4.,  0.,  2.,  1.,  1., -2.,  2., -1.,  2.,  1.,  3., -3., -4.,  2.,  2., -4.,  2., -4.,  2.,  0., -4.,  2.,  3., -4.,  1., -1., -3.],\n",
      "        [ 1.,  4.,  1.,  2.,  3.,  3., -1., -2., -1.,  1.,  2.,  3., -4., -2.,  1., -3., -1., -3.,  2., -3.,  1.,  2.,  3., -4.,  1., -2.,  3., -1.,  3.,  4.,  0.,  0.,  3., -4.,  4., -1.,  2., -4.,  1., -4.,  1., -2.,  1., -1., -1., -2.,  0.,  2., -1.,  4.,  1.,  4.,  0., -3.,  2., -4.,  0., -2., -1., -2.,  4.,  4.,  0.,  0.],\n",
      "        [ 2., -1.,  3., -1., -1.,  0., -2.,  2.,  0.,  3., -4.,  3., -4., -1., -2., -3., -2.,  3.,  3.,  0.,  3., -2.,  0., -2., -2.,  2.,  0., -3., -1.,  3.,  1., -4., -3., -2., -2.,  1.,  0.,  0., -2.,  4., -4., -3.,  1.,  4.,  0.,  1., -4.,  0., -2.,  2., -4., -4.,  4., -2., -2., -4.,  4.,  0., -3., -4., -2.,  1.,  4.,  0.],\n",
      "        [ 3., -3.,  3., -2., -1.,  3.,  4., -1.,  2., -2., -2.,  4.,  4., -3.,  0., -3.,  3., -2., -3.,  0., -2., -3., -2., -4.,  1.,  0., -4.,  2.,  3.,  4.,  4., -1.,  3.,  2.,  1.,  4., -2., -2., -1., -4.,  1., -1.,  4.,  0.,  0., -3., -1., -4., -3.,  0., -1.,  0.,  1.,  1.,  0.,  4.,  3.,  0., -1.,  1., -2., -2.,  4.,  3.],\n",
      "        [ 2., -2.,  3.,  3.,  1., -3.,  3.,  4.,  0.,  0., -3.,  3.,  2.,  0.,  1.,  3.,  4.,  4., -2., -2., -1.,  0.,  4., -3.,  3., -1.,  1., -4., -1., -2.,  3.,  4.,  1.,  1.,  2.,  4.,  0.,  0.,  1.,  3.,  4.,  2., -1., -1., -3.,  4., -2., -4.,  2., -2., -4., -4.,  3.,  2.,  4., -1.,  3.,  0., -1., -2.,  3.,  4., -3., -3.],\n",
      "        [-1.,  3., -4.,  1., -4.,  1., -1.,  4., -2.,  0., -3., -3.,  0., -1., -3.,  4., -1.,  4.,  4.,  4.,  1.,  1., -2.,  0., -3., -1.,  0.,  2., -3., -1.,  3.,  0., -3.,  1., -4., -3.,  4., -2.,  3., -3.,  2.,  4., -2.,  4., -4., -3.,  3., -2.,  3.,  0.,  4., -3., -3., -2., -1., -4., -1.,  3.,  2.,  0., -3., -2.,  4., -4.],\n",
      "        [ 3.,  3.,  4.,  2.,  3.,  1.,  1., -2., -2.,  1., -2., -3.,  0.,  3., -1., -4., -3., -2., -3., -4., -2.,  1.,  3.,  2., -3.,  0., -3., -3.,  0.,  4., -3., -2.,  2.,  4.,  2.,  0.,  0., -4.,  0.,  2.,  1.,  2.,  1.,  3., -1.,  4.,  3., -2., -4.,  2., -4., -4., -2.,  3.,  2., -2., -2.,  2., -1.,  2.,  1., -2.,  2., -4.],\n",
      "        [-3., -3., -2., -1., -3.,  2.,  4.,  0., -2.,  0., -2.,  1.,  0.,  3.,  1., -2.,  0.,  1., -2.,  1., -1.,  3.,  3., -1.,  0., -3.,  3., -4., -1., -2., -4., -1., -1., -2.,  2.,  0.,  4.,  1.,  2.,  4., -3.,  2., -2., -1.,  0.,  3.,  1.,  2., -3.,  1., -2.,  0., -1.,  3.,  1.,  3.,  4., -1.,  1., -3.,  1., -1.,  0.,  2.],\n",
      "        [-1.,  4.,  4., -1.,  1.,  1., -4.,  0., -4., -1.,  3.,  0., -3., -3., -4., -1.,  2., -4.,  1.,  4.,  4., -2.,  2.,  3., -1., -3.,  2., -4.,  0., -3.,  0.,  4., -3.,  0., -4.,  0.,  3., -4.,  4.,  4., -1.,  0., -3., -1.,  1., -3.,  2., -4., -2., -4.,  4., -3., -2.,  0.,  2.,  2.,  1.,  0.,  0., -4., -1.,  4.,  2., -2.],\n",
      "        [-3.,  2.,  0., -4.,  1., -1.,  4.,  4.,  3., -4., -2., -3., -2.,  2., -2., -2.,  3., -3., -3.,  3.,  1.,  3.,  0.,  4., -4.,  2.,  3.,  0.,  0.,  0.,  0.,  1., -4., -4.,  1.,  1., -3.,  0.,  0.,  3., -1.,  1.,  4., -1., -4., -3.,  1.,  3., -3.,  2.,  0.,  0., -2.,  2.,  0., -1.,  0., -2.,  1.,  4.,  0.,  3., -2.,  0.],\n",
      "        [ 3.,  2., -4.,  3.,  4., -1.,  1., -2.,  2.,  0.,  1.,  2.,  0.,  4., -1.,  1.,  1.,  3.,  3.,  2.,  0., -2.,  3.,  2., -2., -3., -1.,  1.,  4.,  2., -4.,  1., -1.,  4., -1., -4., -4.,  4., -1.,  0., -1.,  4.,  4.,  1.,  2., -2., -3., -4.,  3., -4.,  2., -3., -4.,  0.,  3., -3.,  1., -4.,  2., -1.,  2.,  0.,  4.,  3.],\n",
      "        [-3., -4.,  3.,  4.,  4.,  4., -1., -4.,  2., -3.,  2., -1., -3.,  1., -1., -4., -1.,  4.,  4., -2., -1.,  2., -1.,  3.,  2.,  3.,  4., -3., -4.,  3.,  2., -4.,  1., -1.,  2.,  0., -4.,  4., -2.,  2., -4.,  2.,  3., -3., -1., -2.,  4.,  0., -3.,  3.,  2., -2., -1.,  3.,  3.,  3.,  1.,  2.,  3.,  0., -2.,  0., -2.,  3.],\n",
      "        [ 1.,  4., -4.,  0.,  3., -1., -3.,  3., -4., -4., -4., -3., -4.,  2.,  2.,  4.,  1., -1.,  4.,  4., -2.,  3.,  2.,  1., -2.,  0.,  3.,  1.,  3., -4., -3., -3., -4., -2.,  1.,  1., -1.,  1.,  0., -3., -1., -2., -1., -1.,  4.,  3., -4., -2., -3.,  4., -3., -3.,  0.,  4.,  1.,  3., -3., -1., -1.,  4., -2., -2.,  4., -4.],\n",
      "        [ 0.,  0.,  1.,  3., -4.,  3., -4., -1., -4., -3.,  0.,  3.,  4.,  0., -4.,  3., -2., -2., -4.,  2.,  4., -1.,  0.,  3.,  0.,  1.,  1.,  2.,  4.,  0.,  2., -3.,  2., -3.,  3.,  3.,  4.,  1.,  4.,  3.,  2., -2.,  1., -1.,  2.,  1., -3.,  2.,  1.,  4.,  0., -2., -2.,  2.,  4.,  3., -3.,  0., -2., -4.,  2.,  3.,  4., -1.],\n",
      "        [ 4.,  2.,  4.,  3., -2.,  2.,  1.,  2.,  4.,  4., -4., -1., -1., -4., -3.,  4.,  4.,  1., -4., -4., -4.,  4.,  1., -3., -3.,  1.,  4.,  3., -1.,  4., -3., -2., -1.,  4.,  4.,  2.,  1., -2., -3.,  0., -4., -4.,  3., -1., -4.,  4., -4.,  2., -2.,  3.,  0., -1.,  2., -4., -2.,  3.,  0., -2., -1.,  3., -3.,  1.,  4.,  0.],\n",
      "        [ 0.,  1.,  4., -4.,  1., -3., -2.,  1., -3., -4.,  4.,  4., -4.,  3.,  1., -4.,  1.,  4., -1., -3.,  2.,  2.,  2., -3.,  2., -3., -4.,  3.,  0.,  2., -4.,  2., -2.,  3.,  0.,  3.,  2.,  2.,  0.,  1.,  2., -3.,  0.,  0.,  4.,  3.,  1.,  1.,  2.,  2.,  2., -2., -3., -1., -1.,  3., -2., -1.,  1.,  2., -4.,  0., -1., -4.],\n",
      "        [-4.,  3.,  4.,  2.,  0., -3., -2., -4., -2.,  0.,  0., -4., -4.,  3.,  1.,  1.,  1., -4.,  3.,  2., -3., -3.,  1.,  0., -1.,  4., -3., -3., -3., -1., -2., -3., -3.,  4.,  3.,  0., -4.,  3.,  0., -2., -4.,  2., -2.,  4.,  1.,  1.,  0., -3., -1., -4.,  1.,  1.,  4., -3.,  3., -2.,  4.,  2., -2., -4., -4.,  0., -2., -4.],\n",
      "        [-3.,  3., -2., -3.,  0.,  2., -3., -2., -3., -2., -3.,  2.,  3., -1.,  1., -4., -3., -4., -1., -3., -3., -3., -3.,  3.,  1., -1.,  4.,  0.,  1.,  4., -2.,  2.,  0.,  0.,  1.,  4.,  4.,  3., -4., -4., -4.,  4.,  1.,  1., -1.,  4., -1., -2.,  4.,  2., -1.,  4.,  1.,  1.,  2., -4.,  2., -1.,  0., -1., -2.,  2.,  3.,  3.],\n",
      "        [-3.,  0.,  0., -4., -4.,  0.,  3., -4.,  0., -1., -1.,  1., -3.,  3.,  3., -4., -2., -2.,  2.,  2.,  1.,  0.,  2., -2.,  1., -3., -1.,  4.,  1., -2.,  2.,  2.,  0., -3.,  0.,  0.,  2., -2., -2.,  4.,  0., -1.,  0.,  1.,  4., -2., -1., -2., -2.,  4.,  2.,  3., -3., -1.,  4.,  2.,  0., -2.,  1.,  3., -2., -4.,  0.,  2.],\n",
      "        [-2.,  2.,  4.,  2.,  2., -3., -1., -3., -1.,  3.,  0., -2.,  3.,  2., -2., -3.,  1.,  4.,  2.,  0., -4.,  1., -3., -2.,  2.,  3., -4.,  4.,  3., -2.,  0.,  3.,  4., -1., -2.,  2., -1.,  1.,  1.,  0., -3.,  4.,  2., -4., -1.,  3.,  4.,  2.,  0.,  1., -3.,  1., -4., -2.,  3.,  2.,  3.,  4., -1.,  0., -3.,  3., -4.,  4.],\n",
      "        [-4., -1.,  1., -3.,  2.,  4.,  1.,  4.,  3., -2.,  4.,  4.,  3.,  3.,  1., -1., -3.,  2.,  2.,  4.,  3.,  0., -1.,  1., -3.,  3.,  3., -1., -1.,  4., -4., -3., -4., -1., -2.,  2., -2.,  3.,  1.,  4.,  4.,  0.,  3.,  2., -4.,  0.,  3., -4., -3.,  0.,  4.,  4., -4., -3., -1.,  4.,  4., -3., -4., -4.,  3., -4., -2.,  3.],\n",
      "        [-1., -2.,  1.,  2., -3., -3., -1., -4.,  0., -3.,  0., -2.,  1., -1., -4.,  1.,  3., -4., -2.,  1., -3.,  1.,  1.,  3.,  1.,  1.,  1., -1.,  2.,  1., -2., -4.,  4.,  1.,  0.,  3.,  2.,  3.,  3.,  4., -4., -2., -4., -1., -1., -4.,  2., -2.,  3., -4.,  1.,  4., -1., -3.,  0.,  2.,  1.,  4.,  4., -2.,  0., -3., -4., -3.],\n",
      "        [ 0., -1.,  4.,  4., -3.,  2.,  4.,  4.,  3.,  2., -3., -3.,  2.,  3., -3.,  1.,  1.,  3., -2.,  3.,  2.,  1., -2., -2.,  0., -3., -2.,  2., -4., -3.,  4.,  0.,  0.,  3.,  0.,  1., -1., -3., -1.,  4.,  3.,  0.,  3.,  0., -4., -4.,  3.,  0.,  1., -3., -2., -2.,  0.,  0., -1.,  4.,  0.,  1., -1.,  0.,  2., -2., -4., -3.],\n",
      "        [ 4.,  0.,  1., -2.,  1.,  4.,  4., -3., -3.,  2.,  1., -3.,  0., -2.,  1., -4., -3.,  1., -3.,  1.,  4.,  0., -2., -1.,  3.,  1.,  1.,  1.,  4.,  1.,  0.,  1.,  3.,  3.,  1., -3.,  0.,  0., -2.,  1., -3.,  2.,  1.,  2.,  4.,  4.,  0., -4., -1.,  1.,  1., -2., -4.,  3., -1.,  2., -3.,  4., -2.,  3., -1.,  0., -1., -3.],\n",
      "        [-3.,  4., -1.,  1., -3., -1., -4., -3., -4., -1.,  1.,  4., -4., -4.,  2., -1.,  0., -2., -1., -2.,  3.,  2.,  4.,  3.,  0.,  4.,  1.,  3.,  0., -4.,  2.,  1.,  2.,  3.,  4.,  2., -2.,  4.,  3., -4., -4., -1.,  3., -3., -3.,  0.,  0.,  1.,  1.,  2.,  4., -3., -3.,  3.,  4.,  1.,  3.,  1., -3., -4., -1.,  4., -1.,  3.],\n",
      "        [ 2., -4.,  0.,  4.,  3., -4., -4.,  3., -3.,  1.,  4., -1., -4., -2.,  2.,  1.,  4., -1.,  1.,  3., -2.,  0.,  0.,  3.,  3., -1.,  1., -3., -2.,  2.,  3., -4.,  2., -2.,  2.,  0., -3.,  0., -1., -2., -4., -4., -1.,  4., -2., -3., -2.,  4.,  0.,  3.,  1.,  0., -1.,  3., -4.,  4., -3.,  1.,  4.,  0., -2.,  2., -2.,  3.],\n",
      "        [ 2.,  2., -2.,  3., -2., -1., -3., -4.,  0.,  0.,  4.,  0., -1., -2., -2.,  4., -4.,  3., -3., -3.,  1., -2.,  4.,  2.,  3.,  3.,  4.,  4.,  0., -3.,  2., -3.,  2.,  2.,  3.,  4., -3.,  2.,  4.,  1.,  4.,  2., -4., -1.,  4., -2., -4.,  2.,  3., -3.,  2., -1., -3.,  1.,  4.,  3., -3.,  3.,  2.,  1.,  3., -3.,  0.,  0.],\n",
      "        [ 3., -4., -3.,  4.,  2.,  3., -2.,  2., -2., -1., -4., -3., -1.,  3.,  2., -4.,  2., -3., -1.,  4.,  0., -4.,  2.,  1.,  2., -3.,  4.,  0., -2., -1.,  2.,  3.,  0.,  1.,  3.,  4., -3., -4.,  1.,  0., -3., -1., -1., -1., -3., -4., -1.,  2.,  4.,  0.,  1., -1.,  1., -2.,  2., -2., -1.,  1.,  0., -1., -4.,  4., -3., -2.],\n",
      "        [-2.,  2., -3., -1.,  1.,  1.,  3.,  0.,  2., -2.,  3.,  2., -4.,  4., -1., -4., -2.,  4., -3.,  3.,  4.,  4.,  3., -4., -2., -4., -1., -2.,  2., -2., -2.,  4., -4., -2., -2.,  1.,  1.,  4.,  3., -2., -3.,  0.,  4.,  4., -1., -4.,  2.,  1.,  2.,  4., -3.,  1.,  1.,  2.,  1.,  3.,  2.,  2.,  4., -1., -4.,  4., -4.,  3.],\n",
      "        [ 4., -1., -3.,  0., -2.,  4.,  1., -3., -1., -2., -4.,  1., -1.,  3.,  4., -4.,  0.,  4.,  2.,  3.,  1., -4., -1., -3.,  1., -2., -2.,  1., -4., -1., -3.,  4.,  0., -2.,  0.,  0.,  4., -4.,  1., -1., -2., -1.,  2., -1.,  4.,  1.,  0., -3., -2.,  4.,  2.,  4.,  2., -3.,  3., -2., -3., -4.,  3., -3., -1.,  4., -3.,  1.],\n",
      "        [ 4., -1., -4.,  4., -3., -1., -4., -3.,  1.,  2.,  1.,  0., -4.,  3.,  0., -1.,  2., -3., -4., -1., -4.,  3.,  0., -2.,  3., -2., -3., -4., -4., -4.,  1.,  2., -1.,  2.,  1., -2., -4., -1.,  0.,  1.,  3.,  3.,  4.,  4., -1., -3., -4.,  3., -4., -4., -3., -3.,  3., -1., -3., -1.,  2., -2., -1.,  0., -1., -4.,  4.,  0.],\n",
      "        [ 2., -1.,  3.,  0.,  2., -4.,  4.,  1.,  1.,  1., -2.,  4.,  0., -2.,  2.,  0., -2.,  1., -1.,  3., -4.,  0., -3.,  0.,  3.,  1.,  2., -3., -1.,  1.,  1.,  3.,  4., -3., -2.,  0., -3.,  4., -4.,  0., -2., -2., -1.,  2., -2., -4., -2.,  0.,  2.,  1., -4.,  0., -3., -3.,  4.,  2.,  2.,  1.,  1., -4., -3.,  4.,  1.,  4.],\n",
      "        [-3.,  4., -1., -4.,  1., -1.,  1.,  4., -1.,  4.,  1.,  1.,  3.,  1.,  3., -2., -2.,  3.,  2., -3.,  2.,  1.,  1., -2.,  4., -3., -4.,  3., -4.,  2.,  2.,  4.,  1., -4., -4.,  4., -3., -1.,  3., -2., -2.,  0.,  1.,  3.,  3., -1., -4., -4.,  4.,  2.,  4.,  2., -4.,  0.,  4.,  4.,  1.,  2.,  3.,  0., -1.,  0., -1.,  2.],\n",
      "        [-1., -1.,  2., -2.,  2.,  0.,  1.,  3.,  2., -3.,  1.,  1., -1.,  0., -1., -2., -3., -3.,  2., -4.,  1., -1.,  4., -4., -3.,  0.,  0.,  0.,  3.,  3., -2., -3., -4.,  4., -1.,  0., -3.,  2., -4.,  2.,  4.,  0., -4., -2.,  4., -1., -2.,  3.,  0.,  3.,  3.,  4., -3.,  1.,  4., -4., -4., -3.,  1.,  3.,  2.,  1.,  2.,  2.],\n",
      "        [-2.,  4., -4., -1.,  1., -4.,  3., -2., -2.,  3., -2., -3., -2.,  1., -4.,  0., -3.,  1., -1., -2., -3., -4., -3., -4., -3.,  0.,  1.,  4., -4., -3., -1., -2., -4., -1., -1., -1., -2., -4.,  2.,  0.,  3.,  4.,  2., -4., -2.,  0., -1., -1., -4.,  2.,  0., -4.,  1.,  1., -4.,  2., -4.,  1.,  1.,  3.,  2., -4.,  3.,  0.],\n",
      "        [-3.,  4.,  4., -1.,  4., -1.,  2.,  3., -4., -1.,  1., -3.,  1.,  4.,  0., -1.,  0.,  2., -1., -4., -2., -1.,  0., -4.,  3.,  4.,  3., -2.,  2.,  1.,  3.,  3.,  1.,  0., -4.,  0.,  1.,  2., -2.,  1.,  3.,  2.,  2., -1.,  1., -4.,  0.,  3.,  2.,  4.,  1.,  3., -4., -3.,  3.,  1., -4.,  2.,  4.,  2., -4.,  0., -2., -1.],\n",
      "        [ 4., -4.,  4., -2.,  2.,  4.,  3., -2.,  1., -1., -1., -4., -1.,  4.,  1.,  3.,  3., -2.,  1.,  1., -3., -2., -2., -3.,  2., -3.,  1.,  2., -1.,  1., -3., -2.,  2.,  3., -2., -3.,  0., -4.,  0., -3., -3.,  1.,  4., -2.,  0.,  0.,  4., -4.,  0.,  0.,  2.,  1.,  0.,  2.,  0., -3.,  3.,  3.,  2., -3.,  0.,  1., -3.,  4.],\n",
      "        [-1., -1.,  3.,  3., -3., -4.,  2., -3.,  2., -1., -1.,  4.,  4., -4., -4., -1.,  1.,  0.,  3.,  3., -3., -2., -4.,  2., -3.,  0.,  1., -1., -4.,  0., -4., -3.,  1.,  3.,  2.,  0.,  4.,  0.,  1.,  1.,  1.,  3., -2.,  0.,  3., -2.,  2., -3.,  0.,  0., -4., -1.,  0.,  3., -4.,  1., -4., -1.,  2., -4.,  3.,  2.,  4., -1.],\n",
      "        [ 1.,  1.,  4., -2.,  3.,  0.,  1.,  0., -4.,  3.,  2.,  4.,  4., -3.,  3., -1.,  1.,  3.,  1.,  4., -1.,  1., -2., -1.,  3., -4., -3.,  1.,  0., -4., -4.,  3.,  0.,  2., -4., -4.,  4., -1.,  0., -4., -2., -4.,  2., -3., -3.,  0., -2.,  1., -3.,  3.,  0., -2., -2.,  0.,  2., -4.,  0.,  3.,  1., -4.,  2., -1.,  1., -3.],\n",
      "        [ 3.,  2.,  4.,  3., -4., -2.,  1., -4., -2., -2., -3., -3.,  0., -2.,  0., -1., -4.,  2.,  2., -2., -2., -3., -4.,  2.,  2.,  2.,  0., -1.,  3., -3., -2.,  2.,  1., -2., -2., -1.,  0.,  3.,  0.,  0.,  3., -1.,  1., -4., -1., -3.,  0.,  0.,  3.,  4.,  0., -4., -4.,  1.,  1.,  0., -4.,  3.,  2.,  1.,  2.,  4., -4., -2.],\n",
      "        [ 4., -2.,  0., -3., -4.,  4., -1.,  3., -4.,  3.,  2.,  0.,  2.,  4.,  2., -2.,  3.,  4., -1.,  1., -2.,  2., -4., -4., -2.,  2.,  2., -4.,  0.,  2., -2., -3., -4.,  0., -3.,  3., -3.,  0.,  0., -2., -4., -3.,  0.,  2., -4., -3., -4.,  3.,  3., -3., -3., -3.,  3.,  4., -3., -4.,  2.,  3., -4.,  4., -1., -3.,  3., -3.],\n",
      "        [-1.,  4.,  4.,  1., -2., -1.,  1., -2., -2., -4., -4., -2.,  4.,  3.,  3.,  2.,  4.,  4.,  3.,  2.,  2.,  3.,  0., -3., -1., -1., -4.,  2., -2.,  1.,  3.,  1.,  1.,  1.,  4.,  1.,  2.,  0.,  1., -4.,  0., -1., -2., -3., -3.,  4.,  2., -4.,  1.,  0.,  0., -2., -4.,  3., -1.,  0., -3., -3., -3., -3.,  3.,  3.,  0., -3.],\n",
      "        [-1.,  0.,  2., -4.,  4., -2., -4., -2.,  0., -3., -1.,  1., -4., -4.,  2., -1., -1., -3.,  2., -1.,  4., -4.,  1.,  3., -4.,  3.,  2.,  3., -1., -2.,  4.,  2.,  0., -1.,  0., -1., -1., -1., -1.,  3., -2.,  1.,  0., -4., -1.,  4., -1.,  0., -4., -4.,  4.,  3.,  4.,  0., -1.,  4., -4., -3.,  0., -1., -3., -3.,  3.,  1.],\n",
      "        [ 0., -3.,  2.,  3.,  0.,  4.,  0.,  2.,  0.,  2.,  0.,  3.,  1.,  3., -1., -1.,  2., -4.,  4., -1., -2., -1., -4., -3., -2.,  0., -2., -3.,  1., -3.,  0.,  3., -2.,  4., -1.,  0.,  3.,  3.,  4., -2.,  0., -1.,  4.,  3.,  4., -4., -2.,  1.,  3., -3.,  4.,  3., -2.,  2.,  4.,  0., -3., -2.,  1.,  0., -3.,  3., -2., -3.],\n",
      "        [ 3.,  0.,  1.,  1., -2.,  0.,  2.,  1.,  1., -1., -1.,  0., -1., -1.,  4.,  4.,  1.,  1.,  2.,  3., -1.,  2., -3., -2.,  2.,  0.,  1.,  1., -1.,  1.,  2., -3., -4., -4.,  2., -3., -2., -1.,  2.,  3.,  2.,  3., -4.,  3., -2.,  4.,  2.,  3.,  1., -2., -1.,  2.,  4.,  0.,  4., -1., -4.,  0.,  4., -3., -3., -1., -2., -3.],\n",
      "        [-1.,  4., -4.,  1., -3.,  2., -1.,  1., -1., -3., -2.,  1., -3.,  2., -3.,  2.,  4., -2., -1.,  1.,  3., -4.,  4.,  1.,  1.,  1.,  2.,  0.,  4.,  2., -1., -4., -1.,  0.,  1.,  0.,  4.,  2.,  4., -2.,  4., -3.,  3.,  3.,  1.,  2.,  0.,  1., -4.,  1., -3.,  1., -1.,  4.,  4.,  2.,  0., -1., -3., -2., -4., -1., -4., -3.]], device='cuda:0', dtype=torch.float16)\n",
      "B_matrix: shape: torch.Size([64, 64]) \n",
      " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "SDDMM output tensor: shape: torch.Size([64, 1024]) \n",
      " tensor([[  2.0000,   2.0000,  21.0000,  ..., -14.0000,  -4.0000,  -4.0000],\n",
      "        [  0.0000,   1.8750,   0.0000,  ...,   1.8750,   0.0000,   1.8750],\n",
      "        [  0.0000,   1.8750,   0.0000,  ...,   1.8750,   0.0000,   1.8750],\n",
      "        ...,\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]], device='cuda:0', dtype=torch.float16)\n",
      "16/16.0 processed blocks. 16 valid ones, 0 invalid.\n",
      "\n",
      "Densified output tensor: shape: torch.Size([64, 64]) \n",
      " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Input data declaration\n",
    "#transposed_flattened_input = torch.flatten(input, start_dim=0, end_dim=-2).T\n",
    "#flattened_grad_output = torch.flatten(grad_d, start_dim=0, end_dim=-2)\n",
    "#print(\"Input tensor:\\n\", input)\n",
    "#print(\"Grad_output tensor:\\n\", grad_d)\n",
    "#print(\"Sparse tensor metadata:\\n\", sparse_tensor.wrapped_tensor.metadata)\n",
    "#print(\"Sparse tensor values:\\n\", sparse_tensor.wrapped_tensor.values)\n",
    "#print(\"Sparse tensor columns:\\n\", sparse_tensor.wrapped_tensor.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#A_matrix = input.to(dtype=torch.half).cuda()    # Correct would be input.T\n",
    "#B_matrix = grad_d.to(dtype=torch.half).cuda() # Correct would be grad_d\n",
    "\n",
    "A_matrix = A_matrix_new.to(dtype=torch.half).cuda()\n",
    "B_matrix = B_matrix_new.to(dtype=torch.half).cuda()\n",
    "\n",
    "#A_matrix = torch.ones(256, 256).to(dtype=torch.half).cuda() \n",
    "#B_matrix = torch.ones(256, 256).to(dtype=torch.half).cuda() \n",
    "\n",
    "print(\"A_matrix shape:\", A_matrix.shape)\n",
    "print(\"B_matrix shape:\", B_matrix.shape)\n",
    "print(\"A_matrix: shape:\", A_matrix.shape, \"\\n\", A_matrix)\n",
    "print(\"B_matrix: shape:\", B_matrix.shape, \"\\n\", B_matrix)\n",
    "#sddmm_C_matrix = sparse_tensor\n",
    "sddmm_C_matrix = sparse_tensor_new\n",
    "\n",
    "sddmm_output = spatha_sddmm.sddmm(\n",
    "                          A_matrix,   # lhs operand\n",
    "                          B_matrix,   # rhs operand\n",
    "                          sddmm_C_matrix.wrapped_tensor.metadata.cuda(),    # metada for output sparsity distribution\n",
    "                          #sparse_tensor.wrapped_tensor.values.to(dtype=torch.half).cuda(),    # Values for output sparsity distribution\n",
    "                          sddmm_C_matrix.wrapped_tensor.columns.cuda(),           # indices for output sparsity distribution\n",
    "                          sddmm_C_matrix.wrapped_tensor.nrows, # Since LHS is transposed, LHS.shape[0] is LHS.shape[1]\n",
    "                          sddmm_C_matrix.wrapped_tensor.ncols,         \n",
    "                          B_matrix.shape[1],         \n",
    "#                          sddmm_C_matrix.wrapped_tensor.v,          \n",
    "                          sddmm_C_matrix.wrapped_tensor.n,                # N\n",
    "                          sddmm_C_matrix.wrapped_tensor.m,                # M\n",
    "                          sddmm_C_matrix.wrapped_tensor.nnz,              # nnz\n",
    "                          0,                # seed\n",
    "                          32,               # mbrow\n",
    "                          4                 # brow\n",
    "                          )\n",
    "print(\"SDDMM output tensor: shape:\", sddmm_output.shape, \"\\n\", sddmm_output)\n",
    "# Densify sddmm_output to compare\n",
    "#compressed_sddmm_output = SparseVNMTensor(sparse_tensor.wrapped_tensor.v, sparse_tensor.wrapped_tensor.n, sparse_tensor.wrapped_tensor.m, \n",
    "#                              mask_=sparse_tensor.wrapped_tensor.mask, columns_=sparse_tensor.wrapped_tensor.columns, \n",
    "#                              values_=sddmm_output, metadata_=sparse_tensor.wrapped_tensor.metadata)\n",
    "compressed_sddmm_output = SparseVNMTensor(sddmm_C_matrix.wrapped_tensor.v, sddmm_C_matrix.wrapped_tensor.n, sddmm_C_matrix.wrapped_tensor.m, \n",
    "                              mask_=sddmm_C_matrix.wrapped_tensor.mask, columns_=sddmm_C_matrix.wrapped_tensor.columns, \n",
    "                              values_=sddmm_output, metadata_=sddmm_C_matrix.wrapped_tensor.metadata)\n",
    "dense_sddmm_output = compressed_sddmm_output.to_dense().cuda()\n",
    "check_VNM(v, n, m, n+2, dense_sddmm_output)\n",
    "print(\"Densified output tensor: shape:\", dense_sddmm_output.shape,\"\\n\", dense_sddmm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#print( torch.allclose(sparse_grad_input, dense_grad_input) )\n",
    "\n",
    "print( torch.allclose(dense_sddmm_output, masked_dense_grad_weights) )\n",
    "\n",
    "#print(\"dense_sddmm_output:\\n\", dense_sddmm_output)\n",
    "#print(\"masked_dense_grad_weights:\\n\", masked_dense_grad_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end2end",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
