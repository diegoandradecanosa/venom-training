{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/rtx3090/Disco2TB/inno4scale_shared/anaconda3/envs/end2end/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/media/rtx3090/Disco2TB/inno4scale_shared/anaconda3/envs/end2end/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from native_scripting import compile\n",
    "import functools\n",
    "import ctypes\n",
    "import time\n",
    "import math\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cache = functools.cache\n",
    "except AttributeError:\n",
    "    cache = functools.lru_cache(maxsize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def venom2dense(dense_shape, dense_dtype, n, m, tileM):\n",
    "    nrows = dense_shape[0]\n",
    "    ncols = dense_shape[1]\n",
    "\n",
    "    A_size = nrows*ncols\n",
    "    density = n/m\n",
    "\n",
    "    brow = 4 #this->brow = brow_;\n",
    "    mbrow = 32 #this->mbrow = mbrow_;\n",
    "\n",
    "    bm   = tileM\n",
    "    # !IMPORTANT! constants because of architecture constraints\n",
    "    m_fixed = 4\n",
    "    bits_elem_meta=2\n",
    "    mrow_m = 2\n",
    "    bits_elem_cols=8\n",
    "    brow_fixed = 16\n",
    "    nelems=32//bits_elem_meta #(sizeof(uint)*8)=32\n",
    "    nelems_col = nelems//mrow_m\n",
    "\n",
    "    A_num_cols_sp = (ncols/m)*n\n",
    "    A_num_cols_sp_pad_nm = (round_up(ncols, m)/m)*n\n",
    "    A_num_cols_sp_pad = round_up((round_up(ncols, m)/m)*n, 16)\n",
    "    A_nnz = nrows*A_num_cols_sp_pad\n",
    "\n",
    "    assert dense_dtype in (torch.float32, torch.float64)\n",
    "    dtype = \"float\" if dense_dtype == torch.float32 else \"double\"\n",
    "    lib = compile(\n",
    "        f\"\"\"\n",
    "        #include <iostream>\n",
    "        #include <algorithm>\n",
    "        #include <utility>\n",
    "        #include <cstdlib>\n",
    "        #include <cstdio>\n",
    "        #include <cmath>\n",
    "        #include <functional>\n",
    "        #include <tuple>\n",
    "        #include <vector>\n",
    "        #include <numeric>\n",
    "        #include <chrono>\n",
    "\n",
    "        using namespace std;\n",
    "\n",
    "\n",
    "        extern \"C\" void func3({dtype}* hA_dense, {dtype}* hA_values, int *hA_columns, int *hA_metadata){{\n",
    "            //this->hA_dense.resize(this->A_size, 0);\n",
    "\n",
    "            // general variables N:M format\n",
    "            int bm_m = {nrows}/{bm};\n",
    "            int mbrow_m = {bm}/{mbrow};\n",
    "            int mbrow_m2 = {mbrow}/{brow_fixed};\n",
    "            int brow_m = {brow_fixed}/{brow};\n",
    "            // metadata\n",
    "            int mcol_kk = {nelems}/{mrow_m}/{n};\n",
    "            int mcol_k = {A_num_cols_sp_pad}/{n}/mcol_kk;\n",
    "            // indices\n",
    "            int col_kk = mcol_kk;\n",
    "            int col_k = {A_num_cols_sp_pad}/{n}/col_kk;\n",
    "\n",
    "            uint indexes[{nelems}];\n",
    "            uint columns[col_kk*{m_fixed}];\n",
    "\n",
    "            for(int bm_i=0; bm_i<bm_m; bm_i++){{\n",
    "                for(int mbrow_i=0; mbrow_i<mbrow_m; mbrow_i++){{\n",
    "                    for(int mbrow_i2=0; mbrow_i2<mbrow_m2; mbrow_i2++){{\n",
    "                        for(int brow_i=0; brow_i<brow_m; brow_i++){{\n",
    "                            for(int mcol_i=0; mcol_i<mcol_k; mcol_i++){{\n",
    "                                //read columns indexes\n",
    "                                for(int col_i=0; col_i<col_kk; col_i++){{\n",
    "                                    for(int col_ii=0; col_ii<{m_fixed}; col_ii++){{\n",
    "                                        columns[col_i*{m_fixed} + col_ii] =\n",
    "                                        hA_columns[bm_i*col_k*col_kk*{m_fixed} + mcol_i*col_kk*{m_fixed} + col_i*{m_fixed} + col_ii];\n",
    "                                    }}\n",
    "                                }}\n",
    "                                // read metadata\n",
    "                                for(int mbrow_ii=0; mbrow_ii<({brow}/{mrow_m}); mbrow_ii++){{\n",
    "                                    for(int mbrow_iii=0; mbrow_iii<{mrow_m}; mbrow_iii++){{\n",
    "                                        for(int mcol_ii=0; mcol_ii<mcol_kk; mcol_ii++){{\n",
    "                                            for (int n_i=0; n_i<{n}; n_i++) {{\n",
    "                                                indexes[\n",
    "                                                    mbrow_iii*{n} +\n",
    "                                                    mcol_ii*{mrow_m}*{n} +\n",
    "                                                    n_i] =\n",
    "                                                (((hA_metadata[\n",
    "                                                    bm_i*mcol_k*{bm}/{mrow_m} +\n",
    "                                                    mbrow_i*mcol_k*{mbrow}/{mrow_m} +\n",
    "                                                    mbrow_i2*{brow_fixed}/{mrow_m} +\n",
    "                                                    brow_i*{brow}/{mrow_m}  +\n",
    "                                                    mcol_i*{mbrow}/{mrow_m} +\n",
    "                                                    mbrow_ii]) >> (mbrow_iii*({nelems}/{mrow_m})*{bits_elem_meta}+mcol_ii*{n}*{bits_elem_meta}+n_i*{bits_elem_meta})) & 0x3);\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "\n",
    "                                    for(int mcol_ii=0; mcol_ii<mcol_kk; mcol_ii++){{\n",
    "                                        for(int mbrow_iii=0; mbrow_iii<{mrow_m}; mbrow_iii++){{\n",
    "                                            for(int n_i=0; n_i<{n}; n_i++){{\n",
    "                                                unsigned int index = columns[mcol_ii*{m_fixed} + indexes[mcol_ii*{mrow_m}*{n}+mbrow_iii*{n}+n_i]];\n",
    "\n",
    "                                                if((mcol_i*{m}*mcol_kk + mcol_ii*{m} + index) < {ncols}){{\n",
    "                                                    hA_dense[\n",
    "                                                        bm_i*{bm}*{ncols} +\n",
    "                                                        mbrow_i*{mbrow}*{ncols} +\n",
    "                                                        mbrow_i2*{brow_fixed}*{ncols} +\n",
    "                                                        brow_i*{brow}*{ncols} +\n",
    "                                                        mcol_i*{m}*mcol_kk +\n",
    "                                                        mbrow_ii*{mrow_m}*{ncols} +\n",
    "                                                        mcol_ii*{m} +\n",
    "                                                        mbrow_iii*{ncols} +\n",
    "                                                        index] =\n",
    "                                                    hA_values[\n",
    "                                                        bm_i*{bm}*{A_num_cols_sp_pad} +\n",
    "                                                        mbrow_i*{mbrow}*{A_num_cols_sp_pad}+\n",
    "                                                        mbrow_i2*{brow_fixed}*{A_num_cols_sp_pad}+\n",
    "                                                        brow_i*{brow}*{nelems}/{mrow_m}+\n",
    "                                                        mcol_i*{brow_fixed}*{nelems}/{mrow_m} +\n",
    "                                                        mbrow_ii*{mrow_m}*{n} +\n",
    "                                                        mcol_ii*{n}*{brow} +\n",
    "                                                        mbrow_iii*{n} +\n",
    "                                                        n_i];\n",
    "                                                }}\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }}\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\",\n",
    "    )\n",
    "    lib.func3.argtypes = [\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "    ]\n",
    "    return lib.func3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def dense2venom(dense_shape, dense_dtype, n, m, tileM):\n",
    "    nrows = dense_shape[0]\n",
    "    ncols = dense_shape[1]\n",
    "\n",
    "    brow = 4 #this->brow = brow_;\n",
    "    mbrow = 32 #this->mbrow = mbrow_;\n",
    "\n",
    "    bm   = tileM\n",
    "    # !IMPORTANT! constants because of architecture constraints\n",
    "    m_fixed = 4\n",
    "    bits_elem_meta=2\n",
    "    mrow_m = 2\n",
    "    bits_elem_cols=8\n",
    "    brow_fixed = 16\n",
    "    nelems=32//bits_elem_meta #(sizeof(uint)*8)=32\n",
    "    nelems_col = nelems//mrow_m\n",
    "\n",
    "    A_num_cols_sp = (ncols//m)*n\n",
    "    A_num_cols_sp_pad_nm = (round_up(ncols, m)/m)*n\n",
    "    A_num_cols_sp_pad = round_up((round_up(ncols, m)/m)*n, 16)\n",
    "    A_nnz = nrows*A_num_cols_sp_pad\n",
    "\n",
    "    assert dense_dtype in (torch.float32, torch.float64)\n",
    "    dtype = \"float\" if dense_dtype == torch.float32 else \"double\"\n",
    "    lib = compile(\n",
    "        f\"\"\"\n",
    "        #include <iostream>\n",
    "        #include <algorithm>\n",
    "        #include <utility>\n",
    "        #include <cstdlib>\n",
    "        #include <cstdio>\n",
    "        #include <cmath>\n",
    "        #include <functional>\n",
    "        #include <tuple>\n",
    "        #include <vector>\n",
    "        #include <numeric>\n",
    "        #include <chrono>\n",
    "\n",
    "        using namespace std;\n",
    "\n",
    "\n",
    "        extern \"C\" void func2({dtype}* sparse, int* masks, {dtype}* hA_values, int *hA_columns, int *hA_metadata){{\n",
    "\n",
    "            int bm_m = {nrows}/{bm};\n",
    "            int mbrow_m = {bm}/{mbrow};\n",
    "            int mbrow_m2 = {mbrow}/{brow_fixed};\n",
    "            int brow_m = {brow_fixed}/{brow};\n",
    "            // metadata\n",
    "            int mcol_kk = {nelems}/{mrow_m}/{n};\n",
    "            int mcol_k = {A_num_cols_sp_pad}/{n}/mcol_kk;\n",
    "            // indices\n",
    "            int col_kk = mcol_kk;\n",
    "            int col_k = {A_num_cols_sp_pad}/{n}/col_kk;\n",
    "\n",
    "            {dtype} values[{nelems}];\n",
    "            uint indexes[{nelems}];\n",
    "            uint columns[col_kk*{m_fixed}];\n",
    "\n",
    "            int max_idx = 0;\n",
    "\n",
    "            for(int bm_i=0; bm_i<bm_m; bm_i++){{\n",
    "                for(int mbrow_i=0; mbrow_i<mbrow_m; mbrow_i++){{\n",
    "                    for(int mbrow_i2=0; mbrow_i2<mbrow_m2; mbrow_i2++){{\n",
    "                        for(int brow_i=0; brow_i<brow_m; brow_i++){{\n",
    "                            for(int mcol_i=0; mcol_i<mcol_k; mcol_i++){{\n",
    "                                for(int col_i=0; col_i<col_kk; col_i++){{\n",
    "                                    for(int col_ii=0; col_ii<{m_fixed}; col_ii++){{\n",
    "                                        columns[col_i*{m_fixed} + col_ii] =\n",
    "                                        hA_columns[bm_i*col_k*col_kk*{m_fixed} + mcol_i*col_kk*{m_fixed} + col_i*{m_fixed} + col_ii];\n",
    "                                    }}\n",
    "                                }}\n",
    "                                for(int mbrow_ii=0; mbrow_ii<({brow}/{mrow_m}); mbrow_ii++){{\n",
    "                                    for(int mcol_ii=0; mcol_ii<mcol_kk; mcol_ii++){{\n",
    "                                        for(int mbrow_iii=0; mbrow_iii<{mrow_m}; mbrow_iii++){{\n",
    "                                            int pos=0;\n",
    "                                            for(int n_i=0; n_i<{m_fixed}; n_i++){{\n",
    "                                                unsigned int index = columns[mcol_ii*{m_fixed} + n_i];\n",
    "\n",
    "                                                if((mcol_i*{m}*mcol_kk + mcol_ii*{m} + index) < {ncols}){{\n",
    "                                                    int nnz = masks[\n",
    "                                                            bm_i*{bm}*{ncols} +\n",
    "                                                            mbrow_i*{mbrow}*{ncols} +\n",
    "                                                            mbrow_i2*{brow_fixed}*{ncols} +\n",
    "                                                            brow_i*{brow}*{ncols} +\n",
    "                                                            mcol_i*{m}*mcol_kk +\n",
    "                                                            mbrow_ii*{mrow_m}*{ncols} +\n",
    "                                                            mcol_ii*{m} +\n",
    "                                                            mbrow_iii*{ncols} +\n",
    "                                                            index];\n",
    "\n",
    "                                                    if(nnz != 0){{\n",
    "                                                        indexes[\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            pos] = n_i;\n",
    "\n",
    "                                                        values[\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            pos] =\n",
    "                                                        sparse[\n",
    "                                                            bm_i*{bm}*{ncols} +\n",
    "                                                            mbrow_i*{mbrow}*{ncols} +\n",
    "                                                            mbrow_i2*{brow_fixed}*{ncols} +\n",
    "                                                            brow_i*{brow}*{ncols} +\n",
    "                                                            mcol_i*{m}*mcol_kk +\n",
    "                                                            mbrow_ii*{mrow_m}*{ncols} +\n",
    "                                                            mcol_ii*{m} +\n",
    "                                                            mbrow_iii*{ncols} +\n",
    "                                                            index];\n",
    "\n",
    "                                                        pos+=1;\n",
    "                                                    }}\n",
    "                                                }} else {{\n",
    "                                                    if(n_i<2){{\n",
    "                                                        indexes[\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            pos] = 0;\n",
    "\n",
    "                                                        values[\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            pos] = 0;\n",
    "\n",
    "                                                        pos+=1;\n",
    "                                                    }}\n",
    "                                                }}\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                    // write metadata\n",
    "                                    unsigned int meta=0;\n",
    "                                    for(int mbrow_iii=0; mbrow_iii<{mrow_m}; mbrow_iii++){{\n",
    "                                        for(int mcol_ii=0; mcol_ii<mcol_kk; mcol_ii++){{\n",
    "                                            for (int n_i=0; n_i<{n}; n_i++) {{\n",
    "\n",
    "                                                int idx = bm_i*{bm}*{A_num_cols_sp_pad} +\n",
    "                                                        mbrow_i*{mbrow}*{A_num_cols_sp_pad}+\n",
    "                                                        mbrow_i2*{brow_fixed}*{A_num_cols_sp_pad}+\n",
    "                                                        brow_i*{brow}*{nelems}/{mrow_m}+\n",
    "                                                        mcol_i*{brow_fixed}*{nelems}/{mrow_m} +\n",
    "                                                        mbrow_ii*{mrow_m}*{n} +\n",
    "                                                        mcol_ii*{n}*{brow} +\n",
    "                                                        mbrow_iii*{n} +\n",
    "                                                        n_i;\n",
    "\n",
    "                                                max_idx = (idx>max_idx)?(idx):(max_idx);\n",
    "\n",
    "                                                hA_values[\n",
    "                                                        idx] =\n",
    "                                                values[\n",
    "                                                    mcol_ii*{mrow_m}*{n} +\n",
    "                                                    mbrow_iii*{n} +\n",
    "                                                    n_i];\n",
    "\n",
    "                                                unsigned int tmp = indexes[\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            n_i];\n",
    "                                                meta |= (tmp << (mbrow_iii*({nelems}/{mrow_m})*{bits_elem_meta}+mcol_ii*{n}*{bits_elem_meta}+n_i*{bits_elem_meta}));\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                    hA_metadata[bm_i*mcol_k*{bm}/{mrow_m} +\n",
    "                                                mbrow_i*mcol_k*{mbrow}/{mrow_m} +\n",
    "                                                mbrow_i2*{brow_fixed}/{mrow_m} +\n",
    "                                                brow_i*{brow}/{mrow_m}  +\n",
    "                                                mcol_i*{mbrow}/{mrow_m} +\n",
    "                                                mbrow_ii] = meta;\n",
    "                                }}\n",
    "                            }}\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "            cout << \"max_idx: \" << max_idx << endl;\n",
    "        }}\n",
    "        \"\"\",\n",
    "    )\n",
    "    lib.func2.argtypes = [\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "    ]\n",
    "    return lib.func2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(x,y):\n",
    "    return math.ceil(x/y)*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseVNMTensor:\n",
    "    def __init__(self, n_, m_, v_, dense_, mask_, columns_, device_):\n",
    "        self.n = n_\n",
    "        self.m = m_\n",
    "        self.v = v_\n",
    "        self.nnz = 0\n",
    "        self.nrows = None\n",
    "        self.ncols = None\n",
    "        \n",
    "        self.dense = dense_.cpu().to(dtype=torch.float32)\n",
    "        self.device=device_\n",
    "        \n",
    "        self.columns = columns_\n",
    "        self.values = None\n",
    "        self.metadata = None\n",
    "\n",
    "        self.mask = mask_\n",
    "\n",
    "        self.to_venom(dense_.cpu().to(dtype=torch.float32), mask_.cpu())\n",
    "\n",
    "    def to_venom(self, dense_, mask_):\n",
    "        impl_builder = (\n",
    "            dense2venom\n",
    "            )\n",
    "        func = impl_builder(\n",
    "                dense_.shape,\n",
    "                dense_.dtype,\n",
    "                self.n,\n",
    "                self.m,\n",
    "                self.v\n",
    "            )\n",
    "\n",
    "        self.nrows, self.ncols = dense_.shape\n",
    "        A_num_cols_sp_pad = round_up((round_up(self.ncols, self.m)/self.m)*self.n, 16)\n",
    "        self.nnz = self.nrows*A_num_cols_sp_pad\n",
    "        m_fixed = 4\n",
    "        mrow_m = 2\n",
    "        bits_elem_meta=2\n",
    "\n",
    "        nelems = 32//bits_elem_meta\n",
    "        nelems_col = nelems//mrow_m\n",
    "\n",
    "        self.values = torch.zeros(self.nrows * A_num_cols_sp_pad, dtype=torch.float32, device=\"cpu\")\n",
    "        self.metadata = torch.zeros(self.nrows//mrow_m * A_num_cols_sp_pad//nelems_col, dtype=torch.int32, device=\"cpu\")\n",
    "        \n",
    "        func(dense_.data_ptr(), mask_.data_ptr(), self.values.data_ptr(), self.columns.data_ptr(), self.metadata.data_ptr())\n",
    "\n",
    "    def to_dense(self):\n",
    "        impl_builder = (\n",
    "            venom2dense\n",
    "            )\n",
    "        func = impl_builder(\n",
    "                (self.nrows, self.ncols),\n",
    "                torch.float32, \n",
    "                self.n,\n",
    "                self.m,\n",
    "                self.v\n",
    "            )\n",
    "        # initialize with ones\n",
    "        dense = torch.zeros((self.nrows, self.ncols), dtype=self.values.dtype, device=\"cpu\", requires_grad=True)\n",
    "        \n",
    "        func(dense.data_ptr(), self.values.data_ptr(), self.columns.data_ptr(), self.metadata.data_ptr())\n",
    "\n",
    "        return dense.to(device=\"cuda:0\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMVectorSparsifier:\n",
    "    def __init__(self, n, m, v):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.v = v\n",
    "\n",
    "    @staticmethod\n",
    "    def get_random_mask(tensor, m, v):\n",
    "        mask = torch.zeros(tensor.shape, dtype=tensor.dtype)\n",
    "        m_tmp = torch.cat( (torch.tensor([1,0,1,0]), torch.zeros(m-4)), 0 )\n",
    "        mask = mask.reshape(-1, v, m) + m_tmp\n",
    "        mask = mask.reshape(tensor.shape)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def __call__(self, tensor, grad_fmt=None):\n",
    "        nrows, ncols = tensor.shape\n",
    "        columns = torch.zeros(nrows//self.v, ncols//self.m*4, dtype=torch.int32)\n",
    "        columns = columns.reshape((-1,4)) + torch.tensor([0,1,2,3], dtype=torch.int32)\n",
    "        columns = columns.reshape((nrows//self.v, ncols//self.m*4))\n",
    "\n",
    "        mask = NMVectorSparsifier.get_random_mask(tensor, self.m, self.v)\n",
    "\n",
    "        sparse_mtx = sten.SparseTensorWrapper.wrapped_from_dense(\n",
    "            SparseVNMTensor(self.n, self.m, self.v, tensor, mask, columns, tensor.device),\n",
    "            tensor,\n",
    "            grad_fmt,\n",
    "        )\n",
    "\n",
    "        return sparse_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "input = torch.randn(1024, 256, requires_grad=True, dtype=torch.half, device=\"cuda:0\")\n",
    "weight = torch.nn.Linear(256, 512, bias=True, dtype=torch.half, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=128\n",
    "n=2\n",
    "m=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spatha\n",
    "import spatha_sddmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_grad_output = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_dense_mul_dispatch(sparse_values, sparse_indices, sparse_metadata, dense, nrows_sp, ncols_sp, ncols_d, m, n, v, nnz, bias):\n",
    "\n",
    "    dense_ = dense.contiguous()\n",
    "\n",
    "    print(sparse_metadata.dtype, sparse_metadata.device, sparse_metadata.shape)\n",
    "    print(sparse_indices.dtype, sparse_indices.device, sparse_indices.shape)\n",
    "    print(sparse_values.dtype, sparse_values.device, sparse_values.shape)\n",
    "    print(dense_.dtype, dense_.device, dense_.shape)\n",
    "    print(bias.dtype, bias.device, bias.shape)\n",
    "    print(nrows_sp, ncols_sp, ncols_d)\n",
    "    print(v, n, m, nnz, 0, 32, 4)\n",
    "\n",
    "    output = spatha.spmm(\n",
    "                          sparse_metadata,  # metadata\n",
    "                          sparse_indices,   # indices\n",
    "                          sparse_values,    # values\n",
    "                          dense_,           # rhs_matrix\n",
    "                          bias,             # bias\n",
    "                          nrows_sp,         # A_num_rows\n",
    "                          ncols_sp,         # A_num_cols\n",
    "                          ncols_d,          # B_num_cols\n",
    "                          v,                # V\n",
    "                          n,                # N\n",
    "                          m,                # M\n",
    "                          nnz,              # nnz\n",
    "                          0,                # seed\n",
    "                          32,               # mbrow\n",
    "                          4                 # brow\n",
    "                          )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VenomLinearFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, values, bias, columns, metadata, dense):\n",
    "        ctx.save_for_backward(input, values, bias, columns, metadata, dense)\n",
    "\n",
    "        flattened_input = torch.flatten(input, start_dim=0, end_dim=-2)\n",
    "\n",
    "        ncols_d  = flattened_input.T.shape[1]\n",
    "        DM, _    = flattened_input.shape\n",
    "\n",
    "        nrows_sp, ncols_sp = dense.shape\n",
    "\n",
    "        output = sparse_dense_mul_dispatch( values,\n",
    "                                            columns,\n",
    "                                            metadata,\n",
    "                                            flattened_input.T,\n",
    "                                            nrows_sp,\n",
    "                                            ncols_sp,\n",
    "                                            ncols_d,\n",
    "                                            m,\n",
    "                                            n,\n",
    "                                            v,\n",
    "                                            len(values),\n",
    "                                            bias)\n",
    "\n",
    "        output = output.reshape((*input.shape[0:-1], -1))[..., :nrows_sp]\n",
    "        #torch.cuda.synchronize()\n",
    "        print(\"forward\")\n",
    "            \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, values, bias, columns, metadata, dense = ctx.saved_tensors\n",
    "\n",
    "        #global global_grad_output  \n",
    "        #global_grad_output = grad_output \n",
    "\n",
    "        nrows_sp, ncols_sp = dense.shape\n",
    "\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        print(\"bck\", dense.device)\n",
    "        \n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output @ dense\n",
    "        \n",
    "        if ctx.needs_input_grad[1]:\n",
    "            #grad_weight = grad_output.t() @ input.to(\"cuda:0\")\n",
    "            #grad_output = grad_output.T.contiguous()\n",
    "            #input = input.T.contiguous()            \n",
    "            #print(metadata)\n",
    "            #print(columns)\n",
    "            input2 = input.T.contiguous()\n",
    "            grad_output2 = grad_output.T.contiguous()\n",
    "            \n",
    "            \"\"\" print(nrows_sp, ncols_sp, input2.shape[1], n, m, len(values))\n",
    "            print(grad_output2.dtype, input2.dtype)\n",
    "            print(grad_output2.device, input2.device)\n",
    "            print(grad_output2.shape, \n",
    "                    input2.shape, \n",
    "                    metadata.shape, \n",
    "                    columns.shape) \"\"\"\n",
    "            \n",
    "            grad_weight = spatha_sddmm.sddmm(\n",
    "                                            grad_output2,     # A_matrix\n",
    "                                            input2,                  # B_matrix\n",
    "                                            metadata, # C_metadata\n",
    "                                            columns,  # C_indices\n",
    "                                            nrows_sp, # C_num_rows\n",
    "                                            ncols_sp, # C_num_cols    \n",
    "                                            input2.shape[1], \n",
    "                                            n,                      # N\n",
    "                                            m,                      # M\n",
    "                                            len(values),      # nnz\n",
    "                                            0,                      # seed\n",
    "                                            32,                     # mbrow\n",
    "                                            4                       # brow\n",
    "                                            )\n",
    "            #torch.cuda.synchronize()\n",
    "            #dense_grad_weight = grad_output.t() @ input\n",
    "            #print(\"dense_grad_weight\", dense_grad_weight[:4, :4])\n",
    "            #print(\"grad_weight\", grad_weight[:4, :4])\n",
    "        \n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "        \n",
    "        #print(ctx.needs_input_grad[0], ctx.needs_input_grad[1], ctx.needs_input_grad[2])\n",
    "        \"\"\" print(grad_output)\n",
    "        print(input)\n",
    "        print(grad_weight) \"\"\"\n",
    "        #print(grad_input[:4, :4])\n",
    "        #print(\"grad_weight\", grad_weight[:4, :4])\n",
    "\n",
    "        \n",
    "        return grad_input, grad_weight.flatten(), grad_bias, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SrnmSpmm(torch.nn.Module):\n",
    "    def __init__(self, original: torch.nn.Linear):\n",
    "        super(SrnmSpmm, self).__init__()        \n",
    "\n",
    "        self.w = NMVectorSparsifier(n, m, v)(original.weight).wrapped_tensor\n",
    "\n",
    "        self.values = torch.nn.Parameter(self.w.values.to(device=\"cuda:0\").half())\n",
    "        self.columns = self.w.columns.to(device=\"cuda:0\")\n",
    "        self.metadata = self.w.metadata.to(device=\"cuda:0\")\n",
    "\n",
    "        self.bias = original.bias\n",
    "\n",
    "        self.dense = self.w.to_dense() #torch.nn.Parameter(self.w.to_dense())\n",
    "        self.mask = self.w.mask\n",
    "\n",
    "        self.nrows_sp = self.w.nrows\n",
    "        self.ncols_sp = self.w.ncols\n",
    "        self.nnz      = self.w.nnz\n",
    "\n",
    "    def forward(self, input):        \n",
    "        return VenomLinearFunction.apply(\n",
    "                                        input, \n",
    "                                        self.values,\n",
    "                                        self.bias, \n",
    "                                        self.columns,\n",
    "                                        self.metadata,                                            \n",
    "                                        self.dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_idx: 32767\n",
      "<class '__main__.SrnmSpmm'>\n"
     ]
    }
   ],
   "source": [
    "sparse_weight = SrnmSpmm(weight)\n",
    "\n",
    "print( type(sparse_weight) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int32 cuda:0 torch.Size([2048])\n",
      "torch.int32 cuda:0 torch.Size([4, 128])\n",
      "torch.float16 cuda:0 torch.Size([32768])\n",
      "torch.float16 cuda:0 torch.Size([256, 1024])\n",
      "torch.float16 cuda:0 torch.Size([512])\n",
      "512 256 1024\n",
      "128 2 8 32768 0 32 4\n",
      "forward\n",
      "tensor([[-0.25,  0.05,  0.24,  ...,  0.22, -0.05, -0.48],\n",
      "        [-0.07,  0.33,  0.62,  ...,  0.40,  0.09, -0.27],\n",
      "        [-0.07,  0.48, -0.22,  ...,  0.47,  0.02,  0.31],\n",
      "        ...,\n",
      "        [ 0.40, -0.51,  0.20,  ...,  0.24, -0.46, -0.13],\n",
      "        [ 0.32, -0.21,  0.14,  ...,  0.12, -0.25,  0.04],\n",
      "        [ 0.07,  0.13, -0.53,  ...,  0.55, -0.01, -0.03]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<VenomLinearFunctionBackward>)\n"
     ]
    }
   ],
   "source": [
    "sparse = sparse_weight(input)\n",
    "torch.cuda.synchronize()\n",
    "print(sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.14e-02, -5.67e-02,  1.94e-05,  1.80e-02, -5.86e-02,  5.06e-02,\n",
      "         3.96e-02,  4.44e-02], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sparse_weight.values[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bck cuda:0\n"
     ]
    }
   ],
   "source": [
    "sparse.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-14.95,   1.29, -14.95,   1.29, -14.95,   1.29, -14.95,   1.29],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(sparse_weight.values.grad[:8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
