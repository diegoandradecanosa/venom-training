{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from native_scripting import compile\n",
    "import functools\n",
    "import ctypes\n",
    "import time\n",
    "import math\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cache = functools.cache\n",
    "except AttributeError:\n",
    "    cache = functools.lru_cache(maxsize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def venom2dense(dense_shape, dense_dtype, n, m, tileM):\n",
    "    nrows = dense_shape[0]\n",
    "    ncols = dense_shape[1]\n",
    "\n",
    "    A_size = nrows*ncols\n",
    "    density = n/m\n",
    "\n",
    "    brow = 4 #this->brow = brow_;\n",
    "    mbrow = 32 #this->mbrow = mbrow_;\n",
    "\n",
    "    bm   = tileM\n",
    "    # !IMPORTANT! constants because of architecture constraints\n",
    "    m_fixed = 4\n",
    "    bits_elem_meta=2\n",
    "    mrow_m = 2\n",
    "    bits_elem_cols=8\n",
    "    brow_fixed = 16\n",
    "    nelems=32//bits_elem_meta #(sizeof(uint)*8)=32\n",
    "    nelems_col = nelems//mrow_m\n",
    "\n",
    "    A_num_cols_sp = (ncols/m)*n\n",
    "    A_num_cols_sp_pad_nm = (round_up(ncols, m)/m)*n\n",
    "    A_num_cols_sp_pad = round_up((round_up(ncols, m)/m)*n, 16)\n",
    "    A_nnz = nrows*A_num_cols_sp_pad\n",
    "\n",
    "    assert dense_dtype in (torch.float32, torch.float64)\n",
    "    dtype = \"float\" if dense_dtype == torch.float32 else \"double\"\n",
    "    lib = compile(\n",
    "        f\"\"\"\n",
    "        #include <iostream>\n",
    "        #include <algorithm>\n",
    "        #include <utility>\n",
    "        #include <cstdlib>\n",
    "        #include <cstdio>\n",
    "        #include <cmath>\n",
    "        #include <functional>\n",
    "        #include <tuple>\n",
    "        #include <vector>\n",
    "        #include <numeric>\n",
    "        #include <chrono>\n",
    "\n",
    "        using namespace std;\n",
    "\n",
    "\n",
    "        extern \"C\" void func3({dtype}* hA_dense, {dtype}* hA_values, int *hA_columns, int *hA_metadata){{\n",
    "            //this->hA_dense.resize(this->A_size, 0);\n",
    "\n",
    "            // general variables N:M format\n",
    "            int bm_m = {nrows}/{bm};\n",
    "            int mbrow_m = {bm}/{mbrow};\n",
    "            int mbrow_m2 = {mbrow}/{brow_fixed};\n",
    "            int brow_m = {brow_fixed}/{brow};\n",
    "            // metadata\n",
    "            int mcol_kk = {nelems}/{mrow_m}/{n};\n",
    "            int mcol_k = {A_num_cols_sp_pad}/{n}/mcol_kk;\n",
    "            // indices\n",
    "            int col_kk = mcol_kk;\n",
    "            int col_k = {A_num_cols_sp_pad}/{n}/col_kk;\n",
    "\n",
    "            uint indexes[{nelems}];\n",
    "            uint columns[col_kk*{m_fixed}];\n",
    "\n",
    "            for(int bm_i=0; bm_i<bm_m; bm_i++){{\n",
    "                for(int mbrow_i=0; mbrow_i<mbrow_m; mbrow_i++){{\n",
    "                    for(int mbrow_i2=0; mbrow_i2<mbrow_m2; mbrow_i2++){{\n",
    "                        for(int brow_i=0; brow_i<brow_m; brow_i++){{\n",
    "                            for(int mcol_i=0; mcol_i<mcol_k; mcol_i++){{\n",
    "                                //read columns indexes\n",
    "                                for(int col_i=0; col_i<col_kk; col_i++){{\n",
    "                                    for(int col_ii=0; col_ii<{m_fixed}; col_ii++){{\n",
    "                                        columns[col_i*{m_fixed} + col_ii] =\n",
    "                                        hA_columns[bm_i*col_k*col_kk*{m_fixed} + mcol_i*col_kk*{m_fixed} + col_i*{m_fixed} + col_ii];\n",
    "                                    }}\n",
    "                                }}\n",
    "                                // read metadata\n",
    "                                for(int mbrow_ii=0; mbrow_ii<({brow}/{mrow_m}); mbrow_ii++){{\n",
    "                                    for(int mbrow_iii=0; mbrow_iii<{mrow_m}; mbrow_iii++){{\n",
    "                                        for(int mcol_ii=0; mcol_ii<mcol_kk; mcol_ii++){{\n",
    "                                            for (int n_i=0; n_i<{n}; n_i++) {{\n",
    "                                                indexes[\n",
    "                                                    mbrow_iii*{n} +\n",
    "                                                    mcol_ii*{mrow_m}*{n} +\n",
    "                                                    n_i] =\n",
    "                                                (((hA_metadata[\n",
    "                                                    bm_i*mcol_k*{bm}/{mrow_m} +\n",
    "                                                    mbrow_i*mcol_k*{mbrow}/{mrow_m} +\n",
    "                                                    mbrow_i2*{brow_fixed}/{mrow_m} +\n",
    "                                                    brow_i*{brow}/{mrow_m}  +\n",
    "                                                    mcol_i*{mbrow}/{mrow_m} +\n",
    "                                                    mbrow_ii]) >> (mbrow_iii*({nelems}/{mrow_m})*{bits_elem_meta}+mcol_ii*{n}*{bits_elem_meta}+n_i*{bits_elem_meta})) & 0x3);\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "\n",
    "                                    for(int mcol_ii=0; mcol_ii<mcol_kk; mcol_ii++){{\n",
    "                                        for(int mbrow_iii=0; mbrow_iii<{mrow_m}; mbrow_iii++){{\n",
    "                                            for(int n_i=0; n_i<{n}; n_i++){{\n",
    "                                                unsigned int index = columns[mcol_ii*{m_fixed} + indexes[mcol_ii*{mrow_m}*{n}+mbrow_iii*{n}+n_i]];\n",
    "\n",
    "                                                if((mcol_i*{m}*mcol_kk + mcol_ii*{m} + index) < {ncols}){{\n",
    "                                                    hA_dense[\n",
    "                                                        bm_i*{bm}*{ncols} +\n",
    "                                                        mbrow_i*{mbrow}*{ncols} +\n",
    "                                                        mbrow_i2*{brow_fixed}*{ncols} +\n",
    "                                                        brow_i*{brow}*{ncols} +\n",
    "                                                        mcol_i*{m}*mcol_kk +\n",
    "                                                        mbrow_ii*{mrow_m}*{ncols} +\n",
    "                                                        mcol_ii*{m} +\n",
    "                                                        mbrow_iii*{ncols} +\n",
    "                                                        index] =\n",
    "                                                    hA_values[\n",
    "                                                        bm_i*{bm}*{A_num_cols_sp_pad} +\n",
    "                                                        mbrow_i*{mbrow}*{A_num_cols_sp_pad}+\n",
    "                                                        mbrow_i2*{brow_fixed}*{A_num_cols_sp_pad}+\n",
    "                                                        brow_i*{brow}*{nelems}/{mrow_m}+\n",
    "                                                        mcol_i*{brow_fixed}*{nelems}/{mrow_m} +\n",
    "                                                        mbrow_ii*{mrow_m}*{n} +\n",
    "                                                        mcol_ii*{n}*{brow} +\n",
    "                                                        mbrow_iii*{n} +\n",
    "                                                        n_i];\n",
    "                                                }}\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                }}\n",
    "                            }}\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        \"\"\",\n",
    "    )\n",
    "    lib.func3.argtypes = [\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "    ]\n",
    "    return lib.func3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache\n",
    "def dense2venom(dense_shape, dense_dtype, n, m, tileM):\n",
    "    nrows = dense_shape[0]\n",
    "    ncols = dense_shape[1]\n",
    "\n",
    "    brow = 4 #this->brow = brow_;\n",
    "    mbrow = 32 #this->mbrow = mbrow_;\n",
    "\n",
    "    bm   = tileM\n",
    "    # !IMPORTANT! constants because of architecture constraints\n",
    "    m_fixed = 4\n",
    "    bits_elem_meta=2\n",
    "    mrow_m = 2\n",
    "    bits_elem_cols=8\n",
    "    brow_fixed = 16\n",
    "    nelems=32//bits_elem_meta #(sizeof(uint)*8)=32\n",
    "    nelems_col = nelems//mrow_m\n",
    "\n",
    "    A_num_cols_sp = (ncols//m)*n\n",
    "    A_num_cols_sp_pad_nm = (round_up(ncols, m)/m)*n\n",
    "    A_num_cols_sp_pad = round_up((round_up(ncols, m)/m)*n, 16)\n",
    "    A_nnz = nrows*A_num_cols_sp_pad\n",
    "\n",
    "    assert dense_dtype in (torch.float32, torch.float64)\n",
    "    dtype = \"float\" if dense_dtype == torch.float32 else \"double\"\n",
    "    lib = compile(\n",
    "        f\"\"\"\n",
    "        #include <iostream>\n",
    "        #include <algorithm>\n",
    "        #include <utility>\n",
    "        #include <cstdlib>\n",
    "        #include <cstdio>\n",
    "        #include <cmath>\n",
    "        #include <functional>\n",
    "        #include <tuple>\n",
    "        #include <vector>\n",
    "        #include <numeric>\n",
    "        #include <chrono>\n",
    "\n",
    "        using namespace std;\n",
    "\n",
    "\n",
    "        extern \"C\" void func2({dtype}* sparse, int* masks, {dtype}* hA_values, int *hA_columns, int *hA_metadata){{\n",
    "\n",
    "            int bm_m = {nrows}/{bm};\n",
    "            int mbrow_m = {bm}/{mbrow};\n",
    "            int mbrow_m2 = {mbrow}/{brow_fixed};\n",
    "            int brow_m = {brow_fixed}/{brow};\n",
    "            // metadata\n",
    "            int mcol_kk = {nelems}/{mrow_m}/{n};\n",
    "            int mcol_k = {A_num_cols_sp_pad}/{n}/mcol_kk;\n",
    "            // indices\n",
    "            int col_kk = mcol_kk;\n",
    "            int col_k = {A_num_cols_sp_pad}/{n}/col_kk;\n",
    "\n",
    "            {dtype} values[{nelems}];\n",
    "            uint indexes[{nelems}];\n",
    "            uint columns[col_kk*{m_fixed}];\n",
    "\n",
    "            int max_idx = 0;\n",
    "\n",
    "            for(int bm_i=0; bm_i<bm_m; bm_i++){{\n",
    "                for(int mbrow_i=0; mbrow_i<mbrow_m; mbrow_i++){{\n",
    "                    for(int mbrow_i2=0; mbrow_i2<mbrow_m2; mbrow_i2++){{\n",
    "                        for(int brow_i=0; brow_i<brow_m; brow_i++){{\n",
    "                            for(int mcol_i=0; mcol_i<mcol_k; mcol_i++){{\n",
    "                                for(int col_i=0; col_i<col_kk; col_i++){{\n",
    "                                    for(int col_ii=0; col_ii<{m_fixed}; col_ii++){{\n",
    "                                        columns[col_i*{m_fixed} + col_ii] =\n",
    "                                        hA_columns[bm_i*col_k*col_kk*{m_fixed} + mcol_i*col_kk*{m_fixed} + col_i*{m_fixed} + col_ii];\n",
    "                                    }}\n",
    "                                }}\n",
    "                                for(int mbrow_ii=0; mbrow_ii<({brow}/{mrow_m}); mbrow_ii++){{\n",
    "                                    for(int mcol_ii=0; mcol_ii<mcol_kk; mcol_ii++){{\n",
    "                                        for(int mbrow_iii=0; mbrow_iii<{mrow_m}; mbrow_iii++){{\n",
    "                                            int pos=0;\n",
    "                                            for(int n_i=0; n_i<{m_fixed}; n_i++){{\n",
    "                                                unsigned int index = columns[mcol_ii*{m_fixed} + n_i];\n",
    "\n",
    "                                                if((mcol_i*{m}*mcol_kk + mcol_ii*{m} + index) < {ncols}){{\n",
    "                                                    int nnz = masks[\n",
    "                                                            bm_i*{bm}*{ncols} +\n",
    "                                                            mbrow_i*{mbrow}*{ncols} +\n",
    "                                                            mbrow_i2*{brow_fixed}*{ncols} +\n",
    "                                                            brow_i*{brow}*{ncols} +\n",
    "                                                            mcol_i*{m}*mcol_kk +\n",
    "                                                            mbrow_ii*{mrow_m}*{ncols} +\n",
    "                                                            mcol_ii*{m} +\n",
    "                                                            mbrow_iii*{ncols} +\n",
    "                                                            index];\n",
    "\n",
    "                                                    if(nnz != 0){{\n",
    "                                                        indexes[\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            pos] = n_i;\n",
    "\n",
    "                                                        values[\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            pos] =\n",
    "                                                        sparse[\n",
    "                                                            bm_i*{bm}*{ncols} +\n",
    "                                                            mbrow_i*{mbrow}*{ncols} +\n",
    "                                                            mbrow_i2*{brow_fixed}*{ncols} +\n",
    "                                                            brow_i*{brow}*{ncols} +\n",
    "                                                            mcol_i*{m}*mcol_kk +\n",
    "                                                            mbrow_ii*{mrow_m}*{ncols} +\n",
    "                                                            mcol_ii*{m} +\n",
    "                                                            mbrow_iii*{ncols} +\n",
    "                                                            index];\n",
    "\n",
    "                                                        pos+=1;\n",
    "                                                    }}\n",
    "                                                }} else {{\n",
    "                                                    if(n_i<2){{\n",
    "                                                        indexes[\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            pos] = 0;\n",
    "\n",
    "                                                        values[\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            pos] = 0;\n",
    "\n",
    "                                                        pos+=1;\n",
    "                                                    }}\n",
    "                                                }}\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                    // write metadata\n",
    "                                    unsigned int meta=0;\n",
    "                                    for(int mbrow_iii=0; mbrow_iii<{mrow_m}; mbrow_iii++){{\n",
    "                                        for(int mcol_ii=0; mcol_ii<mcol_kk; mcol_ii++){{\n",
    "                                            for (int n_i=0; n_i<{n}; n_i++) {{\n",
    "\n",
    "                                                int idx = bm_i*{bm}*{A_num_cols_sp_pad} +\n",
    "                                                        mbrow_i*{mbrow}*{A_num_cols_sp_pad}+\n",
    "                                                        mbrow_i2*{brow_fixed}*{A_num_cols_sp_pad}+\n",
    "                                                        brow_i*{brow}*{nelems}/{mrow_m}+\n",
    "                                                        mcol_i*{brow_fixed}*{nelems}/{mrow_m} +\n",
    "                                                        mbrow_ii*{mrow_m}*{n} +\n",
    "                                                        mcol_ii*{n}*{brow} +\n",
    "                                                        mbrow_iii*{n} +\n",
    "                                                        n_i;\n",
    "\n",
    "                                                max_idx = (idx>max_idx)?(idx):(max_idx);\n",
    "\n",
    "                                                hA_values[\n",
    "                                                        idx] =\n",
    "                                                values[\n",
    "                                                    mcol_ii*{mrow_m}*{n} +\n",
    "                                                    mbrow_iii*{n} +\n",
    "                                                    n_i];\n",
    "\n",
    "                                                unsigned int tmp = indexes[\n",
    "                                                            mbrow_iii*{n} +\n",
    "                                                            mcol_ii*{mrow_m}*{n} +\n",
    "                                                            n_i];\n",
    "                                                meta |= (tmp << (mbrow_iii*({nelems}/{mrow_m})*{bits_elem_meta}+mcol_ii*{n}*{bits_elem_meta}+n_i*{bits_elem_meta}));\n",
    "                                            }}\n",
    "                                        }}\n",
    "                                    }}\n",
    "                                    hA_metadata[bm_i*mcol_k*{bm}/{mrow_m} +\n",
    "                                                mbrow_i*mcol_k*{mbrow}/{mrow_m} +\n",
    "                                                mbrow_i2*{brow_fixed}/{mrow_m} +\n",
    "                                                brow_i*{brow}/{mrow_m}  +\n",
    "                                                mcol_i*{mbrow}/{mrow_m} +\n",
    "                                                mbrow_ii] = meta;\n",
    "                                }}\n",
    "                            }}\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "            }}\n",
    "            cout << \"max_idx: \" << max_idx << endl;\n",
    "        }}\n",
    "        \"\"\",\n",
    "    )\n",
    "    lib.func2.argtypes = [\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "        ctypes.c_void_p,\n",
    "    ]\n",
    "    return lib.func2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up(x,y):\n",
    "    return math.ceil(x/y)*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseVNMTensor:\n",
    "    def __init__(self, n_, m_, v_, dense_, mask_, columns_, device_):\n",
    "        self.n = n_\n",
    "        self.m = m_\n",
    "        self.v = v_\n",
    "        self.nnz = 0\n",
    "        self.nrows = None\n",
    "        self.ncols = None\n",
    "        \n",
    "        self.dense = dense_.cpu().to(dtype=torch.float32)\n",
    "        self.device=device_\n",
    "        \n",
    "        self.columns = columns_\n",
    "        self.values = None\n",
    "        self.metadata = None\n",
    "\n",
    "        self.mask = mask_\n",
    "\n",
    "        self.to_venom(dense_.cpu().to(dtype=torch.float32), mask_.cpu())\n",
    "\n",
    "    def to_venom(self, dense_, mask_):\n",
    "        impl_builder = (\n",
    "            dense2venom\n",
    "            )\n",
    "        func = impl_builder(\n",
    "                dense_.shape,\n",
    "                dense_.dtype,\n",
    "                self.n,\n",
    "                self.m,\n",
    "                self.v\n",
    "            )\n",
    "\n",
    "        self.nrows, self.ncols = dense_.shape\n",
    "        A_num_cols_sp_pad = round_up((round_up(self.ncols, self.m)/self.m)*self.n, 16)\n",
    "        self.nnz = self.nrows*A_num_cols_sp_pad\n",
    "        m_fixed = 4\n",
    "        mrow_m = 2\n",
    "        bits_elem_meta=2\n",
    "\n",
    "        nelems = 32//bits_elem_meta\n",
    "        nelems_col = nelems//mrow_m\n",
    "\n",
    "        self.values = torch.zeros(self.nrows * A_num_cols_sp_pad, dtype=torch.float32, device=\"cpu\")\n",
    "        self.metadata = torch.zeros(self.nrows//mrow_m * A_num_cols_sp_pad//nelems_col, dtype=torch.int32, device=\"cpu\")\n",
    "        \n",
    "        func(dense_.data_ptr(), mask_.data_ptr(), self.values.data_ptr(), self.columns.data_ptr(), self.metadata.data_ptr())\n",
    "\n",
    "    def to_dense(self):\n",
    "        impl_builder = (\n",
    "            venom2dense\n",
    "            )\n",
    "        func = impl_builder(\n",
    "                (self.nrows, self.ncols),\n",
    "                torch.float32, \n",
    "                self.n,\n",
    "                self.m,\n",
    "                self.v\n",
    "            )\n",
    "        # initialize with ones\n",
    "        dense = torch.zeros((self.nrows, self.ncols), dtype=self.values.dtype, device=\"cpu\", requires_grad=True)\n",
    "        \n",
    "        func(dense.data_ptr(), self.values.data_ptr(), self.columns.data_ptr(), self.metadata.data_ptr())\n",
    "\n",
    "        return dense.to(device=\"cuda:0\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMVectorSparsifier:\n",
    "    def __init__(self, n, m, v):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.v = v\n",
    "\n",
    "    @staticmethod\n",
    "    def get_random_mask(tensor, m, v):\n",
    "        mask = torch.zeros(tensor.shape, dtype=tensor.dtype)\n",
    "        m_tmp = torch.cat( (torch.tensor([1,0,1,0]), torch.zeros(m-4)), 0 )\n",
    "        mask = mask.reshape(-1, v, m) + m_tmp\n",
    "        mask = mask.reshape(tensor.shape)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def __call__(self, tensor, grad_fmt=None):\n",
    "        nrows, ncols = tensor.shape\n",
    "        columns = torch.zeros(nrows//self.v, ncols//self.m*4, dtype=torch.int32)\n",
    "        columns = columns.reshape((-1,4)) + torch.tensor([0,1,2,3], dtype=torch.int32)\n",
    "        columns = columns.reshape((nrows//self.v, ncols//self.m*4))\n",
    "\n",
    "        mask = NMVectorSparsifier.get_random_mask(tensor, self.m, self.v)\n",
    "\n",
    "        sparse_mtx = sten.SparseTensorWrapper.wrapped_from_dense(\n",
    "            SparseVNMTensor(self.n, self.m, self.v, tensor, mask, columns, tensor.device),\n",
    "            tensor,\n",
    "            grad_fmt,\n",
    "        )\n",
    "\n",
    "        return sparse_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "input = torch.randn(256, 64, requires_grad=True, dtype=torch.half, device=\"cuda:0\")\n",
    "weight = torch.nn.Linear(64, 128, bias=True, dtype=torch.half, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=32\n",
    "n=2\n",
    "m=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spatha_sddmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_grad_output = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VenomLinearFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "\n",
    "        output = torch.matmul(input, weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias\n",
    "            \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "\n",
    "        global global_grad_output  \n",
    "        global_grad_output = grad_output \n",
    "\n",
    "        print(input.device, weight.device, grad_output.device)\n",
    "\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "        \n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output @ weight.to(\"cuda:0\")\n",
    "        \n",
    "        if ctx.needs_input_grad[1]:\n",
    "            #grad_weight = grad_output.t() @ input.to(\"cuda:0\")\n",
    "            grad_weight = grad_output.t() @ input.to(\"cuda:0\")\n",
    "\n",
    "            dense_grad_weight = torch.einsum(\"...m,...n->mn\", grad_output, input)\n",
    "            print(\"dense_grad_weight\", dense_grad_weight[:4, :4])\n",
    "        \n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "        \n",
    "        print(ctx.needs_input_grad[0], ctx.needs_input_grad[1], ctx.needs_input_grad[2])\n",
    "        \"\"\" print(grad_output)\n",
    "        print(input)\n",
    "        print(grad_weight) \"\"\"\n",
    "        #print(grad_input[:4, :4])\n",
    "        print(\"grad_weight\", grad_weight[:4, :4])\n",
    "        print(torch.allclose(dense_grad_weight, grad_weight, atol=0.05))\n",
    "        \n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SrnmSpmm(torch.nn.Module):\n",
    "    def __init__(self, original: torch.nn.Linear):\n",
    "        super(SrnmSpmm, self).__init__()        \n",
    "\n",
    "        self.w = NMVectorSparsifier(n, m, v)(original.weight).wrapped_tensor\n",
    "\n",
    "        self.values = torch.nn.Parameter(self.w.values.to(device=\"cuda:0\").half())\n",
    "        self.columns = self.w.columns.to(device=\"cuda:0\")\n",
    "        self.metadata = self.w.metadata.to(device=\"cuda:0\")\n",
    "\n",
    "        self.bias = original.bias\n",
    "\n",
    "        self.dense = torch.nn.Parameter(self.w.to_dense())\n",
    "        self.mask = self.w.mask\n",
    "\n",
    "        self.nrows_sp = self.w.nrows\n",
    "        self.ncols_sp = self.w.ncols\n",
    "        self.nnz      = self.w.nnz\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        return VenomLinearFunction.apply(input, self.dense, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_idx: 2047\n",
      "<class '__main__.SrnmSpmm'>\n"
     ]
    }
   ],
   "source": [
    "sparse_weight = SrnmSpmm(weight)\n",
    "\n",
    "print( type(sparse_weight) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse = sparse_weight(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = input @ sparse_weight.dense.T + sparse_weight.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.weight = torch.nn.Parameter(sparse_weight.dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense2 = weight(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(dense, dense2, atol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(sparse, dense2, atol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.11, -0.20,  0.07,  ..., -0.11,  0.12,  0.33],\n",
       "        [ 0.05,  0.09, -0.01,  ...,  0.17,  0.17,  0.15],\n",
       "        [ 0.21,  0.05, -0.25,  ..., -0.51, -0.15,  0.16],\n",
       "        ...,\n",
       "        [-0.24, -0.46,  0.27,  ..., -0.26,  0.41, -0.01],\n",
       "        [-0.24,  0.03, -0.23,  ..., -0.01, -0.14, -0.33],\n",
       "        [-0.40,  0.02,  0.14,  ...,  0.10,  0.34, -0.10]], device='cuda:0',\n",
       "       dtype=torch.float16, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.11, -0.20,  0.07,  ..., -0.11,  0.12,  0.33],\n",
       "        [ 0.05,  0.09, -0.01,  ...,  0.17,  0.17,  0.15],\n",
       "        [ 0.21,  0.05, -0.25,  ..., -0.51, -0.15,  0.16],\n",
       "        ...,\n",
       "        [-0.24, -0.46,  0.27,  ..., -0.26,  0.41, -0.01],\n",
       "        [-0.24,  0.03, -0.23,  ..., -0.01, -0.13, -0.33],\n",
       "        [-0.40,  0.02,  0.14,  ...,  0.10,  0.34, -0.10]], device='cuda:0',\n",
       "       dtype=torch.float16, grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print( dense[:8, :8] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print( sparse.device, sparse.dtype, dense.device, dense.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print( torch.allclose(sparse.half().cuda(), dense) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(232.12, device='cuda:0', dtype=torch.float16, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sparse.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(sparse_weight.dense.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 cuda:0 cuda:0\n",
      "dense_grad_weight tensor([[ -2.73, -17.98, -17.98, -10.99],\n",
      "        [ -2.73, -17.98, -17.98, -10.99],\n",
      "        [ -2.73, -17.98, -17.98, -10.99],\n",
      "        [ -2.73, -17.98, -17.98, -10.99]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "True True True\n",
      "grad_weight tensor([[ -2.73, -17.98, -17.98, -10.99],\n",
      "        [ -2.73, -17.98, -17.98, -10.99],\n",
      "        [ -2.73, -17.98, -17.98, -10.99],\n",
      "        [ -2.73, -17.98, -17.98, -10.99]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "sparse.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.30,  0.00, -0.59,  0.00,  0.00,  0.00,  0.00,  0.00],\n",
      "        [-2.30,  0.00, -0.59,  0.00,  0.00,  0.00,  0.00,  0.00],\n",
      "        [-2.30,  0.00, -0.59,  0.00,  0.00,  0.00,  0.00,  0.00],\n",
      "        [-2.30,  0.00, -0.59,  0.00,  0.00,  0.00,  0.00,  0.00],\n",
      "        [-2.30,  0.00, -0.59,  0.00,  0.00,  0.00,  0.00,  0.00],\n",
      "        [-2.30,  0.00, -0.59,  0.00,  0.00,  0.00,  0.00,  0.00],\n",
      "        [-2.30,  0.00, -0.59,  0.00,  0.00,  0.00,  0.00,  0.00],\n",
      "        [-2.30,  0.00, -0.59,  0.00,  0.00,  0.00,  0.00,  0.00]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(input.grad[:8, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(sparse_weight.dense.grad[:8, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(global_grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16 cuda:0\n",
      "torch.float16 cuda:0\n",
      "torch.int32 cuda:0\n",
      "torch.int32 cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(global_grad_output.dtype, global_grad_output.device)\n",
    "print(input.dtype, input.device)\n",
    "print(sparse_weight.metadata.dtype, sparse_weight.metadata.device)\n",
    "print(sparse_weight.columns.dtype, sparse_weight.columns.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_grad_output = global_grad_output.T.contiguous()\n",
    "input = input.T.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'> <class 'int'> <class 'int'> <class 'int'> <class 'int'> <class 'int'> <class 'int'> <class 'int'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(  type(global_grad_output),    \n",
    "        type(input),                 \n",
    "        type(sparse_weight.metadata),\n",
    "        type(sparse_weight.columns), \n",
    "        type(sparse_weight.nrows_sp),\n",
    "        type(sparse_weight.ncols_sp),\n",
    "        type(n),                     \n",
    "        type(m),                     \n",
    "        type(sparse_weight.nnz),     \n",
    "        type(0),                     \n",
    "        type(32),                    \n",
    "        type(4)                      \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 256])\n",
      "torch.Size([64, 256])\n",
      "torch.Size([128, 16])\n",
      "128 64 256\n"
     ]
    }
   ],
   "source": [
    "compressed_grad_weights = spatha_sddmm.sddmm(\n",
    "                            global_grad_output,     # A_matrix\n",
    "                            input,                  # B_matrix\n",
    "                            sparse_weight.metadata, # C_metadata\n",
    "                            sparse_weight.columns,  # C_indices\n",
    "                            sparse_weight.nrows_sp, # C_num_rows\n",
    "                            sparse_weight.ncols_sp, # C_num_cols    \n",
    "                            input.shape[1], \n",
    "                            n,                      # N\n",
    "                            m,                      # M\n",
    "                            sparse_weight.nnz,      # nnz\n",
    "                            0,                      # seed\n",
    "                            32,                     # mbrow\n",
    "                            4                       # brow\n",
    "                            )\n",
    "print(global_grad_output.shape)\n",
    "print(input.shape)\n",
    "print(compressed_grad_weights.shape)\n",
    "print(sparse_weight.nrows_sp, \n",
    "      sparse_weight.ncols_sp,\n",
    "      input.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -2.73, -17.98,  -2.73, -17.98,  -2.73, -17.98,  -2.73, -17.98],\n",
      "        [  0.16,  -3.23,   0.16,  -3.23,   0.16,  -3.23,   0.16,  -3.23],\n",
      "        [ -2.73, -17.98,  -2.73, -17.98,  -2.73, -17.98,  -2.73, -17.98],\n",
      "        [  0.16,  -3.23,   0.16,  -3.23,   0.16,  -3.23,   0.16,  -3.23],\n",
      "        [ -2.73, -17.98,  -2.73, -17.98,  -2.73, -17.98,  -2.73, -17.98],\n",
      "        [  0.16,  -3.23,   0.16,  -3.23,   0.16,  -3.23,   0.16,  -3.23],\n",
      "        [ -2.73, -17.98,  -2.73, -17.98,  -2.73, -17.98,  -2.73, -17.98],\n",
      "        [  0.16,  -3.23,   0.16,  -3.23,   0.16,  -3.23,   0.16,  -3.23]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(compressed_grad_weights[:8, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -2.73,   0.00, -17.98,   0.00,   0.00,   0.00,   0.00,   0.00],\n",
      "        [ -2.73,   0.00, -17.98,   0.00,   0.00,   0.00,   0.00,   0.00],\n",
      "        [ -2.73,   0.00, -17.98,   0.00,   0.00,   0.00,   0.00,   0.00],\n",
      "        [ -2.73,   0.00, -17.98,   0.00,   0.00,   0.00,   0.00,   0.00],\n",
      "        [ -2.73,   0.00, -17.98,   0.00,   0.00,   0.00,   0.00,   0.00],\n",
      "        [ -2.73,   0.00, -17.98,   0.00,   0.00,   0.00,   0.00,   0.00],\n",
      "        [ -2.73,   0.00, -17.98,   0.00,   0.00,   0.00,   0.00,   0.00],\n",
      "        [ -2.73,   0.00, -17.98,   0.00,   0.00,   0.00,   0.00,   0.00]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "compressed_grad_weights = compressed_grad_weights.float().cpu()\n",
    "columns = sparse_weight.columns.cpu()\n",
    "metadata = sparse_weight.metadata.cpu()\n",
    "\n",
    "impl_builder = (\n",
    "            venom2dense\n",
    "            )\n",
    "func = impl_builder(\n",
    "        (sparse_weight.nrows_sp, sparse_weight.ncols_sp),\n",
    "        torch.float32, \n",
    "        n,\n",
    "        m,\n",
    "        v\n",
    "    )\n",
    "\n",
    "dense = torch.zeros((sparse_weight.nrows_sp, sparse_weight.ncols_sp), dtype=compressed_grad_weights.dtype, device=\"cpu\", requires_grad=True)\n",
    "\n",
    "func(dense.data_ptr(), compressed_grad_weights.data_ptr(), columns.data_ptr(), metadata.data_ptr())\n",
    "\n",
    "print(dense[0:8, 0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80],\n",
      "        [ -2.73, -17.98, -17.98, -10.99,   9.53,  -8.05,  20.88, -23.80]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(sparse_weight.dense.grad[:8, :8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(dense, sparse_weight.dense.grad.cpu()*sparse_weight.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "print(dense.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
